{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Logistic Regression and Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.utils import resample, class_weight\n",
    "from sklearn.metrics import precision_recall_curve, auc, make_scorer, average_precision_score, accuracy_score,\\\n",
    "recall_score, precision_score, f1_score, roc_curve, balanced_accuracy_score, roc_auc_score, classification_report,\\\n",
    "confusion_matrix, brier_score_loss\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global prameters\n",
    "#Random state:\n",
    "rs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric features used in the model.\n",
    "numModCol = ['acc_open_past_24mths', 'annual_inc', 'avg_cur_bal', 'bc_open_to_buy', 'credit_hist_months',\n",
    "            'dti', 'fico', 'inq_last_6mths', 'installment',  'mo_sin_rcnt_rev_tl_op',\n",
    "            'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'num_actv_rev_tl', 'num_tl_op_past_12m', \n",
    "            'open_acc', 'percent_bc_gt_75', 'pub_rec', 'revol_bal', 'total_rev_hi_lim', 'int_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical features used in the model.\n",
    "catModCol = ['term',  'purpose', 'emp_length', 'home_ownership', 'verification_status', 'grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of categorical features before one hot encoding.\n",
    "len(catModCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of numerical features.\n",
    "len(numModCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Desktop\\Ryerson - Data Science\\Spring 2020\\MRP Course\\FromPandas\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\Alex\\Desktop\\Ryerson - Data Science\\Spring 2020\\MRP Course\\FromPandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the preprocessed pickled dataframe.\n",
    "dfFinal = pd.read_pickle('dfFinal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe for modelling.\n",
    "dfModel = dfFinal[numModCol+catModCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dti</th>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <td>4.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mort_acc</th>\n",
       "      <td>4.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <td>5.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <td>5.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <td>5.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <td>6.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <td>6.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <td>6.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <td>6.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <td>6.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <td>6.143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Instance %\n",
       "Feature                          \n",
       "dti                         0.004\n",
       "acc_open_past_24mths        4.250\n",
       "mort_acc                    4.250\n",
       "mths_since_recent_bc        5.171\n",
       "bc_open_to_buy              5.239\n",
       "percent_bc_gt_75            5.275\n",
       "mo_sin_rcnt_rev_tl_op       6.142\n",
       "mo_sin_rcnt_tl              6.142\n",
       "num_actv_rev_tl             6.142\n",
       "num_tl_op_past_12m          6.142\n",
       "total_rev_hi_lim            6.142\n",
       "avg_cur_bal                 6.143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns with missing values and percentage of instances containing them that need to be addressed.\n",
    "dfMissing = pd.DataFrame((dfModel.isna().sum() / dfModel.shape[0] * 100)[dfModel.isna().sum() > 0])\n",
    "dfMissing.index.name = 'Feature'\n",
    "dfMissing.columns = ['Instance %']\n",
    "dfMissing = dfMissing.round(3)\n",
    "dfMissing = dfMissing.sort_values(by = ['Instance %'])\n",
    "dfMissing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unused categorical labels in categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in catModCol:\n",
    "    dfFinal.loc[:, col] = dfFinal.loc[:,col].cat.remove_unused_categories()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Training and Testing Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of records by term and year.\n",
    "yearTermQty = dfFinal.groupby(['issue_d_year', 'term'], observed=True).size()\n",
    "termQty = dfFinal.groupby(['term'], observed=True).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_d_year  term     \n",
       "2007          36 months    0.000259\n",
       "2008          36 months    0.001417\n",
       "2009          36 months    0.004980\n",
       "2010          36 months    0.008554\n",
       "              60 months    0.019774\n",
       "2011          36 months    0.014904\n",
       "              60 months    0.055729\n",
       "2012          36 months    0.046153\n",
       "              60 months    0.077711\n",
       "2013          36 months    0.106669\n",
       "              60 months    0.271018\n",
       "2014          36 months    0.172686\n",
       "              60 months    0.575768\n",
       "2015          36 months    0.300794\n",
       "2016          36 months    0.343583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearTermQty.div(termQty, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_d_month\n",
       "1          0.056575\n",
       "2          0.115791\n",
       "3          0.182550\n",
       "4          0.262025\n",
       "5          0.344033\n",
       "6          0.421661\n",
       "7          0.549854\n",
       "8          0.625565\n",
       "9          0.669847\n",
       "10         0.836970\n",
       "11         0.953062\n",
       "12         1.000000\n",
       "Missing    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the breakdown of 60 month loans by month in 2014 to establish cut-off.\n",
    "group1 = dfFinal[(dfFinal.issue_d_year == '2014') & (dfFinal.term_numeric==60)].groupby(['issue_d_month']).size()\n",
    "totalQty = dfFinal[(dfFinal.issue_d_year == '2014') & (dfFinal.term_numeric==60)].shape[0]\n",
    "np.cumsum(group1.div(totalQty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617962, 323455)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Locations of 36 months training and testing data.\n",
    "train36 = ((dfFinal.term_numeric==36) & (dfFinal.issue_d <= dt.datetime(year=2015, month =12, day=31)))\n",
    "test36 = ((dfFinal.term_numeric==36) & (dfFinal.issue_d > dt.datetime(year=2015, month =12, day=31)))\n",
    "train36.sum(), test36.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84631, 42250)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Locations of 60 months training and testing data.\n",
    "train60 = ((dfFinal.term_numeric==60) & (dfFinal.issue_d <= dt.datetime(year = 2014, month = 6, day = 30)))\n",
    "test60 = ((dfFinal.term_numeric==60) & (dfFinal.issue_d > dt.datetime(year = 2014, month = 6, day = 30)))\n",
    "train60.sum(), test60.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941417, 126881)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of each type of loan based on term.\n",
    "count36 = (dfFinal.term_numeric==36).sum()\n",
    "count60 = (dfFinal.term_numeric==60).sum()\n",
    "count36, count60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702593, 365705)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAll = (train36 | train60)\n",
    "testAll = (test36 | test60)\n",
    "trainAll.sum(), testAll.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train %</th>\n",
       "      <th>Test %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36 Months</th>\n",
       "      <td>65.6</td>\n",
       "      <td>34.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60 Months</th>\n",
       "      <td>66.7</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Data</th>\n",
       "      <td>65.8</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Train %  Test %\n",
       "36 Months     65.6    34.4\n",
       "60 Months     66.7    33.3\n",
       "All Data      65.8    34.2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage breakdown of training and test sets based on loan term.\n",
    "trainTest36 = pd.Series([train36.sum()/count36, test36.sum()/count36], name = '36 Months')\n",
    "trainTest60 = pd.Series([train60.sum()/count60, test60.sum()/count60], name = '60 Months')\n",
    "trainTestAll = pd.Series([trainAll.sum() / dfFinal.shape[0], testAll.sum()/dfFinal.shape[0]], name = 'All Data')\n",
    "df = (pd.DataFrame([trainTest36, trainTest60, trainTestAll]).T*100).round(1)\n",
    "df.index = ['Train %', 'Test %']\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get target labels for training and testing set.\n",
    "trainLabels = pd.get_dummies(dfFinal.loan_status)['Charged Off'][trainAll]\n",
    "testLabels = pd.get_dummies(dfFinal.loan_status)['Charged Off'][testAll]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scalers for Numeric Features - Standardization (z-score Normalization) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaler based on training set.\n",
    "scaler = scaler.fit(dfFinal.loc[trainAll, numModCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply scaler to entire dataset.\n",
    "scaledArray = scaler.transform(dfFinal[numModCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert array of scaled data into dataframe.\n",
    "dfScaled = pd.DataFrame(scaledArray, columns=numModCol, index =dfFinal[numModCol].index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one-hot encoded indicator columns for all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneEncoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotArray = oneEncoder.fit_transform(dfFinal[catModCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotColumns = oneEncoder.get_feature_names(catModCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinalOne = pd.concat([dfScaled, pd.DataFrame(oneHotArray.toarray(), columns=oneHotColumns, index = dfScaled.index)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>credit_hist_months</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>installment</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>term_36 months</th>\n",
       "      <th>term_60 months</th>\n",
       "      <th>purpose_car</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_debt_consolidation</th>\n",
       "      <th>purpose_educational</th>\n",
       "      <th>purpose_home_improvement</th>\n",
       "      <th>purpose_house</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_medical</th>\n",
       "      <th>purpose_moving</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_renewable_energy</th>\n",
       "      <th>purpose_small_business</th>\n",
       "      <th>purpose_vacation</th>\n",
       "      <th>purpose_wedding</th>\n",
       "      <th>emp_length_1 year</th>\n",
       "      <th>emp_length_10+ years</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "      <th>emp_length_Missing</th>\n",
       "      <th>home_ownership_ANY</th>\n",
       "      <th>home_ownership_MORTGAGE</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "      <th>verification_status_Not Verified</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.580095</td>\n",
       "      <td>-0.047358</td>\n",
       "      <td>2.309900</td>\n",
       "      <td>-0.498368</td>\n",
       "      <td>0.119843</td>\n",
       "      <td>-1.079649</td>\n",
       "      <td>0.637486</td>\n",
       "      <td>0.296605</td>\n",
       "      <td>-0.371608</td>\n",
       "      <td>0.340303</td>\n",
       "      <td>-0.568557</td>\n",
       "      <td>1.586125</td>\n",
       "      <td>-0.185791</td>\n",
       "      <td>-0.851168</td>\n",
       "      <td>2.335077</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-1.385753</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>-0.660921</td>\n",
       "      <td>-0.699985</td>\n",
       "      <td>-0.281675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.133706</td>\n",
       "      <td>-0.734901</td>\n",
       "      <td>-0.665976</td>\n",
       "      <td>-0.295089</td>\n",
       "      <td>1.518996</td>\n",
       "      <td>1.884512</td>\n",
       "      <td>0.637486</td>\n",
       "      <td>0.296605</td>\n",
       "      <td>-0.343091</td>\n",
       "      <td>-0.628604</td>\n",
       "      <td>-0.568557</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>-0.705763</td>\n",
       "      <td>0.093699</td>\n",
       "      <td>-0.598523</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.446996</td>\n",
       "      <td>1.367555</td>\n",
       "      <td>-0.386778</td>\n",
       "      <td>-0.281345</td>\n",
       "      <td>0.077076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.472931</td>\n",
       "      <td>-0.624495</td>\n",
       "      <td>-0.560560</td>\n",
       "      <td>-0.331473</td>\n",
       "      <td>-0.634388</td>\n",
       "      <td>0.199341</td>\n",
       "      <td>0.960813</td>\n",
       "      <td>-0.719712</td>\n",
       "      <td>0.567444</td>\n",
       "      <td>1.793664</td>\n",
       "      <td>2.237010</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>0.594166</td>\n",
       "      <td>-0.851168</td>\n",
       "      <td>-1.185243</td>\n",
       "      <td>-1.212674</td>\n",
       "      <td>0.494581</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>-0.270879</td>\n",
       "      <td>0.424025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.223194</td>\n",
       "      <td>0.102885</td>\n",
       "      <td>-0.649397</td>\n",
       "      <td>0.983254</td>\n",
       "      <td>1.431549</td>\n",
       "      <td>0.290006</td>\n",
       "      <td>1.769133</td>\n",
       "      <td>-0.719712</td>\n",
       "      <td>-0.719057</td>\n",
       "      <td>-0.507490</td>\n",
       "      <td>-0.352744</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>-0.640766</td>\n",
       "      <td>1.353522</td>\n",
       "      <td>0.574917</td>\n",
       "      <td>0.522328</td>\n",
       "      <td>-0.579493</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>-0.224812</td>\n",
       "      <td>0.320449</td>\n",
       "      <td>-1.733198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.901645</td>\n",
       "      <td>-0.740144</td>\n",
       "      <td>-0.548986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.929522</td>\n",
       "      <td>1.078192</td>\n",
       "      <td>1.769133</td>\n",
       "      <td>-0.719712</td>\n",
       "      <td>-1.023848</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>-0.352744</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.166123</td>\n",
       "      <td>0.574917</td>\n",
       "      <td>-1.019896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>-0.430548</td>\n",
       "      <td>-0.370306</td>\n",
       "      <td>-0.451609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_open_past_24mths  annual_inc  avg_cur_bal  bc_open_to_buy  \\\n",
       "0              1.580095   -0.047358     2.309900       -0.498368   \n",
       "1             -1.133706   -0.734901    -0.665976       -0.295089   \n",
       "2             -1.472931   -0.624495    -0.560560       -0.331473   \n",
       "3              0.223194    0.102885    -0.649397        0.983254   \n",
       "5              0.901645   -0.740144    -0.548986             NaN   \n",
       "\n",
       "   credit_hist_months       dti      fico  inq_last_6mths  installment  \\\n",
       "0            0.119843 -1.079649  0.637486        0.296605    -0.371608   \n",
       "1            1.518996  1.884512  0.637486        0.296605    -0.343091   \n",
       "2           -0.634388  0.199341  0.960813       -0.719712     0.567444   \n",
       "3            1.431549  0.290006  1.769133       -0.719712    -0.719057   \n",
       "5           -0.929522  1.078192  1.769133       -0.719712    -1.023848   \n",
       "\n",
       "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  \\\n",
       "0               0.340303       -0.568557  1.586125             -0.185791   \n",
       "1              -0.628604       -0.568557 -0.814465             -0.705763   \n",
       "2               1.793664        2.237010 -0.814465              0.594166   \n",
       "3              -0.507490       -0.352744 -0.814465             -0.640766   \n",
       "5               0.037520       -0.352744 -0.814465                   NaN   \n",
       "\n",
       "   num_actv_rev_tl  num_tl_op_past_12m  open_acc  percent_bc_gt_75   pub_rec  \\\n",
       "0        -0.851168            2.335077 -0.248784         -1.385753 -0.333064   \n",
       "1         0.093699           -0.598523 -0.248784         -0.446996  1.367555   \n",
       "2        -0.851168           -1.185243 -1.212674          0.494581 -0.333064   \n",
       "3         1.353522            0.574917  0.522328         -0.579493 -0.333064   \n",
       "5        -1.166123            0.574917 -1.019896               NaN -0.333064   \n",
       "\n",
       "   revol_bal  total_rev_hi_lim  int_rate  term_36 months  term_60 months  \\\n",
       "0  -0.660921         -0.699985 -0.281675             1.0             0.0   \n",
       "1  -0.386778         -0.281345  0.077076             1.0             0.0   \n",
       "2   0.011673         -0.270879  0.424025             1.0             0.0   \n",
       "3  -0.224812          0.320449 -1.733198             1.0             0.0   \n",
       "5  -0.430548         -0.370306 -0.451609             1.0             0.0   \n",
       "\n",
       "   purpose_car  purpose_credit_card  purpose_debt_consolidation  \\\n",
       "0          0.0                  0.0                         1.0   \n",
       "1          0.0                  1.0                         0.0   \n",
       "2          0.0                  0.0                         1.0   \n",
       "3          0.0                  0.0                         0.0   \n",
       "5          0.0                  0.0                         1.0   \n",
       "\n",
       "   purpose_educational  purpose_home_improvement  purpose_house  \\\n",
       "0                  0.0                       0.0            0.0   \n",
       "1                  0.0                       0.0            0.0   \n",
       "2                  0.0                       0.0            0.0   \n",
       "3                  0.0                       0.0            0.0   \n",
       "5                  0.0                       0.0            0.0   \n",
       "\n",
       "   purpose_major_purchase  purpose_medical  purpose_moving  purpose_other  \\\n",
       "0                     0.0              0.0             0.0            0.0   \n",
       "1                     0.0              0.0             0.0            0.0   \n",
       "2                     0.0              0.0             0.0            0.0   \n",
       "3                     0.0              0.0             0.0            0.0   \n",
       "5                     0.0              0.0             0.0            0.0   \n",
       "\n",
       "   purpose_renewable_energy  purpose_small_business  purpose_vacation  \\\n",
       "0                       0.0                     0.0               0.0   \n",
       "1                       0.0                     0.0               0.0   \n",
       "2                       0.0                     0.0               0.0   \n",
       "3                       0.0                     0.0               1.0   \n",
       "5                       0.0                     0.0               0.0   \n",
       "\n",
       "   purpose_wedding  emp_length_1 year  emp_length_10+ years  \\\n",
       "0              0.0                0.0                   0.0   \n",
       "1              0.0                0.0                   0.0   \n",
       "2              0.0                0.0                   0.0   \n",
       "3              0.0                0.0                   1.0   \n",
       "5              0.0                0.0                   1.0   \n",
       "\n",
       "   emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 0.0                 0.0   \n",
       "2                 0.0                 0.0                 1.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "5                 0.0                 0.0                 0.0   \n",
       "\n",
       "   emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 1.0                 0.0   \n",
       "2                 0.0                 0.0                 0.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "5                 0.0                 0.0                 0.0   \n",
       "\n",
       "   emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \\\n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                 0.0                 0.0                  0.0   \n",
       "2                 0.0                 0.0                  0.0   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "5                 0.0                 0.0                  0.0   \n",
       "\n",
       "   emp_length_Missing  home_ownership_ANY  home_ownership_MORTGAGE  \\\n",
       "0                 1.0                 0.0                      1.0   \n",
       "1                 0.0                 0.0                      0.0   \n",
       "2                 0.0                 0.0                      1.0   \n",
       "3                 0.0                 0.0                      0.0   \n",
       "5                 0.0                 0.0                      0.0   \n",
       "\n",
       "   home_ownership_NONE  home_ownership_OTHER  home_ownership_OWN  \\\n",
       "0                  0.0                   0.0                 0.0   \n",
       "1                  0.0                   0.0                 0.0   \n",
       "2                  0.0                   0.0                 0.0   \n",
       "3                  0.0                   0.0                 0.0   \n",
       "5                  0.0                   0.0                 0.0   \n",
       "\n",
       "   home_ownership_RENT  verification_status_Not Verified  \\\n",
       "0                  0.0                               0.0   \n",
       "1                  1.0                               0.0   \n",
       "2                  0.0                               0.0   \n",
       "3                  1.0                               0.0   \n",
       "5                  1.0                               0.0   \n",
       "\n",
       "   verification_status_Source Verified  verification_status_Verified  grade_A  \\\n",
       "0                                  0.0                           1.0      0.0   \n",
       "1                                  1.0                           0.0      0.0   \n",
       "2                                  1.0                           0.0      0.0   \n",
       "3                                  1.0                           0.0      1.0   \n",
       "5                                  1.0                           0.0      0.0   \n",
       "\n",
       "   grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \n",
       "0      1.0      0.0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      1.0      0.0      0.0      0.0      0.0  \n",
       "2      0.0      1.0      0.0      0.0      0.0      0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0  \n",
       "5      1.0      0.0      0.0      0.0      0.0      0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Excludes original categorical columns.\n",
    "dfFinalOneLimit = dfFinalOne[numModCol + oneHotColumns.tolist()]\n",
    "dfFinalOneLimit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068298, 65)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New dataframe shape after one hot encoding and excluding original categorical columns.\n",
    "dfFinalOneLimit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create imputer.\n",
    "imp = IterativeImputer(estimator=LinearRegression(), verbose=2, min_value = dfFinalOneLimit.loc[trainAll].min(),\n",
    "             max_value = dfFinalOneLimit.loc[trainAll].max(), skip_complete=True, tol = 0.001,\n",
    "                      max_iter=15, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the imputer on training scaled data.\n",
    "imp = imp.fit(dfFinalOneLimit[trainAll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputedArray = imp.transform(dfFinalOneLimit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPostImpute = pd.DataFrame(imputedArray, columns=dfFinalOneLimit.columns, index = dfFinalOneLimit.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature names that needed missing value imputation\n",
    "missingColumns = dfMissing.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for google colab - saving the imputed files to be loaded in Google colab for training the NN model.\n",
    "dfPostImpute[missingColumns].to_csv('missingImputs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading previously imputed values to save time without needing to rerun the imputer every time.\n",
    "dfImput = pd.read_csv('missingImputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature names in the loaded file with imputed values.\n",
    "imputColumns = dfImput.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the loaded imputed values into the dataframe.\n",
    "dfFinalOneLimit.loc[:, imputColumns] = dfImput.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The assigning the dataframe from previous call to a different name which was used in the coding below before I \n",
    "#started loading the imputed values.\n",
    "dfPostImpute = dfFinalOneLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>credit_hist_months</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>installment</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>term_36 months</th>\n",
       "      <th>term_60 months</th>\n",
       "      <th>purpose_car</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_debt_consolidation</th>\n",
       "      <th>purpose_educational</th>\n",
       "      <th>purpose_home_improvement</th>\n",
       "      <th>purpose_house</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_medical</th>\n",
       "      <th>purpose_moving</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_renewable_energy</th>\n",
       "      <th>purpose_small_business</th>\n",
       "      <th>purpose_vacation</th>\n",
       "      <th>purpose_wedding</th>\n",
       "      <th>emp_length_1 year</th>\n",
       "      <th>emp_length_10+ years</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "      <th>emp_length_Missing</th>\n",
       "      <th>home_ownership_ANY</th>\n",
       "      <th>home_ownership_MORTGAGE</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "      <th>verification_status_Not Verified</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.580095</td>\n",
       "      <td>-0.047358</td>\n",
       "      <td>2.309900</td>\n",
       "      <td>-0.498368</td>\n",
       "      <td>0.119843</td>\n",
       "      <td>-1.079649</td>\n",
       "      <td>0.637486</td>\n",
       "      <td>0.296605</td>\n",
       "      <td>-0.371608</td>\n",
       "      <td>0.340303</td>\n",
       "      <td>-0.568557</td>\n",
       "      <td>1.586125</td>\n",
       "      <td>-0.185791</td>\n",
       "      <td>-0.851168</td>\n",
       "      <td>2.335077</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-1.385753</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>-0.660921</td>\n",
       "      <td>-0.699985</td>\n",
       "      <td>-0.281675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.133706</td>\n",
       "      <td>-0.734901</td>\n",
       "      <td>-0.665976</td>\n",
       "      <td>-0.295089</td>\n",
       "      <td>1.518996</td>\n",
       "      <td>1.884512</td>\n",
       "      <td>0.637486</td>\n",
       "      <td>0.296605</td>\n",
       "      <td>-0.343091</td>\n",
       "      <td>-0.628604</td>\n",
       "      <td>-0.568557</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>-0.705763</td>\n",
       "      <td>0.093699</td>\n",
       "      <td>-0.598523</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.446996</td>\n",
       "      <td>1.367555</td>\n",
       "      <td>-0.386778</td>\n",
       "      <td>-0.281345</td>\n",
       "      <td>0.077076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.472931</td>\n",
       "      <td>-0.624495</td>\n",
       "      <td>-0.560560</td>\n",
       "      <td>-0.331473</td>\n",
       "      <td>-0.634388</td>\n",
       "      <td>0.199341</td>\n",
       "      <td>0.960813</td>\n",
       "      <td>-0.719712</td>\n",
       "      <td>0.567444</td>\n",
       "      <td>1.793664</td>\n",
       "      <td>2.237010</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>0.594166</td>\n",
       "      <td>-0.851168</td>\n",
       "      <td>-1.185243</td>\n",
       "      <td>-1.212674</td>\n",
       "      <td>0.494581</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>-0.270879</td>\n",
       "      <td>0.424025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.223194</td>\n",
       "      <td>0.102885</td>\n",
       "      <td>-0.649397</td>\n",
       "      <td>0.983254</td>\n",
       "      <td>1.431549</td>\n",
       "      <td>0.290006</td>\n",
       "      <td>1.769133</td>\n",
       "      <td>-0.719712</td>\n",
       "      <td>-0.719057</td>\n",
       "      <td>-0.507490</td>\n",
       "      <td>-0.352744</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>-0.640766</td>\n",
       "      <td>1.353522</td>\n",
       "      <td>0.574917</td>\n",
       "      <td>0.522328</td>\n",
       "      <td>-0.579493</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>-0.224812</td>\n",
       "      <td>0.320449</td>\n",
       "      <td>-1.733198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.901645</td>\n",
       "      <td>-0.740144</td>\n",
       "      <td>-0.548986</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>-0.929522</td>\n",
       "      <td>1.078192</td>\n",
       "      <td>1.769133</td>\n",
       "      <td>-0.719712</td>\n",
       "      <td>-1.023848</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>-0.352744</td>\n",
       "      <td>-0.814465</td>\n",
       "      <td>-0.033203</td>\n",
       "      <td>-1.166123</td>\n",
       "      <td>0.574917</td>\n",
       "      <td>-1.019896</td>\n",
       "      <td>-0.612549</td>\n",
       "      <td>-0.333064</td>\n",
       "      <td>-0.430548</td>\n",
       "      <td>-0.370306</td>\n",
       "      <td>-0.451609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_open_past_24mths  annual_inc  avg_cur_bal  bc_open_to_buy  \\\n",
       "0              1.580095   -0.047358     2.309900       -0.498368   \n",
       "1             -1.133706   -0.734901    -0.665976       -0.295089   \n",
       "2             -1.472931   -0.624495    -0.560560       -0.331473   \n",
       "3              0.223194    0.102885    -0.649397        0.983254   \n",
       "5              0.901645   -0.740144    -0.548986        0.057983   \n",
       "\n",
       "   credit_hist_months       dti      fico  inq_last_6mths  installment  \\\n",
       "0            0.119843 -1.079649  0.637486        0.296605    -0.371608   \n",
       "1            1.518996  1.884512  0.637486        0.296605    -0.343091   \n",
       "2           -0.634388  0.199341  0.960813       -0.719712     0.567444   \n",
       "3            1.431549  0.290006  1.769133       -0.719712    -0.719057   \n",
       "5           -0.929522  1.078192  1.769133       -0.719712    -1.023848   \n",
       "\n",
       "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  \\\n",
       "0               0.340303       -0.568557  1.586125             -0.185791   \n",
       "1              -0.628604       -0.568557 -0.814465             -0.705763   \n",
       "2               1.793664        2.237010 -0.814465              0.594166   \n",
       "3              -0.507490       -0.352744 -0.814465             -0.640766   \n",
       "5               0.037520       -0.352744 -0.814465             -0.033203   \n",
       "\n",
       "   num_actv_rev_tl  num_tl_op_past_12m  open_acc  percent_bc_gt_75   pub_rec  \\\n",
       "0        -0.851168            2.335077 -0.248784         -1.385753 -0.333064   \n",
       "1         0.093699           -0.598523 -0.248784         -0.446996  1.367555   \n",
       "2        -0.851168           -1.185243 -1.212674          0.494581 -0.333064   \n",
       "3         1.353522            0.574917  0.522328         -0.579493 -0.333064   \n",
       "5        -1.166123            0.574917 -1.019896         -0.612549 -0.333064   \n",
       "\n",
       "   revol_bal  total_rev_hi_lim  int_rate  term_36 months  term_60 months  \\\n",
       "0  -0.660921         -0.699985 -0.281675             1.0             0.0   \n",
       "1  -0.386778         -0.281345  0.077076             1.0             0.0   \n",
       "2   0.011673         -0.270879  0.424025             1.0             0.0   \n",
       "3  -0.224812          0.320449 -1.733198             1.0             0.0   \n",
       "5  -0.430548         -0.370306 -0.451609             1.0             0.0   \n",
       "\n",
       "   purpose_car  purpose_credit_card  purpose_debt_consolidation  \\\n",
       "0          0.0                  0.0                         1.0   \n",
       "1          0.0                  1.0                         0.0   \n",
       "2          0.0                  0.0                         1.0   \n",
       "3          0.0                  0.0                         0.0   \n",
       "5          0.0                  0.0                         1.0   \n",
       "\n",
       "   purpose_educational  purpose_home_improvement  purpose_house  \\\n",
       "0                  0.0                       0.0            0.0   \n",
       "1                  0.0                       0.0            0.0   \n",
       "2                  0.0                       0.0            0.0   \n",
       "3                  0.0                       0.0            0.0   \n",
       "5                  0.0                       0.0            0.0   \n",
       "\n",
       "   purpose_major_purchase  purpose_medical  purpose_moving  purpose_other  \\\n",
       "0                     0.0              0.0             0.0            0.0   \n",
       "1                     0.0              0.0             0.0            0.0   \n",
       "2                     0.0              0.0             0.0            0.0   \n",
       "3                     0.0              0.0             0.0            0.0   \n",
       "5                     0.0              0.0             0.0            0.0   \n",
       "\n",
       "   purpose_renewable_energy  purpose_small_business  purpose_vacation  \\\n",
       "0                       0.0                     0.0               0.0   \n",
       "1                       0.0                     0.0               0.0   \n",
       "2                       0.0                     0.0               0.0   \n",
       "3                       0.0                     0.0               1.0   \n",
       "5                       0.0                     0.0               0.0   \n",
       "\n",
       "   purpose_wedding  emp_length_1 year  emp_length_10+ years  \\\n",
       "0              0.0                0.0                   0.0   \n",
       "1              0.0                0.0                   0.0   \n",
       "2              0.0                0.0                   0.0   \n",
       "3              0.0                0.0                   1.0   \n",
       "5              0.0                0.0                   1.0   \n",
       "\n",
       "   emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 0.0                 0.0   \n",
       "2                 0.0                 0.0                 1.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "5                 0.0                 0.0                 0.0   \n",
       "\n",
       "   emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 1.0                 0.0   \n",
       "2                 0.0                 0.0                 0.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "5                 0.0                 0.0                 0.0   \n",
       "\n",
       "   emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \\\n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                 0.0                 0.0                  0.0   \n",
       "2                 0.0                 0.0                  0.0   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "5                 0.0                 0.0                  0.0   \n",
       "\n",
       "   emp_length_Missing  home_ownership_ANY  home_ownership_MORTGAGE  \\\n",
       "0                 1.0                 0.0                      1.0   \n",
       "1                 0.0                 0.0                      0.0   \n",
       "2                 0.0                 0.0                      1.0   \n",
       "3                 0.0                 0.0                      0.0   \n",
       "5                 0.0                 0.0                      0.0   \n",
       "\n",
       "   home_ownership_NONE  home_ownership_OTHER  home_ownership_OWN  \\\n",
       "0                  0.0                   0.0                 0.0   \n",
       "1                  0.0                   0.0                 0.0   \n",
       "2                  0.0                   0.0                 0.0   \n",
       "3                  0.0                   0.0                 0.0   \n",
       "5                  0.0                   0.0                 0.0   \n",
       "\n",
       "   home_ownership_RENT  verification_status_Not Verified  \\\n",
       "0                  0.0                               0.0   \n",
       "1                  1.0                               0.0   \n",
       "2                  0.0                               0.0   \n",
       "3                  1.0                               0.0   \n",
       "5                  1.0                               0.0   \n",
       "\n",
       "   verification_status_Source Verified  verification_status_Verified  grade_A  \\\n",
       "0                                  0.0                           1.0      0.0   \n",
       "1                                  1.0                           0.0      0.0   \n",
       "2                                  1.0                           0.0      0.0   \n",
       "3                                  1.0                           0.0      1.0   \n",
       "5                                  1.0                           0.0      0.0   \n",
       "\n",
       "   grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \n",
       "0      1.0      0.0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      1.0      0.0      0.0      0.0      0.0  \n",
       "2      0.0      1.0      0.0      0.0      0.0      0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0  \n",
       "5      1.0      0.0      0.0      0.0      0.0      0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview of the dataframe to be used in modelling.\n",
    "dfPostImpute.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068298, 65)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the dataframe to be used in modelling.\n",
    "dfPostImpute.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample for performing cross validation grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain, dfTest, yTrain, yTest = train_test_split(dfPostImpute[trainAll], trainLabels, stratify = trainLabels, train_size =0.7, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used in modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCost(y, y_pred, cost = (1,4), returnThreshold = False):\n",
    "    #y_true must be a single column of binary labels for positive class.\n",
    "    #y_prob must be single column of probabilities for positive class.\n",
    "    \n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    result = y_pred.reshape(-1,1) >=  np.array(thresholds)\n",
    "    \n",
    "    #Convert boolean result array into binary array.\n",
    "    resultBinary = result.astype('int')\n",
    "    \n",
    "    #Calculate loss from False Negatives\n",
    "    lossFN = np.sum((y.values.reshape(-1,1) > resultBinary), axis=0)*cost[1]\n",
    "    \n",
    "    #Calculate loss from False Positive\n",
    "    lossFP = np.sum((y.values.reshape(-1,1) < resultBinary), axis=0)*cost[0]\n",
    "    \n",
    "    totalLoss = lossFN + lossFP\n",
    "    \n",
    "    minInd = np.argmin(totalLoss)\n",
    "    minThres = thresholds[minInd]\n",
    "    minLoss = totalLoss[minInd]\n",
    "    \n",
    "    if returnThreshold:\n",
    "        return minLoss, minThres\n",
    "    else:\n",
    "        return minLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCostRev(y, y_pred, cost = (1,4), returnThreshold = False):\n",
    "    #y_true must be a single column of binary labels for positive class.\n",
    "    #y_prob must be single column of probabilities for positive class.\n",
    "    \n",
    "    thresholds = [0.5]\n",
    "    result = y_pred.reshape(-1,1) >=  np.array(thresholds)\n",
    "    \n",
    "    #Convert boolean result array into binary array.\n",
    "    resultBinary = result.astype('int')\n",
    "    \n",
    "    #Calculate loss from False Negatives\n",
    "    lossFN = np.sum((y.values.reshape(-1,1) > resultBinary), axis=0)*cost[1]\n",
    "    \n",
    "    #Calculate loss from False Positive\n",
    "    lossFP = np.sum((y.values.reshape(-1,1) < resultBinary), axis=0)*cost[0]\n",
    "    \n",
    "    totalLoss = lossFN + lossFP\n",
    "    \n",
    "    minInd = np.argmin(totalLoss)\n",
    "    minThres = thresholds[minInd]\n",
    "    minLoss = totalLoss[minInd]\n",
    "    \n",
    "    if returnThreshold:\n",
    "        return minLoss, minThres\n",
    "    else:\n",
    "        return minLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(y, y_hat, cost = (1,5)):\n",
    "    #Calculate loss from False Negatives\n",
    "    lossFN = np.sum(y > y_hat)*cost[1]\n",
    "    \n",
    "    #Calculate loss from False Positive\n",
    "    lossFP = np.sum(y < y_hat)*cost[0]\n",
    "    \n",
    "    return lossFN+lossFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFScore(y, y_prob):\n",
    "    prec, recl, thres = precision_recall_curve(y, y_prob)\n",
    "    fscore = (2 * prec * recl) / (prec + recl)\n",
    "    maxInd = np.nanargmax(fscore)\n",
    "    return fscore[maxInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFScoreThres(y, y_prob, threshold=True):\n",
    "    prec, recl, thres = precision_recall_curve(y, y_prob)\n",
    "    fscore = (2 * prec * recl) / (prec + recl)\n",
    "    maxInd = np.nanargmax(fscore)\n",
    "    if threshold:\n",
    "        return fscore[maxInd], thres[maxInd]\n",
    "    else:\n",
    "        return fscore[maxInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "highFScore = make_scorer(score_func=getFScore, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_prob, plt, labelName):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    aucScore = np.round(roc_auc_score(y_true, y_prob),3)\n",
    "    #fig, axs = plt.subplots(figsize=(3,3))\n",
    "    plt.plot(fpr, tpr, label = labelName + '- AUC: '+str(aucScore))\n",
    "    plt.legend(loc = 'center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pre_recl(y_true, y_prob, plt, labelName):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    plt.plot(recall, precision, label = labelName)\n",
    "    plt.legend(loc = 'center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(y_true, y_pred, y_prob):\n",
    "    scoresDict ={}\n",
    "    scoresDict['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    scoresDict['Precision(+)'] = precision_score(y_true, y_pred)\n",
    "    scoresDict['Recall(+)'] = recall_score(y_true, y_pred)\n",
    "    scoresDict['AUC'] = roc_auc_score(y_true, y_prob)\n",
    "    scoresDict['Error Cost'] = getLoss(y_true, y_pred)\n",
    "    return pd.DataFrame(scoresDict, index=['Result'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(origProb, threshold):\n",
    "    result = origProb >= threshold\n",
    "    return result.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fScoreThres(y, y_prob):\n",
    "    prec, recl, thres = precision_recall_curve(y, y_prob)\n",
    "    fscore = (2 * prec * recl) / (prec + recl)\n",
    "    maxInd = np.argmax(fscore)\n",
    "    return fscore[maxInd], thres[maxInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aucPreRec(y, y_prob):\n",
    "    prec, recl, thres = precision_recall_curve(y, y_prob)\n",
    "    aucScore = auc(prec, recl)\n",
    "    return aucScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGridScores(gridObj):\n",
    "    allResults = {}\n",
    "    for score in gridObj.scorer_.keys():\n",
    "        allResults[score] = gridObj.cv_results_['mean_test_'+score].tolist()\n",
    "    pd.DataFrame.from_dict(allResults).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring functions used in the gridsearches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoringDict2 = {'accuracy': make_scorer(accuracy_score), \n",
    "                'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "               'f1': make_scorer(f1_score),\n",
    "              'average_precision': make_scorer(average_precision_score, needs_proba=True), \n",
    "               'roc_auc': make_scorer(roc_auc_score, needs_proba=True), \n",
    "               'loan_loss': make_scorer(score_func=calcCostRev, greater_is_better=False, needs_proba=True, cost = (1,3.7), returnThreshold = False),\n",
    "               'loan_loss2': make_scorer(score_func=calcCost, greater_is_better=False, needs_proba=True, cost = (1,2), returnThreshold = False),\n",
    "               'loan_loss3': make_scorer(score_func=calcCost, greater_is_better=False, needs_proba=True, cost = (1,3), returnThreshold = False),\n",
    "               'loan_loss4': make_scorer(score_func=calcCost, greater_is_better=False, needs_proba=True, cost = (1,4), returnThreshold = False),\n",
    "               'loan_loss5': make_scorer(score_func=calcCost, greater_is_better=False, needs_proba=True, cost = (1,5), returnThreshold = False),\n",
    "               'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation object used across all modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "kFold = StratifiedKFold(n_splits=3, shuffle=True, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR: Alpha, Penalty, and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR model.\n",
    "sgdLearner1 = SGDClassifier(loss='log', penalty='l1', n_jobs=-1, random_state=rs, learning_rate='constant', \n",
    "                            early_stopping=True,\n",
    "                            n_iter_no_change=20, eta0 = 0.001, alpha=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Values for alpha parameter\n",
    "alpha = [10**i for i in range(-7,0)]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "steps = [('under', RandomUnderSampler(random_state=rs)),('model', sgdLearner1)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate parameters\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Penalty parameters\n",
    "penalty = ['l1', 'l2', 'elasticnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid.\n",
    "pram_grid = [{'model__alpha': alpha, 'model__learning_rate': learning_rate,\n",
    "              'model__penalty': penalty,\n",
    "             'under': [None,\n",
    "                        RandomUnderSampler(sampling_strategy=0.2, random_state=rs),\n",
    "                        RandomUnderSampler(sampling_strategy=0.6, random_state=rs),\n",
    "                        RandomUnderSampler(sampling_strategy=1.0, random_state=rs)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch\n",
    "gridResultsLR2 = GridSearchCV(pipeline, param_grid=pram_grid, scoring=scoringDict2, cv = kFold, verbose=2, \n",
    "                            refit='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('under',\n",
       "                                        RandomUnderSampler(random_state=0,\n",
       "                                                           sampling_strategy=0.4)),\n",
       "                                       ('model',\n",
       "                                        SGDClassifier(alpha=1e-05,\n",
       "                                                      early_stopping=True,\n",
       "                                                      eta0=0.001,\n",
       "                                                      learning_rate='constant',\n",
       "                                                      loss='log', n_jobs=-1,\n",
       "                                                      penalty='l1',\n",
       "                                                      random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'model__alpha': [1e...\n",
       "                      'loan_loss3': make_scorer(calcCost, greater_is_better=False, needs_proba=True, cost=(1, 3), returnThreshold=False),\n",
       "                      'loan_loss4': make_scorer(calcCost, greater_is_better=False, needs_proba=True, cost=(1, 4), returnThreshold=False),\n",
       "                      'loan_loss5': make_scorer(calcCost, greater_is_better=False, needs_proba=True, cost=(1, 5), returnThreshold=False),\n",
       "                      'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True),\n",
       "                      'roc_auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 1415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the models based on the gridsearch\n",
    "gridResultsLR2.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the results\n",
    "results_Grid_LR1_AUC_Orig = gridResultsLR2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 9.79674586, 11.04724288,  5.47792665,  4.04433465,  7.15105669,\n",
       "         8.82697058,  4.43303482,  3.57833783,  8.11972109, 10.15810919,\n",
       "         5.19350402,  4.06870786,  9.06362271, 10.23627957,  5.49236314,\n",
       "         4.15246216,  7.05695073,  8.17616534,  4.46994646,  3.66510955,\n",
       "         9.60930371, 10.17026178,  5.12731425,  4.07798656,  7.62460343,\n",
       "         9.86577566,  5.02318565,  3.95858558,  6.28738268,  8.63107038,\n",
       "         4.79121081,  3.72911819,  8.0638752 ,  9.75060383,  4.95591187,\n",
       "         3.94666918, 27.67504247, 27.61723328, 12.84219265, 10.00343362,\n",
       "        20.78279265, 21.54239726,  9.96506182,  7.67849636, 30.24208244,\n",
       "        30.80185056, 14.62287132, 10.41331577,  7.43676829,  9.43085265,\n",
       "         4.72904785,  3.79046782,  6.10653067,  8.13226048,  4.28875597,\n",
       "         3.42096861,  7.74838869,  9.62516713,  4.87963438,  3.97041043,\n",
       "         9.42461141,  9.89089012,  5.04108787,  4.50844256,  6.04243882,\n",
       "         8.21032159,  4.42383154,  3.61990396,  7.94058323,  9.73725939,\n",
       "         5.22288545,  4.12353388,  7.52215171,  9.51966667,  4.90512355,\n",
       "         3.88214771,  6.30656751,  8.19267154,  4.32399845,  3.48409073,\n",
       "         8.02408202,  9.99565959,  5.02533476,  3.88512325, 27.70102008,\n",
       "        27.73776372, 12.9673864 ,  9.73015181, 20.48841556, 21.73037553,\n",
       "         9.99282527,  7.73161292, 30.25958172, 30.16551987, 14.09545779,\n",
       "        10.43068361,  7.42321761,  9.29429936,  4.73753564,  3.75242098,\n",
       "         6.02962255,  8.09350403,  4.27037024,  3.44761467,  7.83219298,\n",
       "         9.74471132,  4.92433929,  3.90208292,  7.70406493,  9.57854597,\n",
       "         5.14069756,  4.25247288,  6.02859839,  8.51716224,  4.37357489,\n",
       "         3.60222761,  8.00506838,  9.53026064,  5.07937654,  4.25161886,\n",
       "         7.79433767,  9.66781354,  4.85987687,  3.91931136,  6.22545528,\n",
       "         8.24563543,  4.34832899,  3.51218692,  8.08419045,  9.93422643,\n",
       "         5.06993977,  3.95257417, 27.42694696, 27.87443391, 12.73313347,\n",
       "         9.74024844, 21.00398374, 22.05607136, 10.31727489,  7.71984474,\n",
       "        29.69740542, 30.01857583, 13.92687249, 10.3619206 ,  7.33385491,\n",
       "         9.3124427 ,  4.69882806,  3.93446938,  6.89549073,  9.29921166,\n",
       "         4.71515266,  3.62072563,  7.99696668,  9.98274414,  5.1462187 ,\n",
       "         4.01830689,  7.58059756,  9.61615698,  5.03591188,  4.24984717,\n",
       "         6.28799113,  8.94038781,  4.74764752,  3.70149843,  8.43649824,\n",
       "        11.39984425,  5.1940515 ,  4.24717967,  7.91792226,  9.87853026,\n",
       "         5.06697369,  4.06137514,  7.80504219, 11.41832479,  6.30943998,\n",
       "         4.48768775, 10.33268094, 10.96400634,  5.33702199,  4.04436429,\n",
       "        28.10877442, 28.4695313 , 13.15856751, 10.04313723, 21.57381376,\n",
       "        22.6560386 , 10.58702381,  8.81192136, 31.31418228, 31.34714397,\n",
       "        14.44609944, 10.61526473,  7.49006923,  9.60661912,  4.88806979,\n",
       "         3.88530453,  6.24024749,  8.75527438,  4.55361907,  3.56533154,\n",
       "         7.90219975,  9.68257364,  4.9924945 ,  3.9674921 ,  7.53826006,\n",
       "         9.70027129,  4.93032479,  3.91458225,  7.55833729,  8.789229  ,\n",
       "         4.63067166,  3.64445631,  8.04750649,  9.85310205,  5.21687039,\n",
       "         4.13939007,  7.77919571,  9.92632818,  5.19809842,  4.00975188,\n",
       "         6.53450028,  8.83439755,  4.55304257,  3.93510016, 10.18871959,\n",
       "        13.38707217,  6.9463648 ,  5.04363743, 32.12612057, 33.39667328,\n",
       "        15.57559967, 11.94580309, 27.22384159, 29.30534164, 13.21492068,\n",
       "        10.83694196, 32.98889367, 33.23940468, 15.9712925 , 11.13124887,\n",
       "         8.20606629, 11.88398957,  6.12032557,  4.6197927 ,  7.49406322,\n",
       "        11.09484315,  5.52495186,  4.2816062 ,  9.88077943, 11.37224913,\n",
       "         6.29357203,  5.62724304,  9.06241981, 11.29317474,  5.7816058 ,\n",
       "         4.52594876,  6.72645227,  9.54150581,  5.03525686,  3.96136649,\n",
       "         8.70928764, 11.1797893 ,  5.94674261,  4.43159199,  8.79759026,\n",
       "        12.50361792,  6.44148731,  4.74321032,  8.04824026, 10.23015213,\n",
       "         5.71312507,  4.47100186,  9.30016939, 12.03381125,  6.08079425,\n",
       "         4.74693871, 31.09700505, 32.59613864, 15.885264  , 11.26431505,\n",
       "        23.31471682, 24.59667277, 11.91916641,  9.2546316 , 32.13929256,\n",
       "        34.8213462 , 15.81668409, 11.83930763,  8.12594708, 11.25061719,\n",
       "         5.52169681,  4.46281767,  8.23547387,  9.19657755,  5.18935172,\n",
       "         4.80157828, 10.15599887, 10.71536024,  5.37223069,  4.23181311,\n",
       "         8.28071062, 10.85943333,  5.77798804,  4.723176  ,  6.63279231,\n",
       "         9.88361311,  5.59625085,  4.53743839,  8.64827196, 10.24914408,\n",
       "         5.24430497,  4.09868797,  8.26170087, 10.43129373,  5.32698925,\n",
       "         4.39794278,  6.93571011,  9.63777741,  4.78455305,  3.73741841,\n",
       "         7.93430567,  9.72721187,  5.03520226,  4.03017831, 28.80039334,\n",
       "        29.12372796, 13.63984895, 10.30178793, 22.70218952, 23.51795514,\n",
       "        11.43065405,  7.99113464, 30.35175157, 29.32758172, 13.83629473,\n",
       "         9.85065516]),\n",
       " 'std_fit_time': array([0.07544128, 0.49243579, 0.20869054, 0.01681696, 0.13458176,\n",
       "        0.26499028, 0.07711766, 0.04558938, 0.04433566, 0.29213585,\n",
       "        0.25290224, 0.02731426, 1.7455252 , 0.5589972 , 0.48266815,\n",
       "        0.07424337, 1.03668379, 0.0649494 , 0.34394724, 0.03736993,\n",
       "        1.55714913, 0.80148792, 0.29210325, 0.14177363, 0.07978358,\n",
       "        0.35862849, 0.1663852 , 0.0214538 , 0.04979882, 0.34286118,\n",
       "        0.27010187, 0.15456152, 0.05399599, 0.18931455, 0.2393973 ,\n",
       "        0.02246488, 0.03381038, 0.11903644, 0.05179723, 0.29771555,\n",
       "        0.1975685 , 0.05384578, 0.04868626, 0.04865902, 0.0666364 ,\n",
       "        0.66007164, 0.48957431, 0.18485257, 0.13188218, 0.12097   ,\n",
       "        0.03526106, 0.0360086 , 0.07977738, 0.11474004, 0.11568708,\n",
       "        0.03286408, 0.06912859, 0.2534378 , 0.23563232, 0.01512122,\n",
       "        2.06161198, 0.42956633, 0.19403587, 0.37491683, 0.0302007 ,\n",
       "        0.12444294, 0.26856927, 0.03456959, 0.07257176, 0.21040028,\n",
       "        0.31786979, 0.100165  , 0.01870109, 0.11031083, 0.14419367,\n",
       "        0.02501508, 0.03988267, 0.08875129, 0.05716678, 0.02188707,\n",
       "        0.05678605, 0.31025928, 0.22128746, 0.00479392, 0.03559773,\n",
       "        0.07326969, 0.19025603, 0.02045882, 0.18510876, 0.21496859,\n",
       "        0.03435079, 0.02871294, 0.3616553 , 0.30243554, 0.20080775,\n",
       "        0.12660681, 0.07438442, 0.08513255, 0.0340437 , 0.02407808,\n",
       "        0.03677967, 0.08801792, 0.05018905, 0.02421691, 0.07245735,\n",
       "        0.18897974, 0.22083951, 0.0048333 , 0.07403093, 0.11715252,\n",
       "        0.16751409, 0.07119814, 0.08384126, 0.13789931, 0.2612065 ,\n",
       "        0.09974445, 0.0596272 , 0.05135615, 0.09868   , 0.15051238,\n",
       "        0.05278988, 0.31214813, 0.19898159, 0.04711683, 0.08764521,\n",
       "        0.01253645, 0.11452058, 0.04823338, 0.10145739, 0.21474518,\n",
       "        0.20496803, 0.00967129, 0.12122487, 0.36416387, 0.12335837,\n",
       "        0.04459042, 0.20654118, 0.0767596 , 0.29788959, 0.10602332,\n",
       "        0.04584772, 0.30415447, 0.28826527, 0.02125965, 0.0793672 ,\n",
       "        0.13582494, 0.02551204, 0.13170988, 0.14992535, 0.31425365,\n",
       "        0.19880135, 0.06302322, 0.10351061, 0.27874974, 0.25104473,\n",
       "        0.03485237, 0.06160482, 0.17371588, 0.17559176, 0.09709108,\n",
       "        0.05127872, 0.63274749, 0.57129236, 0.09702659, 0.08734749,\n",
       "        1.12082946, 0.3114243 , 0.12792147, 0.02601907, 0.13030105,\n",
       "        0.1479858 , 0.02637342, 0.04805985, 0.87285467, 0.40042575,\n",
       "        0.03283435, 0.34778821, 0.53313768, 0.21255412, 0.02089827,\n",
       "        0.05897495, 0.11718014, 0.13322094, 0.0891884 , 0.20623198,\n",
       "        0.11071655, 0.40966091, 0.21405325, 0.06218885, 0.43006   ,\n",
       "        0.26251466, 0.09368925, 0.14627001, 0.09136211, 0.04508979,\n",
       "        0.05586027, 0.06188783, 0.03396693, 0.13941724, 0.02935864,\n",
       "        0.12546597, 0.12306428, 0.09628306, 0.04809871, 0.06574903,\n",
       "        0.04577798, 0.22106749, 0.0306085 , 1.04146578, 0.28080963,\n",
       "        0.1854597 , 0.06537418, 0.34946614, 0.26799864, 0.33159436,\n",
       "        0.13036518, 0.0703146 , 0.2663848 , 0.0310671 , 0.03398379,\n",
       "        0.03802768, 0.08741674, 0.2256459 , 0.19471671, 0.04294917,\n",
       "        0.64451076, 0.76095654, 0.12357438, 0.1977197 , 0.45206286,\n",
       "        0.943517  , 0.5728918 , 0.86808707, 0.40506366, 0.44183658,\n",
       "        0.10137647, 0.18775543, 0.30111231, 0.4850117 , 0.52471554,\n",
       "        0.69239701, 0.19822408, 0.02867494, 0.04167542, 0.1831863 ,\n",
       "        0.43568519, 0.29004181, 0.20547938, 0.31971991, 0.56631028,\n",
       "        0.25879518, 0.1810525 , 0.1091044 , 0.16554408, 0.2785877 ,\n",
       "        0.19790289, 0.03962232, 0.29576246, 0.09894933, 0.05030046,\n",
       "        0.16207309, 0.38331433, 0.35648539, 0.05987781, 0.09866188,\n",
       "        0.7011376 , 0.56444864, 0.28719001, 0.2844316 , 0.4158728 ,\n",
       "        0.1254684 , 0.068426  , 0.07148313, 0.54197811, 0.16430777,\n",
       "        0.06000857, 0.0989254 , 1.0695468 , 0.86620127, 0.11437763,\n",
       "        0.42186534, 1.01262846, 0.34467243, 0.22894101, 0.28975048,\n",
       "        2.6316643 , 0.7739219 , 0.92445644, 0.32887377, 0.15658083,\n",
       "        0.2389175 , 0.13368792, 0.85568427, 0.15790845, 0.5194394 ,\n",
       "        0.09756709, 1.16209731, 0.13668299, 0.04468004, 0.02438609,\n",
       "        0.09705342, 0.62651818, 0.29699018, 0.14278451, 0.13182298,\n",
       "        0.39942454, 0.04535961, 0.11464717, 0.21518102, 0.15089545,\n",
       "        0.22543225, 0.01144538, 0.05295312, 0.2722717 , 0.17944283,\n",
       "        0.03013608, 0.03500302, 0.1801116 , 0.39482737, 0.12653882,\n",
       "        0.06126399, 0.0763324 , 0.05626347, 0.02891279, 0.19153448,\n",
       "        0.39779751, 0.31393004, 0.20205448, 1.11526145, 0.19090881,\n",
       "        0.20756853, 0.0786073 , 1.07836267, 0.4012435 , 0.14291925,\n",
       "        0.20749517]),\n",
       " 'mean_score_time': array([1.14912566, 1.16344698, 1.13299664, 1.30291406, 1.08410247,\n",
       "        1.09545271, 1.08862726, 1.09478553, 1.07953127, 1.04416124,\n",
       "        1.07590675, 1.08969251, 1.05857786, 1.0971059 , 1.15181398,\n",
       "        1.05272547, 1.04146687, 1.05811707, 1.0125169 , 1.04306912,\n",
       "        1.04464157, 1.03073812, 1.01858775, 1.02183326, 1.01915034,\n",
       "        1.07584214, 1.1538713 , 1.07257414, 1.03615149, 1.20959218,\n",
       "        1.13773322, 1.07464401, 1.03120685, 1.01708285, 1.03553406,\n",
       "        1.08746719, 1.01915034, 1.04731258, 1.06585503, 1.05472708,\n",
       "        1.02246769, 1.05550273, 1.0490702 , 1.07908972, 1.12133296,\n",
       "        1.13540538, 1.07695731, 1.11477772, 1.09086363, 1.01223683,\n",
       "        1.03004742, 1.08705656, 1.02549267, 1.03266422, 1.05366047,\n",
       "        1.06226858, 1.0581154 , 1.05881508, 1.0574642 , 1.09917784,\n",
       "        1.03021248, 1.03695464, 1.05720663, 1.06423267, 1.05958501,\n",
       "        1.06053162, 1.05438193, 1.07925979, 1.04747057, 1.06963849,\n",
       "        1.0409499 , 1.04637257, 1.02358937, 1.029538  , 1.05546244,\n",
       "        1.0752693 , 1.0418938 , 1.00842985, 1.04942266, 1.09394153,\n",
       "        1.06566437, 1.03533228, 1.04491003, 1.03845954, 0.99938679,\n",
       "        1.04274106, 1.05435848, 1.05015914, 1.05199862, 1.03345299,\n",
       "        1.05802496, 1.0818758 , 1.0416712 , 1.04738792, 1.05970486,\n",
       "        1.06107481, 1.06124552, 1.034808  , 1.05108929, 1.06380208,\n",
       "        1.01977022, 1.03752502, 1.0659434 , 1.09920867, 1.11308551,\n",
       "        1.08000882, 1.04472955, 1.10496577, 1.04899891, 1.05104359,\n",
       "        1.07422996, 1.06022422, 1.05808759, 1.06197604, 1.0700659 ,\n",
       "        1.07008394, 1.01628319, 1.04782629, 1.04512286, 1.07636166,\n",
       "        1.04358117, 1.0348254 , 1.04122718, 1.06451066, 1.04174026,\n",
       "        1.02973747, 1.06920036, 1.07544287, 1.0339915 , 1.03690004,\n",
       "        1.04062017, 1.05278079, 1.03092655, 1.04750824, 1.03726315,\n",
       "        1.05639029, 1.0438784 , 1.04894845, 1.05109278, 1.0759999 ,\n",
       "        1.08081492, 1.05391645, 1.02520927, 1.05506277, 1.05164989,\n",
       "        1.03232861, 1.08077979, 1.12216187, 1.3752528 , 1.08524497,\n",
       "        1.06459284, 1.08184822, 1.05777168, 1.09020662, 1.0583628 ,\n",
       "        1.06465761, 1.04748789, 1.06684701, 1.08163071, 1.10102765,\n",
       "        1.0716459 , 1.06238468, 1.05249866, 1.19953712, 1.10218151,\n",
       "        1.04077999, 1.08385698, 1.0839719 , 1.09139291, 1.04861116,\n",
       "        1.05951508, 1.34372568, 1.2881786 , 1.59275738, 1.45360351,\n",
       "        1.46623119, 1.07188861, 1.04629683, 1.05968142, 1.07943837,\n",
       "        1.05684288, 1.06284173, 1.10032042, 1.07733695, 1.04749958,\n",
       "        1.10097067, 1.10360273, 1.0923365 , 1.08494361, 1.08017222,\n",
       "        1.06309191, 1.07035653, 1.04395922, 1.05313293, 1.07372197,\n",
       "        1.10592922, 1.095644  , 1.05697831, 1.06929517, 1.08514611,\n",
       "        1.04907799, 1.04398664, 1.07169708, 1.08399494, 1.0589304 ,\n",
       "        1.05718263, 1.06558228, 1.07436999, 1.09089613, 1.10764519,\n",
       "        1.0964911 , 1.10149384, 1.0717144 , 1.1057895 , 1.05847462,\n",
       "        1.09664639, 1.04552364, 1.06928245, 1.09000643, 1.09871554,\n",
       "        1.10896913, 1.07019846, 1.06857316, 1.35019588, 1.4516685 ,\n",
       "        1.35618973, 1.27471685, 1.47562146, 1.36325828, 1.18737769,\n",
       "        1.11853584, 1.24614302, 1.34373403, 1.2789526 , 1.4176534 ,\n",
       "        1.28668571, 1.06778026, 1.09957496, 1.14390747, 1.16135963,\n",
       "        1.20905264, 1.35284774, 1.14145875, 1.27167416, 1.40385556,\n",
       "        1.30025117, 1.30230165, 1.3672506 , 1.13239511, 1.36125064,\n",
       "        1.40823293, 1.35357857, 1.34073242, 1.28154993, 1.24268691,\n",
       "        1.14530214, 1.1584096 , 1.16232753, 1.28650633, 1.34586763,\n",
       "        1.23595715, 1.16972645, 1.17009306, 1.20535612, 1.1995333 ,\n",
       "        1.37887828, 1.27397903, 1.56460174, 1.11565765, 1.25238927,\n",
       "        1.24759213, 1.18085178, 1.09898162, 1.14918645, 1.35394669,\n",
       "        1.12764581, 1.23474352, 1.1827236 , 1.2247858 , 1.25113368,\n",
       "        1.08650867, 1.17383146, 1.19352237, 1.2309711 , 1.21457346,\n",
       "        1.22964795, 1.24237506, 1.18145275, 0.92631642, 1.06548262,\n",
       "        1.11239942, 1.07820773, 1.17033704, 1.2701238 , 1.46227288,\n",
       "        1.52723662, 1.18125232, 1.21911677, 1.17786535, 1.20302137,\n",
       "        0.94608903, 1.07544939, 1.13369759, 1.08067664, 1.07487925,\n",
       "        1.36546644, 1.48738098, 1.4387215 , 1.10098704, 1.1087629 ,\n",
       "        1.13671819, 1.13812065, 0.972471  , 0.94365756, 1.0583845 ,\n",
       "        1.01266074, 1.28186425, 1.16096703, 1.16691057, 1.11561084,\n",
       "        1.06588062, 1.09041707, 1.08345954, 1.11297599, 0.90406108,\n",
       "        0.90499345, 0.97874522, 0.99156133, 1.10429708, 1.11970353,\n",
       "        1.08193692, 1.0838654 , 1.08584563, 1.08551486, 1.09008511,\n",
       "        1.05582094]),\n",
       " 'std_score_time': array([0.03221339, 0.04871532, 0.02817845, 0.14193722, 0.02849872,\n",
       "        0.01296926, 0.02510407, 0.00538182, 0.01116008, 0.02514528,\n",
       "        0.02292039, 0.0463273 , 0.02267456, 0.06724349, 0.06795217,\n",
       "        0.01586122, 0.02424736, 0.01907549, 0.01227623, 0.02079752,\n",
       "        0.01606796, 0.02136671, 0.00810838, 0.0441181 , 0.02365022,\n",
       "        0.0271207 , 0.1098595 , 0.02211247, 0.01886912, 0.13115936,\n",
       "        0.11846426, 0.01001813, 0.01908929, 0.01226643, 0.01631196,\n",
       "        0.03255159, 0.00693235, 0.01544971, 0.01894978, 0.02962027,\n",
       "        0.00776148, 0.01534588, 0.01007428, 0.01601985, 0.0799028 ,\n",
       "        0.15647106, 0.02244012, 0.03815575, 0.04125582, 0.01866559,\n",
       "        0.01308291, 0.04839103, 0.0079521 , 0.00401384, 0.02348353,\n",
       "        0.00107361, 0.02857396, 0.01844668, 0.02744967, 0.02385393,\n",
       "        0.02021949, 0.03399134, 0.01937612, 0.00812829, 0.03175744,\n",
       "        0.01243091, 0.01644018, 0.02533749, 0.02297913, 0.01772932,\n",
       "        0.02201736, 0.01266845, 0.01375964, 0.00689754, 0.00320434,\n",
       "        0.00590984, 0.00649018, 0.02307256, 0.0124427 , 0.03715053,\n",
       "        0.03121757, 0.01720957, 0.0084241 , 0.00168611, 0.01487184,\n",
       "        0.00803815, 0.0027992 , 0.0222445 , 0.00766692, 0.02296312,\n",
       "        0.0329202 , 0.02053847, 0.02272966, 0.01882786, 0.02163078,\n",
       "        0.02451016, 0.01311504, 0.0203125 , 0.01376288, 0.02024757,\n",
       "        0.03841046, 0.01683935, 0.03649708, 0.01451745, 0.02154904,\n",
       "        0.01501625, 0.0077152 , 0.01855213, 0.03141062, 0.01355289,\n",
       "        0.01893487, 0.02204997, 0.00522507, 0.03995644, 0.01690255,\n",
       "        0.01240166, 0.02434099, 0.02498927, 0.02231881, 0.03374117,\n",
       "        0.01004458, 0.00372577, 0.01412377, 0.0060533 , 0.01085743,\n",
       "        0.00973463, 0.01661997, 0.01121783, 0.01885715, 0.02166767,\n",
       "        0.00141071, 0.01167188, 0.01520204, 0.01223362, 0.01250331,\n",
       "        0.01367065, 0.01558285, 0.0318335 , 0.01663729, 0.03904608,\n",
       "        0.03546941, 0.01706117, 0.01484429, 0.00975629, 0.0055235 ,\n",
       "        0.01673886, 0.02454798, 0.02578786, 0.19731631, 0.04601254,\n",
       "        0.00349042, 0.00843801, 0.00773173, 0.01918523, 0.02676184,\n",
       "        0.02478861, 0.01894123, 0.0073445 , 0.00609772, 0.01139602,\n",
       "        0.01245936, 0.00595092, 0.00889486, 0.10698952, 0.01241406,\n",
       "        0.00912125, 0.02480214, 0.04626885, 0.0468052 , 0.01792639,\n",
       "        0.00855816, 0.18509287, 0.04218007, 0.21864333, 0.09093478,\n",
       "        0.06488603, 0.03663886, 0.02975658, 0.00200004, 0.01489979,\n",
       "        0.02613428, 0.00590244, 0.05078811, 0.01992406, 0.01302902,\n",
       "        0.05709138, 0.04000548, 0.00525175, 0.02263767, 0.03508601,\n",
       "        0.00804657, 0.01565221, 0.02566808, 0.03201989, 0.01803041,\n",
       "        0.0287107 , 0.03602474, 0.01708143, 0.02499408, 0.01791647,\n",
       "        0.02068744, 0.0217161 , 0.01584328, 0.00946267, 0.03373227,\n",
       "        0.01023157, 0.01635599, 0.0117689 , 0.03722676, 0.0186732 ,\n",
       "        0.03275645, 0.02412536, 0.01457702, 0.01517351, 0.01673133,\n",
       "        0.02663813, 0.02008251, 0.00811546, 0.02544122, 0.01063659,\n",
       "        0.00836523, 0.00430934, 0.03150711, 0.14194699, 0.07080609,\n",
       "        0.19447464, 0.16103888, 0.06702814, 0.13052691, 0.08655893,\n",
       "        0.05005006, 0.10555955, 0.11803072, 0.09218084, 0.20648817,\n",
       "        0.05904184, 0.02734767, 0.01386991, 0.01744301, 0.01511584,\n",
       "        0.10656418, 0.04172128, 0.09066001, 0.00944187, 0.14165596,\n",
       "        0.08902447, 0.0575624 , 0.11615132, 0.04385129, 0.18004711,\n",
       "        0.03448352, 0.10607556, 0.04088836, 0.09200861, 0.12235039,\n",
       "        0.03475958, 0.07745684, 0.03829814, 0.09121006, 0.03819368,\n",
       "        0.04196285, 0.00918806, 0.04180256, 0.03559291, 0.11815534,\n",
       "        0.18407823, 0.12301942, 0.20206949, 0.0577882 , 0.10892918,\n",
       "        0.10631703, 0.05018738, 0.07295256, 0.01992224, 0.19018239,\n",
       "        0.00249381, 0.05140051, 0.03441463, 0.01407987, 0.04345863,\n",
       "        0.03417831, 0.08468514, 0.03990407, 0.02592092, 0.04525344,\n",
       "        0.03710734, 0.03233654, 0.08978079, 0.06291637, 0.07201098,\n",
       "        0.07728808, 0.1084877 , 0.0169575 , 0.19990903, 0.19099813,\n",
       "        0.04200256, 0.04999884, 0.09500644, 0.04734214, 0.03593872,\n",
       "        0.02638018, 0.1750994 , 0.03269182, 0.07429331, 0.00727644,\n",
       "        0.0919727 , 0.07791571, 0.03781857, 0.02925099, 0.01791978,\n",
       "        0.02414436, 0.01060904, 0.00974688, 0.01569171, 0.07012632,\n",
       "        0.01910267, 0.08536468, 0.06049884, 0.07220143, 0.01264396,\n",
       "        0.01094117, 0.03598148, 0.0137428 , 0.01800683, 0.03994266,\n",
       "        0.01358702, 0.00033829, 0.02989158, 0.04497593, 0.02575999,\n",
       "        0.01121897, 0.02021959, 0.03001247, 0.013149  , 0.0102093 ,\n",
       "        0.02149008]),\n",
       " 'param_model__alpha': masked_array(data=[1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
       "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
       "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
       "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
       "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
       "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
       "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
       "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
       "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
       "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
       "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
       "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
       "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
       "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
       "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
       "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
       "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
       "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__learning_rate': masked_array(data=['constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'optimal', 'optimal', 'optimal',\n",
       "                    'optimal', 'optimal', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'invscaling', 'invscaling',\n",
       "                    'invscaling', 'invscaling', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'adaptive', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__penalty': masked_array(data=['l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_under': masked_array(data=[None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-07,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-06,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 1e-05,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.0001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.001,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.01,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'constant',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'optimal',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'invscaling',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l1',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'l2',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': None},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__alpha': 0.1,\n",
       "   'model__learning_rate': 'adaptive',\n",
       "   'model__penalty': 'elasticnet',\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)}],\n",
       " 'split0_test_accuracy': array([0.84684327, 0.84674079, 0.77896054, 0.61016747, 0.84684327,\n",
       "        0.84674506, 0.77896481, 0.61015466, 0.84684327, 0.84674506,\n",
       "        0.77896481, 0.61015893, 0.75551029, 0.82620261, 0.38146782,\n",
       "        0.63070991, 0.74657341, 0.81112136, 0.52414197, 0.65340865,\n",
       "        0.69313145, 0.78691962, 0.38146782, 0.48262581, 0.8467963 ,\n",
       "        0.84682619, 0.79623652, 0.59449696, 0.8467963 , 0.84682619,\n",
       "        0.79623652, 0.59449696, 0.8467963 , 0.84682619, 0.79623652,\n",
       "        0.59449696, 0.84678776, 0.84677495, 0.77447715, 0.62023587,\n",
       "        0.84678776, 0.84677495, 0.77448996, 0.62023587, 0.84678776,\n",
       "        0.84677495, 0.77448142, 0.62023587, 0.84684754, 0.84674506,\n",
       "        0.77894346, 0.61020589, 0.84684327, 0.84674506, 0.77896054,\n",
       "        0.6101632 , 0.84684754, 0.84674079, 0.77897762, 0.61018027,\n",
       "        0.76310643, 0.8392514 , 0.24767077, 0.56778879, 0.73939572,\n",
       "        0.83346997, 0.384888  , 0.66805438, 0.77216714, 0.81476358,\n",
       "        0.39920922, 0.4593421 , 0.8467963 , 0.84682619, 0.79623652,\n",
       "        0.59449696, 0.8467963 , 0.84682619, 0.79623652, 0.59449696,\n",
       "        0.8467963 , 0.84682619, 0.79623652, 0.59449696, 0.84678776,\n",
       "        0.84677922, 0.7744558 , 0.62024014, 0.84678776, 0.84677922,\n",
       "        0.77449423, 0.62023587, 0.84678776, 0.84677922, 0.77446861,\n",
       "        0.62024014, 0.84683473, 0.84677068, 0.77921673, 0.61045782,\n",
       "        0.84684327, 0.84674506, 0.77898189, 0.61016747, 0.84684754,\n",
       "        0.84674079, 0.77893065, 0.61024859, 0.8464846 , 0.84684754,\n",
       "        0.79492993, 0.70440397, 0.84003279, 0.84534881, 0.73745719,\n",
       "        0.46204921, 0.84603199, 0.8466682 , 0.79004518, 0.47646009,\n",
       "        0.8467963 , 0.84682619, 0.79623652, 0.59450123, 0.8467963 ,\n",
       "        0.84682619, 0.79623652, 0.59449696, 0.8467963 , 0.84682619,\n",
       "        0.79623652, 0.59450123, 0.84678349, 0.84680057, 0.7745839 ,\n",
       "        0.62021025, 0.84677922, 0.84677922, 0.77449423, 0.62021025,\n",
       "        0.84677922, 0.84678349, 0.77447715, 0.6202316 , 0.84684754,\n",
       "        0.84678776, 0.78010914, 0.61172598, 0.84684754, 0.84674506,\n",
       "        0.77905021, 0.61022724, 0.84685608, 0.84677495, 0.77980171,\n",
       "        0.61037669, 0.84687316, 0.84685181, 0.79865755, 0.65899367,\n",
       "        0.84682192, 0.84647606, 0.7431575 , 0.56060684, 0.84686889,\n",
       "        0.84657854, 0.75849922, 0.58172572, 0.8467963 , 0.84682192,\n",
       "        0.79627068, 0.59447988, 0.8467963 , 0.84682619, 0.79624079,\n",
       "        0.59449696, 0.8467963 , 0.84682619, 0.79624506, 0.59450123,\n",
       "        0.84680484, 0.84674933, 0.77494684, 0.61981315, 0.84677495,\n",
       "        0.84678776, 0.77451131, 0.62021879, 0.84679203, 0.84680484,\n",
       "        0.77462233, 0.62019317, 0.84679203, 0.84694575, 0.78356775,\n",
       "        0.61217004, 0.84685181, 0.84677922, 0.77967361, 0.60998386,\n",
       "        0.84684327, 0.84685608, 0.77946865, 0.61191812, 0.84685608,\n",
       "        0.84686035, 0.78315784, 0.61148686, 0.84684327, 0.84682619,\n",
       "        0.78920828, 0.60870716, 0.84680057, 0.84689878, 0.78970785,\n",
       "        0.61119224, 0.84679203, 0.84678776, 0.79699656, 0.59441584,\n",
       "        0.8467963 , 0.84681765, 0.79626641, 0.59448842, 0.8467963 ,\n",
       "        0.84681338, 0.79633473, 0.59448415, 0.8469244 , 0.84681338,\n",
       "        0.77920819, 0.61834858, 0.846839  , 0.84684754, 0.77513472,\n",
       "        0.61977899, 0.84684327, 0.84684754, 0.77576239, 0.61924525,\n",
       "        0.84684754, 0.8468817 , 0.79277791, 0.62097029, 0.84686889,\n",
       "        0.84684754, 0.78343965, 0.61010769, 0.84684327, 0.84687316,\n",
       "        0.78790169, 0.61142281, 0.84685181, 0.84687316, 0.78830733,\n",
       "        0.62758435, 0.84689451, 0.84691159, 0.78105278, 0.6177209 ,\n",
       "        0.84687743, 0.84684754, 0.7854038 , 0.61803687, 0.84683046,\n",
       "        0.84680484, 0.80496845, 0.59044911, 0.8467963 , 0.84680911,\n",
       "        0.79658665, 0.59441584, 0.84678776, 0.84677495, 0.79774379,\n",
       "        0.5942792 , 0.84686889, 0.84687743, 0.78915277, 0.62651688,\n",
       "        0.8468817 , 0.84688597, 0.77908436, 0.61912997, 0.84690305,\n",
       "        0.84684327, 0.7835037 , 0.61775506, 0.84683046, 0.84683046,\n",
       "        0.84683046, 0.63225988, 0.84680484, 0.84682192, 0.80346544,\n",
       "        0.61049198, 0.84683046, 0.84683473, 0.82160394, 0.61401464,\n",
       "        0.84683046, 0.84683046, 0.84681765, 0.63405751, 0.84683046,\n",
       "        0.84683046, 0.79836719, 0.61693097, 0.84683046, 0.84683046,\n",
       "        0.81662952, 0.62477049, 0.84683046, 0.84683046, 0.84683046,\n",
       "        0.5847488 , 0.84680911, 0.84680484, 0.79964389, 0.59382232,\n",
       "        0.84683046, 0.84683046, 0.81271403, 0.5835148 , 0.84683046,\n",
       "        0.84683046, 0.84681765, 0.63405751, 0.84682619, 0.84683046,\n",
       "        0.79821775, 0.61804968, 0.84683046, 0.84683046, 0.81693695,\n",
       "        0.623323  ]),\n",
       " 'split1_test_accuracy': array([0.84651876, 0.84674933, 0.77060863, 0.62050487, 0.84651876,\n",
       "        0.84674506, 0.77059582, 0.62052195, 0.84651876, 0.84674506,\n",
       "        0.77059582, 0.62052195, 0.78951998, 0.84072025, 0.72900281,\n",
       "        0.71134681, 0.82001981, 0.66788786, 0.62267398, 0.76481439,\n",
       "        0.83475094, 0.79390943, 0.78674882, 0.6478578 , 0.84681765,\n",
       "        0.84682619, 0.7929487 , 0.59606401, 0.84681765, 0.84682619,\n",
       "        0.7929487 , 0.59606401, 0.84681765, 0.84682619, 0.7929487 ,\n",
       "        0.59606401, 0.84655292, 0.84634796, 0.7706556 , 0.62063724,\n",
       "        0.84655292, 0.84634796, 0.77065987, 0.62065005, 0.84655292,\n",
       "        0.84634796, 0.77065987, 0.62065005, 0.84651876, 0.8467536 ,\n",
       "        0.77062571, 0.62062016, 0.84651876, 0.84674506, 0.77059582,\n",
       "        0.62052195, 0.84652303, 0.84674933, 0.7706129 , 0.62050487,\n",
       "        0.76984005, 0.84674506, 0.81406331, 0.34145467, 0.76516879,\n",
       "        0.64015491, 0.75338389, 0.68407074, 0.79682149, 0.62780639,\n",
       "        0.71508296, 0.70734165, 0.84681765, 0.84682619, 0.7929487 ,\n",
       "        0.59605974, 0.84681765, 0.84682619, 0.7929487 , 0.59606401,\n",
       "        0.84681765, 0.84682619, 0.7929487 , 0.59606401, 0.84655292,\n",
       "        0.84635223, 0.77064279, 0.62064151, 0.84655292, 0.84634796,\n",
       "        0.77065133, 0.62064151, 0.84655719, 0.84634796, 0.77065133,\n",
       "        0.62064151, 0.84654011, 0.84674079, 0.77088617, 0.62074399,\n",
       "        0.84652303, 0.84674506, 0.77058728, 0.6205006 , 0.84652303,\n",
       "        0.84674506, 0.77062571, 0.6206287 , 0.83917027, 0.84680484,\n",
       "        0.80238516, 0.77716718, 0.83439654, 0.8188584 , 0.76441302,\n",
       "        0.75104826, 0.83896959, 0.84677922, 0.80306407, 0.75901588,\n",
       "        0.84681765, 0.84682619, 0.79295297, 0.59605547, 0.84681765,\n",
       "        0.84682619, 0.7929487 , 0.59606401, 0.84681765, 0.84682619,\n",
       "        0.7929487 , 0.5960512 , 0.84655292, 0.84636077, 0.77065133,\n",
       "        0.6206287 , 0.84656146, 0.84634796, 0.77063425, 0.62065859,\n",
       "        0.84656146, 0.84634796, 0.77062144, 0.62064151, 0.84647606,\n",
       "        0.84674933, 0.77159925, 0.61973202, 0.84651876, 0.84674079,\n",
       "        0.77064706, 0.62046644, 0.84654865, 0.84674506, 0.77094595,\n",
       "        0.62053049, 0.84548544, 0.84682619, 0.79684711, 0.58564548,\n",
       "        0.84563916, 0.84679203, 0.79484026, 0.68520653, 0.84555803,\n",
       "        0.84682619, 0.78720997, 0.65703806, 0.84681765, 0.84682192,\n",
       "        0.79302129, 0.59603839, 0.84681765, 0.84682619, 0.79295297,\n",
       "        0.59606401, 0.84681765, 0.84682619, 0.79295297, 0.59605547,\n",
       "        0.84655719, 0.8463138 , 0.77097157, 0.62051768, 0.84656573,\n",
       "        0.84636931, 0.77062998, 0.62059454, 0.84658281, 0.84636504,\n",
       "        0.770741  , 0.62049633, 0.84658281, 0.84674506, 0.77657367,\n",
       "        0.61905738, 0.84654865, 0.84677922, 0.77106124, 0.61977472,\n",
       "        0.84650595, 0.84674079, 0.77208601, 0.61901468, 0.84668528,\n",
       "        0.84665112, 0.77804251, 0.61434769, 0.84669382, 0.84645898,\n",
       "        0.77121922, 0.61938616, 0.84665112, 0.84670663, 0.77237637,\n",
       "        0.61391643, 0.84683046, 0.84680484, 0.7936148 , 0.59604266,\n",
       "        0.84681765, 0.84682619, 0.7929914 , 0.5960512 , 0.84682192,\n",
       "        0.84681765, 0.79308107, 0.59602132, 0.84670663, 0.84635223,\n",
       "        0.77503224, 0.61955696, 0.84657427, 0.84638639, 0.7711381 ,\n",
       "        0.62047498, 0.84657427, 0.84642055, 0.77176577, 0.61995833,\n",
       "        0.846839  , 0.84683473, 0.79256014, 0.62512062, 0.84665966,\n",
       "        0.84657854, 0.77539518, 0.61812654, 0.84662977, 0.84684754,\n",
       "        0.7812919 , 0.61825464, 0.84684327, 0.84674933, 0.78632183,\n",
       "        0.62721287, 0.84678349, 0.84671517, 0.77714156, 0.6195655 ,\n",
       "        0.84678349, 0.8466682 , 0.78124066, 0.61976191, 0.84681338,\n",
       "        0.84682192, 0.80183008, 0.59140129, 0.84682619, 0.84680911,\n",
       "        0.79327321, 0.59601278, 0.84683473, 0.84680484, 0.79430226,\n",
       "        0.59594019, 0.846839  , 0.84677922, 0.78630048, 0.62690971,\n",
       "        0.84673652, 0.84657   , 0.77525854, 0.61987293, 0.84677068,\n",
       "        0.84656146, 0.77966934, 0.61930503, 0.84683046, 0.84683046,\n",
       "        0.84683046, 0.63350242, 0.84682192, 0.84682192, 0.79689408,\n",
       "        0.61544078, 0.84681765, 0.84682192, 0.82029736, 0.62018036,\n",
       "        0.84683046, 0.84683046, 0.84685181, 0.63350242, 0.84680911,\n",
       "        0.8467963 , 0.79586077, 0.6185578 , 0.84682192, 0.84681765,\n",
       "        0.81483616, 0.62366032, 0.84683046, 0.84683046, 0.84683046,\n",
       "        0.58503915, 0.84682619, 0.84681338, 0.79620236, 0.59538937,\n",
       "        0.84682192, 0.84681338, 0.81069864, 0.58532951, 0.84683046,\n",
       "        0.84683046, 0.84685181, 0.63350242, 0.84680911, 0.84679203,\n",
       "        0.79548075, 0.61950999, 0.84682192, 0.84682192, 0.8148447 ,\n",
       "        0.6237201 ]),\n",
       " 'split2_test_accuracy': array([0.84662485, 0.84668036, 0.78056081, 0.61689091, 0.84662912,\n",
       "        0.84668036, 0.78055227, 0.61689945, 0.84662912, 0.84668036,\n",
       "        0.78056081, 0.61690372, 0.65230127, 0.79340897, 0.69990222,\n",
       "        0.56257339, 0.66135348, 0.77192705, 0.66553799, 0.54595063,\n",
       "        0.65014069, 0.7889341 , 0.67543991, 0.59595127, 0.84680846,\n",
       "        0.84680846, 0.79477107, 0.5970401 , 0.84680846, 0.84680846,\n",
       "        0.79477107, 0.5970401 , 0.84680846, 0.84680846, 0.79477107,\n",
       "        0.5970401 , 0.84674014, 0.84659069, 0.7735667 , 0.62292429,\n",
       "        0.84674014, 0.84659069, 0.77356243, 0.62292002, 0.84674014,\n",
       "        0.84659069, 0.77356243, 0.62292002, 0.84662058, 0.84667609,\n",
       "        0.78062059, 0.61681405, 0.84662912, 0.84668036, 0.78056508,\n",
       "        0.61690799, 0.84662485, 0.84668036, 0.78056935, 0.61688237,\n",
       "        0.8393233 , 0.83170579, 0.6698079 , 0.59232612, 0.67433827,\n",
       "        0.8017481 , 0.74262266, 0.62199345, 0.70041888, 0.80159438,\n",
       "        0.64634047, 0.64327895, 0.84680846, 0.84680846, 0.79477107,\n",
       "        0.59703583, 0.84680846, 0.84680846, 0.79477107, 0.5970401 ,\n",
       "        0.84680846, 0.84680846, 0.79477107, 0.5970401 , 0.84674014,\n",
       "        0.84659069, 0.77355389, 0.62295845, 0.84674014, 0.84659069,\n",
       "        0.7735667 , 0.62292429, 0.84674014, 0.84659069, 0.7735667 ,\n",
       "        0.62292429, 0.84666755, 0.84670171, 0.7809024 , 0.61713429,\n",
       "        0.84662912, 0.84668036, 0.7805907 , 0.61689945, 0.84661204,\n",
       "        0.84668463, 0.78065048, 0.61682259, 0.84458383, 0.84669317,\n",
       "        0.54069437, 0.66675491, 0.83005333, 0.83525835, 0.78293915,\n",
       "        0.62183973, 0.84012178, 0.8467316 , 0.82399433, 0.67000431,\n",
       "        0.84680846, 0.84680846, 0.79478388, 0.59702729, 0.84680846,\n",
       "        0.84680846, 0.79477107, 0.5970401 , 0.84680846, 0.84680846,\n",
       "        0.79477107, 0.59703583, 0.84674441, 0.84661204, 0.77353681,\n",
       "        0.6229371 , 0.84674014, 0.84659069, 0.77358378, 0.62293283,\n",
       "        0.84674441, 0.8466035 , 0.77357951, 0.6229371 , 0.84674441,\n",
       "        0.84677857, 0.78046687, 0.61782602, 0.84659496, 0.8466889 ,\n",
       "        0.78055654, 0.61682259, 0.84669317, 0.84670171, 0.78091094,\n",
       "        0.61749297, 0.84571963, 0.84683835, 0.81649637, 0.65909042,\n",
       "        0.84565558, 0.84677003, 0.81472436, 0.66237825, 0.84569401,\n",
       "        0.84685543, 0.81564666, 0.64616114, 0.84681273, 0.84681273,\n",
       "        0.79483512, 0.59701021, 0.84680846, 0.84680846, 0.79477107,\n",
       "        0.59703156, 0.84680846, 0.84680846, 0.79478388, 0.59701021,\n",
       "        0.84680846, 0.84665047, 0.7738784 , 0.62280046, 0.84673587,\n",
       "        0.84658642, 0.77357097, 0.62297553, 0.84674868, 0.84661631,\n",
       "        0.77361794, 0.62301396, 0.84672306, 0.84667182, 0.78351132,\n",
       "        0.61742892, 0.84665047, 0.84675722, 0.78112871, 0.61648527,\n",
       "        0.84675722, 0.84683835, 0.78167526, 0.61681405, 0.84674014,\n",
       "        0.84659923, 0.78329355, 0.62011896, 0.84676149, 0.84670171,\n",
       "        0.78103904, 0.61713856, 0.84680846, 0.84677003, 0.78182043,\n",
       "        0.61824874, 0.84682127, 0.84679565, 0.79559089, 0.59679671,\n",
       "        0.84681273, 0.84681273, 0.79479242, 0.59701448, 0.84681273,\n",
       "        0.84680846, 0.79489063, 0.59702302, 0.84677003, 0.84666328,\n",
       "        0.77747794, 0.62110958, 0.84677857, 0.84664193, 0.7742328 ,\n",
       "        0.62258697, 0.84683835, 0.84668036, 0.77468968, 0.62231369,\n",
       "        0.84682554, 0.84687677, 0.79215361, 0.62540511, 0.84678711,\n",
       "        0.84683408, 0.78474532, 0.61547757, 0.84680419, 0.84679138,\n",
       "        0.78826373, 0.61704462, 0.84682554, 0.84684689, 0.7878837 ,\n",
       "        0.63069126, 0.84682981, 0.84679138, 0.77860092, 0.62166467,\n",
       "        0.84682554, 0.84672306, 0.78263599, 0.62176287, 0.84682554,\n",
       "        0.84682554, 0.80272591, 0.59146787, 0.84682554, 0.84679565,\n",
       "        0.79509131, 0.59695897, 0.84682127, 0.84679565, 0.79615025,\n",
       "        0.59673267, 0.84683835, 0.84683408, 0.78816552, 0.62695935,\n",
       "        0.84684262, 0.84674014, 0.77726871, 0.62169456, 0.84677857,\n",
       "        0.84672733, 0.7816411 , 0.62062708, 0.84682981, 0.84682981,\n",
       "        0.846817  , 0.63269384, 0.84684262, 0.84684262, 0.80313155,\n",
       "        0.61558005, 0.84682981, 0.84682981, 0.81983971, 0.61861595,\n",
       "        0.84682981, 0.84682981, 0.84681273, 0.63439754, 0.84683408,\n",
       "        0.84682127, 0.79773866, 0.62088754, 0.84682981, 0.84682981,\n",
       "        0.81595836, 0.62374838, 0.84682981, 0.84682981, 0.84682981,\n",
       "        0.58556258, 0.84683408, 0.84682127, 0.79799485, 0.59648074,\n",
       "        0.84682981, 0.84682981, 0.81090279, 0.5843969 , 0.84682981,\n",
       "        0.84682981, 0.84681273, 0.63439754, 0.84683835, 0.84682127,\n",
       "        0.79738852, 0.62033673, 0.84682981, 0.84682981, 0.81590712,\n",
       "        0.62356051]),\n",
       " 'mean_test_accuracy': array([0.84666229, 0.84672349, 0.77670999, 0.61585442, 0.84666372,\n",
       "        0.84672349, 0.7767043 , 0.61585868, 0.84666372, 0.84672349,\n",
       "        0.77670715, 0.61586153, 0.73244385, 0.82011061, 0.60345762,\n",
       "        0.6348767 , 0.7426489 , 0.75031209, 0.60411798, 0.65472456,\n",
       "        0.72600769, 0.78992105, 0.61455218, 0.57547829, 0.84680747,\n",
       "        0.84682028, 0.7946521 , 0.59586703, 0.84680747, 0.84682028,\n",
       "        0.7946521 , 0.59586703, 0.84680747, 0.84682028, 0.7946521 ,\n",
       "        0.59586703, 0.84669361, 0.8465712 , 0.77289982, 0.6212658 ,\n",
       "        0.84669361, 0.8465712 , 0.77290409, 0.62126865, 0.84669361,\n",
       "        0.8465712 , 0.77290124, 0.62126865, 0.84666229, 0.84672492,\n",
       "        0.77672992, 0.61588003, 0.84666372, 0.84672349, 0.77670715,\n",
       "        0.61586438, 0.84666514, 0.84672349, 0.77671996, 0.61585584,\n",
       "        0.7907566 , 0.83923408, 0.57718066, 0.50052319, 0.72630093,\n",
       "        0.75845766, 0.62696485, 0.65803952, 0.75646917, 0.74805478,\n",
       "        0.58687755, 0.6033209 , 0.84680747, 0.84682028, 0.7946521 ,\n",
       "        0.59586418, 0.84680747, 0.84682028, 0.7946521 , 0.59586703,\n",
       "        0.84680747, 0.84682028, 0.7946521 , 0.59586703, 0.84669361,\n",
       "        0.84657405, 0.77288416, 0.62128003, 0.84669361, 0.84657262,\n",
       "        0.77290409, 0.62126722, 0.84669503, 0.84657262, 0.77289555,\n",
       "        0.62126865, 0.8466808 , 0.84673773, 0.77700177, 0.61611203,\n",
       "        0.84666514, 0.84672349, 0.77671996, 0.61585584, 0.84666087,\n",
       "        0.84672349, 0.77673561, 0.61589996, 0.8434129 , 0.84678185,\n",
       "        0.71266982, 0.71610869, 0.83482755, 0.83315519, 0.76160312,\n",
       "        0.61164573, 0.84170779, 0.84672634, 0.80570119, 0.63516009,\n",
       "        0.84680747, 0.84682028, 0.79465779, 0.59586133, 0.84680747,\n",
       "        0.84682028, 0.7946521 , 0.59586703, 0.84680747, 0.84682028,\n",
       "        0.7946521 , 0.59586276, 0.84669361, 0.84659113, 0.77292401,\n",
       "        0.62125868, 0.84669361, 0.84657262, 0.77290409, 0.62126722,\n",
       "        0.84669503, 0.84657832, 0.7728927 , 0.62127007, 0.84668934,\n",
       "        0.84677189, 0.77739175, 0.61642801, 0.84665375, 0.84672492,\n",
       "        0.77675127, 0.61583876, 0.8466993 , 0.84674057, 0.77721953,\n",
       "        0.61613338, 0.84602608, 0.84683878, 0.80400034, 0.63457653,\n",
       "        0.84603889, 0.84667937, 0.78424071, 0.63606388, 0.84604031,\n",
       "        0.84675338, 0.78711861, 0.62830831, 0.84680889, 0.84681886,\n",
       "        0.79470903, 0.59584283, 0.84680747, 0.84682028, 0.79465494,\n",
       "        0.59586418, 0.84680747, 0.84682028, 0.79466064, 0.59585564,\n",
       "        0.84672349, 0.8465712 , 0.7732656 , 0.62104376, 0.84669218,\n",
       "        0.84658116, 0.77290409, 0.62126295, 0.84670784, 0.8465954 ,\n",
       "        0.77299375, 0.62123449, 0.8466993 , 0.84678754, 0.78121758,\n",
       "        0.61621878, 0.84668364, 0.84677189, 0.77728785, 0.61541462,\n",
       "        0.84670214, 0.84681174, 0.77774331, 0.61591562, 0.8467605 ,\n",
       "        0.84670357, 0.78149797, 0.61531784, 0.84676619, 0.84666229,\n",
       "        0.78048885, 0.6150773 , 0.84675338, 0.84679181, 0.78130155,\n",
       "        0.61445247, 0.84681459, 0.84679608, 0.79540075, 0.59575174,\n",
       "        0.84680889, 0.84681886, 0.79468341, 0.59585137, 0.84681032,\n",
       "        0.84681316, 0.79476881, 0.59584283, 0.84680035, 0.84660963,\n",
       "        0.77723945, 0.6196717 , 0.84673061, 0.84662529, 0.77350187,\n",
       "        0.62094698, 0.84675196, 0.84664948, 0.77407261, 0.62050576,\n",
       "        0.84683736, 0.8468644 , 0.79249722, 0.62383201, 0.84677189,\n",
       "        0.84675338, 0.78119338, 0.6145706 , 0.84675908, 0.84683736,\n",
       "        0.78581911, 0.61557403, 0.8468402 , 0.84682313, 0.78750429,\n",
       "        0.62849616, 0.84683593, 0.84680605, 0.77893176, 0.61965036,\n",
       "        0.84682882, 0.84674627, 0.78309348, 0.61985389, 0.84682313,\n",
       "        0.84681743, 0.80317481, 0.59110609, 0.84681601, 0.84680462,\n",
       "        0.79498372, 0.59579586, 0.84681459, 0.84679181, 0.79606543,\n",
       "        0.59565068, 0.84684874, 0.84683024, 0.78787292, 0.62679531,\n",
       "        0.84682028, 0.84673203, 0.77720387, 0.62023248, 0.84681743,\n",
       "        0.84671068, 0.78160471, 0.61922906, 0.84683024, 0.84683024,\n",
       "        0.84682597, 0.63281872, 0.84682313, 0.84682882, 0.80116369,\n",
       "        0.6138376 , 0.84682597, 0.84682882, 0.82058033, 0.61760365,\n",
       "        0.84683024, 0.84683024, 0.8468274 , 0.63398582, 0.84682455,\n",
       "        0.84681601, 0.79732221, 0.61879211, 0.8468274 , 0.84682597,\n",
       "        0.81580801, 0.62405973, 0.84683024, 0.84683024, 0.84683024,\n",
       "        0.58511685, 0.84682313, 0.84681316, 0.79794703, 0.59523081,\n",
       "        0.8468274 , 0.84682455, 0.81143848, 0.58441374, 0.84683024,\n",
       "        0.84683024, 0.8468274 , 0.63398582, 0.84682455, 0.84681459,\n",
       "        0.79702901, 0.6192988 , 0.8468274 , 0.8468274 , 0.81589626,\n",
       "        0.62353454]),\n",
       " 'std_test_accuracy': array([1.35100981e-04, 3.06999011e-05, 4.36349894e-03, 4.28339688e-03,\n",
       "        1.34720982e-04, 3.05012997e-05, 4.36769537e-03, 4.29593587e-03,\n",
       "        1.34720982e-04, 3.05012997e-05, 4.37020438e-03, 4.29439106e-03,\n",
       "        5.83454621e-02, 1.97892841e-02, 1.57419427e-01, 6.08079175e-02,\n",
       "        6.48346765e-02, 6.04393035e-02, 5.91971430e-02, 8.93555993e-02,\n",
       "        7.88706577e-02, 2.93767646e-03, 1.70965219e-01, 6.89915899e-02,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04753259e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04753259e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04753259e-03,\n",
       "        1.01363656e-04, 1.74861485e-04, 1.62984963e-03, 1.18412240e-03,\n",
       "        1.01363656e-04, 1.74861485e-04, 1.63145390e-03, 1.17987698e-03,\n",
       "        1.01363656e-04, 1.74861485e-04, 1.62868946e-03, 1.17987698e-03,\n",
       "        1.37427004e-04, 3.47025706e-05, 4.37029499e-03, 4.30259741e-03,\n",
       "        1.34720982e-04, 3.05012997e-05, 4.37072554e-03, 4.29284665e-03,\n",
       "        1.35509814e-04, 3.06999011e-05, 4.36695916e-03, 4.27704331e-03,\n",
       "        3.44516991e-02, 6.13977081e-03, 2.40326141e-01, 1.12923624e-01,\n",
       "        3.82199863e-02, 8.46491764e-02, 1.71230550e-01, 2.63137557e-02,\n",
       "        4.08916107e-02, 8.51982569e-02, 1.35636611e-01, 1.05113999e-01,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04567134e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04753259e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04753259e-03,\n",
       "        1.01363656e-04, 1.74714372e-04, 1.62709740e-03, 1.19807843e-03,\n",
       "        1.01363656e-04, 1.76523606e-04, 1.63732814e-03, 1.18336836e-03,\n",
       "        9.93889325e-05, 1.76523606e-04, 1.62905810e-03, 1.18212896e-03,\n",
       "        1.20643275e-04, 2.82413103e-05, 4.37879405e-03, 4.26107001e-03,\n",
       "        1.33195938e-04, 3.05012997e-05, 4.38591269e-03, 4.28254366e-03,\n",
       "        1.36906662e-04, 2.75372511e-05, 4.37703343e-03, 4.28758533e-03,\n",
       "        3.09872477e-03, 6.50846396e-05, 1.21643088e-01, 4.58291597e-02,\n",
       "        4.08548204e-03, 1.09164363e-02, 1.86739338e-02, 1.18203359e-01,\n",
       "        3.09364298e-03, 4.54747506e-05, 1.39845644e-02, 1.17954886e-01,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34346396e-03, 1.04035468e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04753259e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34487915e-03, 1.04328884e-03,\n",
       "        1.00753075e-04, 1.80155038e-04, 1.66291231e-03, 1.19905192e-03,\n",
       "        9.47955596e-05, 1.76523606e-04, 1.64749307e-03, 1.19189995e-03,\n",
       "        9.55128762e-05, 1.78693376e-04, 1.64730231e-03, 1.19058819e-03,\n",
       "        1.56576221e-04, 1.63842911e-05, 4.09852363e-03, 3.41467660e-03,\n",
       "        1.40515011e-04, 2.55287514e-05, 4.35991482e-03, 4.23762850e-03,\n",
       "        1.25583476e-04, 3.00691549e-05, 4.45914584e-03, 4.25529247e-03,\n",
       "        6.06559041e-04, 1.04635975e-05, 8.86688295e-03, 3.45994967e-02,\n",
       "        5.53728581e-04, 1.44044653e-04, 3.01630757e-02, 5.41639860e-02,\n",
       "        5.88517499e-04, 1.24210658e-04, 2.33304332e-02, 3.32368263e-02,\n",
       "        9.12774188e-06, 4.33403451e-06, 1.32954963e-03, 1.04221567e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34475474e-03, 1.04434772e-03,\n",
       "        8.74378939e-06, 8.35973522e-06, 1.34681091e-03, 1.03398578e-03,\n",
       "        1.17606583e-04, 1.86429493e-04, 1.67974819e-03, 1.27503797e-03,\n",
       "        9.08298272e-05, 1.70871661e-04, 1.65322542e-03, 1.22065166e-03,\n",
       "        9.01651509e-05, 1.80155038e-04, 1.64486803e-03, 1.26434860e-03,\n",
       "        8.70522244e-05, 1.15794523e-04, 3.28382128e-03, 2.93906651e-03,\n",
       "        1.25968637e-04, 1.03726844e-05, 4.44277410e-03, 4.06816573e-03,\n",
       "        1.43111048e-04, 5.06867006e-05, 4.10048895e-03, 2.96599336e-03,\n",
       "        7.11981273e-05, 1.12867043e-04, 2.44400508e-03, 3.59018754e-03,\n",
       "        6.11018376e-05, 1.52481972e-04, 7.35429724e-03, 4.59687346e-03,\n",
       "        7.23799122e-05, 7.99412126e-05, 7.08505727e-03, 2.90563279e-03,\n",
       "        1.63843159e-05, 6.97951912e-06, 1.38712647e-03, 9.93520114e-04,\n",
       "        9.12774188e-06, 5.56223902e-06, 1.33923596e-03, 1.04089351e-03,\n",
       "        1.05970355e-05, 3.75655658e-06, 1.33109009e-03, 1.04414280e-03,\n",
       "        9.14514916e-05, 1.92046999e-04, 1.71314564e-03, 1.13009088e-03,\n",
       "        1.13271809e-04, 1.88630453e-04, 1.71151699e-03, 1.19394859e-03,\n",
       "        1.25664812e-04, 1.75679498e-04, 1.68894849e-03, 1.31112952e-03,\n",
       "        9.05755187e-06, 2.10768471e-05, 2.58720291e-04, 2.02686985e-03,\n",
       "        8.60909608e-05, 1.23758003e-04, 4.13445575e-03, 3.33591135e-03,\n",
       "        9.28120938e-05, 3.41547504e-05, 3.20462874e-03, 2.97662651e-03,\n",
       "        1.09427774e-05, 5.32708694e-05, 8.53822653e-04, 1.55955865e-03,\n",
       "        4.55293205e-05, 8.08541886e-05, 1.61379475e-03, 1.61115272e-03,\n",
       "        3.84200491e-05, 7.50301948e-05, 1.73010977e-03, 1.52252286e-03,\n",
       "        7.17802282e-06, 9.02518864e-06, 1.31996849e-03, 4.65350642e-04,\n",
       "        1.39383218e-05, 6.34691495e-06, 1.35484198e-03, 1.04949893e-03,\n",
       "        1.97482223e-05, 1.24997349e-05, 1.40627920e-03, 1.02232821e-03,\n",
       "        1.42465698e-05, 4.01845931e-05, 1.18267923e-03, 1.97923946e-04,\n",
       "        6.13364410e-05, 1.29122214e-04, 1.56255828e-03, 1.07741662e-03,\n",
       "        6.06251207e-05, 1.15649948e-04, 1.56558329e-03, 1.17372611e-03,\n",
       "        3.08308022e-07, 3.08308022e-07, 6.34687198e-06, 5.14891728e-04,\n",
       "        1.54451350e-05, 9.75594805e-06, 3.02214660e-03, 2.36639674e-03,\n",
       "        5.89043863e-06, 5.27591907e-06, 7.47524296e-04, 2.61694496e-03,\n",
       "        3.08308022e-07, 3.08308022e-07, 1.73800066e-05, 3.68928431e-04,\n",
       "        1.10158595e-05, 1.44323657e-05, 1.06477421e-03, 1.62373774e-03,\n",
       "        3.88073416e-06, 5.89043863e-06, 7.39812047e-04, 5.03867936e-04,\n",
       "        3.08308022e-07, 3.08308022e-07, 3.08308022e-07, 3.36735600e-04,\n",
       "        1.04198826e-05, 6.70745766e-06, 1.40540650e-03, 1.09107113e-03,\n",
       "        3.88073416e-06, 7.90174256e-06, 9.05787740e-04, 7.40945405e-04,\n",
       "        3.08308022e-07, 3.08308022e-07, 1.73800066e-05, 3.68928431e-04,\n",
       "        1.19915364e-05, 1.63843159e-05, 1.14592968e-03, 9.45547134e-04,\n",
       "        3.88073416e-06, 3.88073416e-06, 8.54190685e-04, 1.63152185e-04]),\n",
       " 'rank_test_accuracy': array([135, 103, 219, 298, 131, 103, 222, 295, 131, 103, 221, 294, 243,\n",
       "        163, 310, 252, 242, 240, 309, 249, 245, 193, 305, 335,  64,  38,\n",
       "        183, 312,  64,  38, 183, 312,  64,  38, 183, 312, 118, 151, 233,\n",
       "        271, 118, 151, 231, 267, 118, 151, 232, 267, 136, 100, 216, 292,\n",
       "        131, 103, 220, 293, 130, 103, 218, 297, 192, 159, 334, 336, 244,\n",
       "        238, 259, 248, 239, 241, 331, 311,  64,  38, 183, 318,  64,  38,\n",
       "        183, 312,  64,  38, 183, 312, 118, 146, 236, 264, 118, 147, 230,\n",
       "        270, 116, 147, 234, 266, 127,  95, 213, 289, 129, 103, 217, 296,\n",
       "        137, 102, 215, 291, 157,  82, 247, 246, 160, 161, 237, 308, 158,\n",
       "         98, 167, 251,  64,  38, 181, 321,  64,  38, 183, 312,  64,  38,\n",
       "        183, 320, 117, 143, 227, 273, 118, 147, 228, 269, 115, 145, 235,\n",
       "        265, 125,  84, 208, 286, 138,  99, 214, 299, 114,  94, 211, 288,\n",
       "        156,   4, 168, 253, 155, 128, 198, 250, 154,  89, 196, 258,  62,\n",
       "         49, 178, 325,  64,  38, 182, 319,  64,  38, 180, 322, 101, 150,\n",
       "        225, 275, 124, 144, 229, 272, 110, 142, 226, 274, 113,  81, 203,\n",
       "        287, 126,  85, 209, 301, 112,  60, 207, 290,  87, 111, 201, 302,\n",
       "         86, 134, 205, 303,  91,  80, 202, 306,  55,  78, 175, 327,  62,\n",
       "         49, 179, 323,  61,  59, 177, 324,  77, 141, 210, 280,  97, 140,\n",
       "        224, 276,  92, 139, 223, 277,   5,   1, 191, 262,  83,  90, 204,\n",
       "        304,  88,   6, 197, 300,   3,  33, 195, 257,   7,  75, 206, 281,\n",
       "         20,  93, 199, 279,  36,  51, 169, 330,  53,  76, 176, 326,  57,\n",
       "         79, 174, 328,   2,   8, 194, 260,  37,  96, 212, 278,  52, 109,\n",
       "        200, 283,   9,   9,  29, 256,  34,  18, 170, 307,  27,  19, 162,\n",
       "        285,   9,   9,  25, 254,  31,  54, 172, 284,  21,  27, 165, 261,\n",
       "          9,   9,   9, 332,  35,  58, 171, 329,  21,  32, 166, 333,   9,\n",
       "          9,  25, 254,  30,  55, 173, 282,  21,  21, 164, 263]),\n",
       " 'split0_test_balanced_accuracy': array([0.50128631, 0.5021506 , 0.59973249, 0.63674788, 0.50128631,\n",
       "        0.50216454, 0.59971218, 0.6367289 , 0.50128631, 0.50216454,\n",
       "        0.59971218, 0.63673142, 0.52663058, 0.5184533 , 0.54857194,\n",
       "        0.56554169, 0.56701188, 0.5274055 , 0.57227726, 0.58457261,\n",
       "        0.50090888, 0.5179911 , 0.56765033, 0.56556423, 0.500094  ,\n",
       "        0.50037425, 0.58064738, 0.62992733, 0.500094  , 0.50037425,\n",
       "        0.58064738, 0.62992733, 0.500094  , 0.50037425, 0.58064738,\n",
       "        0.62992733, 0.50213267, 0.50372353, 0.60330779, 0.6387879 ,\n",
       "        0.50213267, 0.50372353, 0.60331535, 0.6387879 , 0.50213267,\n",
       "        0.50372353, 0.60331031, 0.6387879 , 0.50130024, 0.50216454,\n",
       "        0.59989367, 0.63678198, 0.50128631, 0.50216454, 0.59970966,\n",
       "        0.63673394, 0.50130024, 0.5021506 , 0.599754  , 0.63677828,\n",
       "        0.54812746, 0.50796995, 0.53331627, 0.57984871, 0.52060965,\n",
       "        0.51160088, 0.56815122, 0.58778534, 0.5096118 , 0.52466937,\n",
       "        0.57156053, 0.55388321, 0.500094  , 0.50037425, 0.58064738,\n",
       "        0.62992733, 0.500094  , 0.50037425, 0.58064738, 0.62992733,\n",
       "        0.500094  , 0.50037425, 0.58064738, 0.62992733, 0.50213267,\n",
       "        0.50372605, 0.60331802, 0.63881326, 0.50213267, 0.50372605,\n",
       "        0.60331788, 0.6387879 , 0.50213267, 0.50372605, 0.60329133,\n",
       "        0.63879042, 0.50122418, 0.50207691, 0.59958691, 0.63677089,\n",
       "        0.50128631, 0.50215313, 0.59973368, 0.63673646, 0.50130024,\n",
       "        0.5021506 , 0.59996603, 0.63677294, 0.5028328 , 0.5002841 ,\n",
       "        0.57270583, 0.58762302, 0.51370613, 0.50266456, 0.59560735,\n",
       "        0.57305288, 0.50436951, 0.5004979 , 0.57211657, 0.60457897,\n",
       "        0.500094  , 0.50037425, 0.58064738, 0.62992985, 0.500094  ,\n",
       "        0.50037425, 0.58064738, 0.62992733, 0.500094  , 0.50037425,\n",
       "        0.58064738, 0.62992985, 0.50209589, 0.50371582, 0.60342791,\n",
       "        0.63886411, 0.50210479, 0.50372605, 0.60332929, 0.63877278,\n",
       "        0.50210479, 0.50372857, 0.60331921, 0.63880822, 0.50134591,\n",
       "        0.50207558, 0.59893783, 0.63697162, 0.50127741, 0.50209604,\n",
       "        0.59971693, 0.63681742, 0.50123678, 0.50199951, 0.59924726,\n",
       "        0.63676865, 0.50121262, 0.50118859, 0.5808666 , 0.62553604,\n",
       "        0.50110244, 0.50658407, 0.60851792, 0.62462289, 0.50102742,\n",
       "        0.50594812, 0.60164904, 0.62999067, 0.500094  , 0.50036031,\n",
       "        0.58056479, 0.62989441, 0.500094  , 0.50037425, 0.5806499 ,\n",
       "        0.62992733, 0.500094  , 0.50037425, 0.58065242, 0.62991843,\n",
       "        0.50199433, 0.50338872, 0.60317409, 0.63878949, 0.50207943,\n",
       "        0.50368543, 0.6032252 , 0.63889199, 0.5020781 , 0.50369551,\n",
       "        0.60334784, 0.6388997 , 0.50107338, 0.50227162, 0.59613897,\n",
       "        0.63616059, 0.50112009, 0.50189928, 0.59938856, 0.63667372,\n",
       "        0.5011493 , 0.50199033, 0.59994117, 0.63674255, 0.50136237,\n",
       "        0.50265505, 0.59653632, 0.63564304, 0.50119497, 0.50230378,\n",
       "        0.59374925, 0.63663921, 0.50110125, 0.50227814, 0.59293674,\n",
       "        0.63616554, 0.50005723, 0.50023739, 0.58005716, 0.62994793,\n",
       "        0.500094  , 0.50034637, 0.58064219, 0.62991087, 0.500094  ,\n",
       "        0.50033244, 0.58055694, 0.62986268, 0.50204208, 0.50334662,\n",
       "        0.59937635, 0.63840429, 0.50198024, 0.50342387, 0.60285116,\n",
       "        0.63873507, 0.50182292, 0.50332111, 0.60260522, 0.63838568,\n",
       "        0.50015851, 0.50036136, 0.57812576, 0.62823804, 0.50078765,\n",
       "        0.50149434, 0.59565231, 0.63595904, 0.5006926 , 0.50141813,\n",
       "        0.59112819, 0.63502293, 0.50021812, 0.5007445 , 0.57892279,\n",
       "        0.62933456, 0.50129372, 0.50242271, 0.5978509 , 0.6375199 ,\n",
       "        0.50122655, 0.50235064, 0.59295296, 0.63663323, 0.50001142,\n",
       "        0.49999629, 0.5732097 , 0.62894166, 0.50008259, 0.50030708,\n",
       "        0.58050017, 0.62990226, 0.50003188, 0.50016132, 0.5796763 ,\n",
       "        0.62979875, 0.50025104, 0.50074702, 0.58009559, 0.62965193,\n",
       "        0.50135467, 0.50247609, 0.59953159, 0.6375184 , 0.50126451,\n",
       "        0.50234812, 0.5946169 , 0.63644401, 0.5       , 0.5       ,\n",
       "        0.5       , 0.61622506, 0.50001913, 0.50013197, 0.5758274 ,\n",
       "        0.6334001 , 0.5       , 0.50001394, 0.55141728, 0.62762488,\n",
       "        0.5       , 0.5       , 0.50004952, 0.61638447, 0.5000685 ,\n",
       "        0.50010276, 0.58108336, 0.634222  , 0.5       , 0.5       ,\n",
       "        0.55523926, 0.62975111, 0.5       , 0.5       , 0.5       ,\n",
       "        0.61793778, 0.50002165, 0.50011046, 0.57820645, 0.62971167,\n",
       "        0.5       , 0.5       , 0.56493846, 0.62769031, 0.5       ,\n",
       "        0.5       , 0.50004952, 0.61638447, 0.50006598, 0.50010276,\n",
       "        0.58090378, 0.63435733, 0.5       , 0.5       , 0.55624283,\n",
       "        0.62961575]),\n",
       " 'split1_test_balanced_accuracy': array([0.50260179, 0.50042021, 0.60094375, 0.63781641, 0.50260179,\n",
       "        0.50041769, 0.60092477, 0.63784933, 0.50260179, 0.50041769,\n",
       "        0.60092477, 0.63783792, 0.53745168, 0.50558327, 0.54429539,\n",
       "        0.53927104, 0.51025864, 0.47917656, 0.54637687, 0.48715113,\n",
       "        0.50528988, 0.51466262, 0.52396428, 0.52321514, 0.50019795,\n",
       "        0.50046559, 0.5802018 , 0.63082974, 0.50019795, 0.50046559,\n",
       "        0.5802018 , 0.63082974, 0.50019795, 0.50046559, 0.5802018 ,\n",
       "        0.63082974, 0.50208535, 0.5037226 , 0.60242149, 0.63889929,\n",
       "        0.50208535, 0.5037226 , 0.60243542, 0.63890686, 0.50208535,\n",
       "        0.5037226 , 0.60243542, 0.63890686, 0.50260179, 0.50042273,\n",
       "        0.60096525, 0.63797582, 0.50260179, 0.50041769, 0.60092477,\n",
       "        0.63784933, 0.50260431, 0.50042021, 0.60094627, 0.63781641,\n",
       "        0.55340482, 0.50001808, 0.50772359, 0.51654353, 0.55273611,\n",
       "        0.46422917, 0.53093532, 0.51011622, 0.54065528, 0.47039919,\n",
       "        0.51227144, 0.49658018, 0.50019795, 0.50046559, 0.5802018 ,\n",
       "        0.63082722, 0.50019795, 0.50046559, 0.5802018 , 0.63082974,\n",
       "        0.50019795, 0.50046559, 0.5802018 , 0.63082974, 0.50207393,\n",
       "        0.50372513, 0.60239109, 0.63891323, 0.50208535, 0.5037226 ,\n",
       "        0.60241897, 0.63890182, 0.50208787, 0.5037226 , 0.60241897,\n",
       "        0.63890182, 0.5026144 , 0.50041517, 0.60117613, 0.63875681,\n",
       "        0.50260431, 0.50040627, 0.60087406, 0.63782531, 0.50260431,\n",
       "        0.50041769, 0.60089675, 0.63803795, 0.51439569, 0.50001913,\n",
       "        0.55139581, 0.56944522, 0.51995743, 0.4938398 , 0.53316582,\n",
       "        0.51945192, 0.51455121, 0.50011817, 0.55032382, 0.55323628,\n",
       "        0.50019795, 0.50046559, 0.58019291, 0.63083611, 0.50019795,\n",
       "        0.50046559, 0.5802018 , 0.63082974, 0.50019795, 0.50046559,\n",
       "        0.5802018 , 0.63082218, 0.50203968, 0.5036845 , 0.60238471,\n",
       "        0.63888284, 0.50209039, 0.50369977, 0.60239746, 0.6389119 ,\n",
       "        0.50207897, 0.50368835, 0.6023899 , 0.63891323, 0.50247382,\n",
       "        0.50039738, 0.60053534, 0.63798805, 0.50256754, 0.50039233,\n",
       "        0.60082943, 0.63791932, 0.50255094, 0.50042911, 0.60100591,\n",
       "        0.63861934, 0.50532555, 0.50012307, 0.58018585, 0.63046684,\n",
       "        0.50548481, 0.50002298, 0.57906943, 0.61532405, 0.50510581,\n",
       "        0.50006598, 0.58780835, 0.62916524, 0.50019795, 0.50045165,\n",
       "        0.58018757, 0.63075753, 0.50019795, 0.50046559, 0.58019291,\n",
       "        0.63082974, 0.50019795, 0.50046559, 0.58017007, 0.63083611,\n",
       "        0.50189377, 0.5033485 , 0.60224269, 0.63907989, 0.50205866,\n",
       "        0.50368954, 0.60229219, 0.6388855 , 0.50202307, 0.50362993,\n",
       "        0.60236915, 0.63887319, 0.50222858, 0.50048619, 0.59698737,\n",
       "        0.63721294, 0.50235684, 0.5003351 , 0.59992083, 0.63752231,\n",
       "        0.50222887, 0.50030099, 0.59988652, 0.63765584, 0.50207216,\n",
       "        0.50111577, 0.5965873 , 0.63768611, 0.50140358, 0.50344563,\n",
       "        0.5999456 , 0.63695037, 0.50177798, 0.50071468, 0.6002178 ,\n",
       "        0.63828778, 0.50019409, 0.50037306, 0.57964745, 0.63080572,\n",
       "        0.50019795, 0.50046559, 0.58020418, 0.63082218, 0.50020047,\n",
       "        0.50043771, 0.58015437, 0.63074744, 0.50200485, 0.50343969,\n",
       "        0.59911426, 0.63902642, 0.50189244, 0.50337994, 0.60211267,\n",
       "        0.63899759, 0.50174401, 0.50327452, 0.60212933, 0.63874962,\n",
       "        0.50051882, 0.50004819, 0.576433  , 0.62881611, 0.50164601,\n",
       "        0.50263709, 0.59630296, 0.63625232, 0.50243899, 0.50024985,\n",
       "        0.59237475, 0.63601968, 0.50025874, 0.5008084 , 0.5803993 ,\n",
       "        0.62873846, 0.50125101, 0.50194138, 0.59768803, 0.63794681,\n",
       "        0.50107975, 0.50203924, 0.59421694, 0.63705806, 0.50000133,\n",
       "        0.50007488, 0.57335472, 0.6293326 , 0.50020299, 0.500387  ,\n",
       "        0.58000522, 0.63082232, 0.50016236, 0.50033881, 0.57904863,\n",
       "        0.63050544, 0.50031331, 0.50083747, 0.5804552 , 0.62962127,\n",
       "        0.50131462, 0.50255212, 0.59941914, 0.63793424, 0.50132337,\n",
       "        0.50246716, 0.59513878, 0.63737058, 0.5       , 0.5       ,\n",
       "        0.5       , 0.61572563, 0.49999496, 0.49999496, 0.5766742 ,\n",
       "        0.63333072, 0.49999244, 0.49999496, 0.55098834, 0.62875353,\n",
       "        0.5       , 0.5       , 0.50010394, 0.61572563, 0.50001023,\n",
       "        0.50003692, 0.58165859, 0.63521679, 0.49999496, 0.49999244,\n",
       "        0.55754852, 0.63030587, 0.5       , 0.5       , 0.5       ,\n",
       "        0.61709307, 0.5000774 , 0.50022968, 0.57769295, 0.63061408,\n",
       "        0.49999496, 0.49998992, 0.56573512, 0.628248  , 0.5       ,\n",
       "        0.5       , 0.50010394, 0.61572563, 0.50001023, 0.5000344 ,\n",
       "        0.58225627, 0.63520813, 0.49999496, 0.49999496, 0.55755356,\n",
       "        0.63037541]),\n",
       " 'split2_test_balanced_accuracy': array([0.50341836, 0.50308578, 0.59846259, 0.63910778, 0.5034323 ,\n",
       "        0.50308578, 0.59846897, 0.63913565, 0.5034323 , 0.50308578,\n",
       "        0.59847401, 0.63912676, 0.5342877 , 0.52347847, 0.5484411 ,\n",
       "        0.56121892, 0.54498719, 0.51573845, 0.56101026, 0.56834755,\n",
       "        0.50684349, 0.50719263, 0.53537913, 0.58939813, 0.50014724,\n",
       "        0.50036417, 0.5795426 , 0.63311862, 0.50014724, 0.50036417,\n",
       "        0.5795426 , 0.63311862, 0.50014724, 0.50036417, 0.5795426 ,\n",
       "        0.63311862, 0.50223052, 0.50425449, 0.60418616, 0.64040948,\n",
       "        0.50223052, 0.50425449, 0.60418364, 0.64040696, 0.50223052,\n",
       "        0.50425449, 0.60418364, 0.64040696, 0.50333592, 0.50307184,\n",
       "        0.59840655, 0.63902814, 0.5034323 , 0.50308578, 0.59847653,\n",
       "        0.63914069, 0.50341836, 0.50308578, 0.59845622, 0.63910273,\n",
       "        0.51345884, 0.50286432, 0.51133132, 0.56728879, 0.56122832,\n",
       "        0.51125337, 0.53065576, 0.57186965, 0.53149457, 0.50561379,\n",
       "        0.54316745, 0.54059485, 0.50014724, 0.50036417, 0.5795426 ,\n",
       "        0.6331161 , 0.50014724, 0.50036417, 0.5795426 , 0.63311862,\n",
       "        0.50014724, 0.50036417, 0.5795426 , 0.63311862, 0.50223052,\n",
       "        0.50425449, 0.60414435, 0.64045248, 0.50223052, 0.50425449,\n",
       "        0.60418616, 0.64040948, 0.50223052, 0.50425449, 0.60418616,\n",
       "        0.64040948, 0.50322664, 0.50305272, 0.59833318, 0.6390688 ,\n",
       "        0.5034323 , 0.50307436, 0.59850307, 0.63911282, 0.50330804,\n",
       "        0.50307688, 0.59836711, 0.63905602, 0.507568  , 0.50074137,\n",
       "        0.59987283, 0.61840442, 0.52298788, 0.50330637, 0.53666049,\n",
       "        0.5752726 , 0.5180862 , 0.5005243 , 0.53990453, 0.61017298,\n",
       "        0.50014724, 0.50036417, 0.57955016, 0.63311105, 0.50014724,\n",
       "        0.50036417, 0.5795426 , 0.63311862, 0.50014724, 0.50036417,\n",
       "        0.5795426 , 0.6331161 , 0.50219879, 0.50424426, 0.60400867,\n",
       "        0.64039421, 0.50221911, 0.50425449, 0.60419625, 0.64042594,\n",
       "        0.50222163, 0.50426205, 0.60418231, 0.64041704, 0.50298659,\n",
       "        0.50288117, 0.59856697, 0.63908903, 0.50333221, 0.5030109 ,\n",
       "        0.59838015, 0.63909027, 0.50309334, 0.50297279, 0.59823546,\n",
       "        0.63923491, 0.50801027, 0.50168339, 0.5596822 , 0.63246644,\n",
       "        0.50894292, 0.5018029 , 0.55797373, 0.62428052, 0.50812073,\n",
       "        0.50079151, 0.55908916, 0.63345259, 0.50014976, 0.50036669,\n",
       "        0.57945482, 0.63306672, 0.50014724, 0.50036417, 0.5795426 ,\n",
       "        0.63311357, 0.50014724, 0.50036417, 0.57955016, 0.63310097,\n",
       "        0.50222519, 0.50415278, 0.60378792, 0.64043912, 0.5021595 ,\n",
       "        0.50417205, 0.60412018, 0.64056532, 0.50216706, 0.50414403,\n",
       "        0.60409083, 0.64054234, 0.50263146, 0.50269255, 0.59502121,\n",
       "        0.63811244, 0.50302246, 0.50274297, 0.59783884, 0.63897103,\n",
       "        0.50265163, 0.50268812, 0.59721391, 0.63869704, 0.50177383,\n",
       "        0.50319772, 0.59516665, 0.63845626, 0.50160376, 0.50283579,\n",
       "        0.59790007, 0.63888865, 0.50110629, 0.50281904, 0.59731104,\n",
       "        0.63879059, 0.50013197, 0.5002881 , 0.57904476, 0.63275798,\n",
       "        0.50014976, 0.50036669, 0.57950953, 0.63310349, 0.50014976,\n",
       "        0.50035275, 0.57939626, 0.63306286, 0.50198557, 0.50396625,\n",
       "        0.60035298, 0.63881281, 0.50191069, 0.50382805, 0.60398576,\n",
       "        0.64049574, 0.50202591, 0.50383932, 0.60352481, 0.64027731,\n",
       "        0.50008882, 0.50057576, 0.57835109, 0.63153014, 0.50229251,\n",
       "        0.50185213, 0.59370611, 0.63838746, 0.50048724, 0.50192967,\n",
       "        0.58934413, 0.63733751, 0.50026008, 0.50082072, 0.58000872,\n",
       "        0.63092923, 0.5011874 , 0.50266038, 0.59826446, 0.63918622,\n",
       "        0.50113921, 0.50235745, 0.59355674, 0.6380568 , 0.49999748,\n",
       "        0.50002031, 0.57197722, 0.63132423, 0.50015732, 0.50031093,\n",
       "        0.57927499, 0.63308213, 0.50010913, 0.50025385, 0.57842739,\n",
       "        0.63275442, 0.50029048, 0.50074465, 0.58025504, 0.63167145,\n",
       "        0.50135481, 0.50272147, 0.59936173, 0.63934088, 0.5011229 ,\n",
       "        0.50249698, 0.59513862, 0.63788855, 0.5       , 0.5       ,\n",
       "        0.50003811, 0.61625296, 0.50004182, 0.50005323, 0.57645255,\n",
       "        0.63632434, 0.5       , 0.5       , 0.55284205, 0.63117512,\n",
       "        0.5       , 0.5       , 0.50008125, 0.61627699, 0.50004819,\n",
       "        0.50007488, 0.58115776, 0.63779116, 0.5       , 0.5       ,\n",
       "        0.55572241, 0.63072321, 0.5       , 0.5       , 0.5       ,\n",
       "        0.61833831, 0.50008244, 0.50017764, 0.57722163, 0.63295961,\n",
       "        0.5       , 0.5       , 0.56456577, 0.63054022, 0.5       ,\n",
       "        0.5       , 0.50008125, 0.61627699, 0.50006213, 0.50007488,\n",
       "        0.58151048, 0.63775137, 0.5       , 0.5       , 0.55638862,\n",
       "        0.63124023]),\n",
       " 'mean_test_balanced_accuracy': array([0.50243549, 0.50188553, 0.59971295, 0.63789069, 0.50244013,\n",
       "        0.50188934, 0.59970197, 0.63790463, 0.50244013, 0.50188934,\n",
       "        0.59970365, 0.6378987 , 0.53278999, 0.51583834, 0.54710281,\n",
       "        0.55534388, 0.54075257, 0.50744017, 0.55988813, 0.54669043,\n",
       "        0.50434742, 0.51328211, 0.54233125, 0.5593925 , 0.5001464 ,\n",
       "        0.50040134, 0.58013059, 0.63129189, 0.5001464 , 0.50040134,\n",
       "        0.58013059, 0.63129189, 0.5001464 , 0.50040134, 0.58013059,\n",
       "        0.63129189, 0.50214951, 0.50390021, 0.60330515, 0.63936556,\n",
       "        0.50214951, 0.50390021, 0.60331147, 0.63936724, 0.50214951,\n",
       "        0.50390021, 0.60330979, 0.63936724, 0.50241265, 0.50188637,\n",
       "        0.59975516, 0.63792865, 0.50244013, 0.50188934, 0.59970365,\n",
       "        0.63790799, 0.50244097, 0.50188553, 0.59971883, 0.63789914,\n",
       "        0.53833037, 0.50361745, 0.51745706, 0.55456035, 0.54485802,\n",
       "        0.49569447, 0.54324743, 0.5565904 , 0.52725388, 0.50022745,\n",
       "        0.54233314, 0.53035275, 0.5001464 , 0.50040134, 0.58013059,\n",
       "        0.63129021, 0.5001464 , 0.50040134, 0.58013059, 0.63129189,\n",
       "        0.5001464 , 0.50040134, 0.58013059, 0.63129189, 0.50214571,\n",
       "        0.50390189, 0.60328449, 0.63939299, 0.50214951, 0.50390105,\n",
       "        0.60330767, 0.6393664 , 0.50215035, 0.50390105, 0.60329882,\n",
       "        0.63936724, 0.50235507, 0.50184827, 0.59969874, 0.63819883,\n",
       "        0.50244097, 0.50187792, 0.5997036 , 0.63789153, 0.5024042 ,\n",
       "        0.50188173, 0.5997433 , 0.63795564, 0.5082655 , 0.5003482 ,\n",
       "        0.57465815, 0.59182422, 0.51888381, 0.49993691, 0.55514455,\n",
       "        0.5559258 , 0.51233564, 0.50038012, 0.55411498, 0.58932941,\n",
       "        0.5001464 , 0.50040134, 0.58013015, 0.63129234, 0.5001464 ,\n",
       "        0.50040134, 0.58013059, 0.63129189, 0.5001464 , 0.50040134,\n",
       "        0.58013059, 0.63128937, 0.50211145, 0.50388153, 0.60327376,\n",
       "        0.63938039, 0.50213809, 0.50389344, 0.60330767, 0.6393702 ,\n",
       "        0.50213513, 0.50389299, 0.60329714, 0.6393795 , 0.50226878,\n",
       "        0.50178471, 0.59934671, 0.63801623, 0.50239239, 0.50183309,\n",
       "        0.59964217, 0.63794234, 0.50229369, 0.50180047, 0.59949621,\n",
       "        0.63820763, 0.50484948, 0.50099835, 0.57357822, 0.62948977,\n",
       "        0.50517673, 0.50280331, 0.5818537 , 0.62140915, 0.50475132,\n",
       "        0.50226854, 0.58284885, 0.6308695 , 0.50014724, 0.50039288,\n",
       "        0.58006906, 0.63123955, 0.5001464 , 0.50040134, 0.58012847,\n",
       "        0.63129021, 0.5001464 , 0.50040134, 0.58012422, 0.63128517,\n",
       "        0.50203776, 0.50363   , 0.60306823, 0.63943617, 0.5020992 ,\n",
       "        0.50384901, 0.60321252, 0.6394476 , 0.50208941, 0.50382316,\n",
       "        0.60326927, 0.63943841, 0.50197781, 0.50181679, 0.59604918,\n",
       "        0.63716199, 0.50216646, 0.50165912, 0.59904941, 0.63772235,\n",
       "        0.50200993, 0.50165981, 0.59901386, 0.63769848, 0.50173612,\n",
       "        0.50232285, 0.59609676, 0.6372618 , 0.50140077, 0.50286173,\n",
       "        0.59719831, 0.63749274, 0.50132851, 0.50193729, 0.59682186,\n",
       "        0.63774797, 0.50012776, 0.50029952, 0.57958312, 0.63117054,\n",
       "        0.50014724, 0.50039288, 0.58011863, 0.63127884, 0.50014808,\n",
       "        0.5003743 , 0.58003585, 0.63122433, 0.50201083, 0.50358418,\n",
       "        0.59961453, 0.63874784, 0.50192779, 0.50354395, 0.60298319,\n",
       "        0.63940947, 0.50186428, 0.50347832, 0.60275312, 0.63913754,\n",
       "        0.50025538, 0.50032844, 0.57763662, 0.6295281 , 0.50157539,\n",
       "        0.50199452, 0.59522046, 0.63686627, 0.50120628, 0.50119922,\n",
       "        0.59094902, 0.63612671, 0.50024565, 0.50079121, 0.57977693,\n",
       "        0.62966742, 0.50124405, 0.50234149, 0.59793446, 0.63821765,\n",
       "        0.50114851, 0.50224911, 0.59357555, 0.63724937, 0.50000341,\n",
       "        0.50003049, 0.57284721, 0.62986616, 0.50014763, 0.50033501,\n",
       "        0.57992679, 0.63126891, 0.50010112, 0.50025133, 0.57905077,\n",
       "        0.63101954, 0.50028494, 0.50077638, 0.58026861, 0.63031488,\n",
       "        0.50134136, 0.50258323, 0.59943749, 0.6382645 , 0.50123693,\n",
       "        0.50243742, 0.59496477, 0.63723438, 0.5       , 0.5       ,\n",
       "        0.5000127 , 0.61606788, 0.50001863, 0.50006005, 0.57631805,\n",
       "        0.63435172, 0.49999748, 0.50000297, 0.55174922, 0.62918451,\n",
       "        0.5       , 0.5       , 0.50007824, 0.61612903, 0.50004231,\n",
       "        0.50007152, 0.58129991, 0.63574331, 0.49999832, 0.49999748,\n",
       "        0.55617006, 0.63026006, 0.5       , 0.5       , 0.5       ,\n",
       "        0.61778972, 0.5000605 , 0.50017259, 0.57770701, 0.63109512,\n",
       "        0.49999832, 0.49999664, 0.56507978, 0.62882617, 0.5       ,\n",
       "        0.5       , 0.50007824, 0.61612903, 0.50004611, 0.50007068,\n",
       "        0.58155684, 0.63577227, 0.49999832, 0.49999832, 0.55672834,\n",
       "        0.63041047]),\n",
       " 'std_test_balanced_accuracy': array([8.78315297e-04, 1.10423818e-03, 1.01302365e-03, 9.64855066e-04,\n",
       "        8.83523679e-04, 1.10649008e-03, 1.00260387e-03, 9.83331722e-04,\n",
       "        8.83523679e-04, 1.10649008e-03, 1.00053758e-03, 9.78836991e-04,\n",
       "        4.54286594e-03, 7.53604864e-03, 1.98586298e-03, 1.15014120e-02,\n",
       "        2.33621000e-02, 2.05451339e-02, 1.06035219e-02, 4.26185357e-02,\n",
       "        2.51277596e-03, 4.51523994e-03, 1.84998473e-02, 2.73692619e-02,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34319782e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34319782e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34319782e-03,\n",
       "        6.04535568e-05, 2.50515343e-04, 7.20429371e-04, 7.39562197e-04,\n",
       "        6.04535568e-05, 2.50515343e-04, 7.13712631e-04, 7.36793763e-04,\n",
       "        6.04535568e-05, 2.50515343e-04, 7.13707452e-04, 7.36793763e-04,\n",
       "        8.41752819e-04, 1.09923617e-03, 1.04916854e-03, 9.17597178e-04,\n",
       "        8.83523679e-04, 1.10649008e-03, 9.99499192e-04, 9.83429110e-04,\n",
       "        8.72396430e-04, 1.10423818e-03, 1.01686530e-03, 9.50757418e-04,\n",
       "        1.77183038e-02, 3.28972796e-03, 1.13104586e-02, 2.73666035e-02,\n",
       "        1.74931843e-02, 2.22497830e-02, 1.76100096e-02, 3.34984034e-02,\n",
       "        1.30233660e-02, 2.24806955e-02, 2.42118598e-02, 2.44892437e-02,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34234434e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34319782e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34319782e-03,\n",
       "        6.45910827e-05, 2.49326885e-04, 7.16158499e-04, 7.50284018e-04,\n",
       "        6.04535568e-05, 2.49924794e-04, 7.21492281e-04, 7.39033146e-04,\n",
       "        5.95667534e-05, 2.49924794e-04, 7.21475617e-04, 7.38375991e-04,\n",
       "        8.37815943e-04, 1.08884482e-03, 1.16331978e-03, 1.01771245e-03,\n",
       "        8.83678228e-04, 1.10649009e-03, 9.68184317e-04, 9.71273492e-04,\n",
       "        8.31803402e-04, 1.10213445e-03, 1.04466149e-03, 9.33877949e-04,\n",
       "        4.74622208e-03, 2.98319342e-04, 1.98387504e-02, 2.02070659e-02,\n",
       "        3.86455844e-03, 4.31926380e-03, 2.86470659e-02, 2.58068418e-02,\n",
       "        5.81483577e-03, 1.85540807e-04, 1.34209671e-02, 2.56236709e-02,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.50129545e-04, 1.33818777e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34319782e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.53826497e-04, 1.34207448e-03,\n",
       "        6.58841254e-05, 2.56810156e-04, 6.71878935e-04, 7.16920653e-04,\n",
       "        5.75845510e-05, 2.55528421e-04, 7.34510370e-04, 7.48673607e-04,\n",
       "        6.20648133e-05, 2.61481395e-04, 7.31914925e-04, 7.34906465e-04,\n",
       "        6.85316026e-04, 1.03465289e-03, 8.54012511e-04, 8.64659564e-04,\n",
       "        8.47961544e-04, 1.08507445e-03, 1.00131255e-03, 9.28029112e-04,\n",
       "        7.79459959e-04, 1.04795055e-03, 1.14464721e-03, 1.04808539e-03,\n",
       "        2.79547257e-03, 6.51047315e-04, 9.82989630e-03, 2.91245638e-03,\n",
       "        3.20826857e-03, 2.77039171e-03, 2.07282866e-02, 4.30508486e-03,\n",
       "        2.90666176e-03, 2.61866264e-03, 1.77253734e-02, 1.85734549e-03,\n",
       "        4.24724978e-05, 4.16363957e-05, 4.60825417e-04, 1.33918989e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.54344277e-04, 1.34091221e-03,\n",
       "        4.24392324e-05, 4.56207000e-05, 4.51161836e-04, 1.33750364e-03,\n",
       "        1.38744080e-04, 3.70025685e-04, 6.35262206e-04, 7.19036332e-04,\n",
       "        4.34752140e-05, 2.28432406e-04, 7.46329935e-04, 7.90349910e-04,\n",
       "        5.93250384e-05, 2.28463610e-04, 7.05063040e-04, 7.80674051e-04,\n",
       "        6.60341272e-04, 9.56435774e-04, 8.05187011e-04, 7.97656666e-04,\n",
       "        7.88221506e-04, 9.97569010e-04, 8.83149721e-04, 9.48477191e-04,\n",
       "        6.32562262e-04, 1.00216990e-03, 1.27295759e-03, 7.98486840e-04,\n",
       "        2.90993731e-04, 8.81816731e-04, 6.58012995e-04, 1.18703591e-03,\n",
       "        1.66900018e-04, 4.66519019e-04, 2.57785966e-03, 9.95194406e-04,\n",
       "        3.17829095e-04, 8.92266639e-04, 2.99253783e-03, 1.13762148e-03,\n",
       "        5.59531755e-05, 5.59744757e-05, 4.15803894e-04, 1.17584647e-03,\n",
       "        4.24724978e-05, 5.20754912e-05, 4.66344720e-04, 1.34278738e-03,\n",
       "        4.34805513e-05, 4.56006982e-05, 4.81197759e-04, 1.34928625e-03,\n",
       "        2.34555013e-05, 2.72818179e-04, 5.33010729e-04, 2.58106190e-04,\n",
       "        3.78299965e-05, 2.01686248e-04, 7.70363764e-04, 7.75552902e-04,\n",
       "        1.18741932e-04, 2.55977129e-04, 5.79219315e-04, 8.19518833e-04,\n",
       "        1.88440055e-04, 2.16635302e-04, 8.56040514e-04, 1.43519355e-03,\n",
       "        6.16380764e-04, 4.77266082e-04, 1.10326167e-03, 1.08228598e-03,\n",
       "        8.75684587e-04, 7.03038485e-04, 1.24371155e-03, 9.47947559e-04,\n",
       "        1.94735432e-05, 3.34041756e-05, 6.24668454e-04, 9.24830283e-04,\n",
       "        4.36842296e-05, 2.99095826e-04, 2.42629707e-04, 7.06714082e-04,\n",
       "        6.02912521e-05, 1.48424972e-04, 5.16189221e-04, 5.96706130e-04,\n",
       "        5.87678737e-06, 3.28813651e-05, 6.18022116e-04, 1.04329020e-03,\n",
       "        4.96297403e-05, 3.68006674e-05, 5.03243269e-04, 1.33603200e-03,\n",
       "        5.35716655e-05, 7.24821380e-05, 5.09867943e-04, 1.26021436e-03,\n",
       "        2.57225905e-05, 4.32050990e-05, 1.47123010e-04, 9.59318652e-04,\n",
       "        1.89127118e-05, 1.02562603e-04, 7.05482274e-05, 7.79813305e-04,\n",
       "        8.41348832e-05, 6.43065893e-05, 2.45979646e-04, 5.97545235e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.79633364e-05, 2.42278290e-04,\n",
       "        1.91326839e-05, 5.61408194e-05, 3.58551324e-04, 1.39514220e-03,\n",
       "        3.56537610e-06, 8.02751695e-06, 7.92339491e-04, 1.48107023e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.23191769e-05, 2.88604595e-04,\n",
       "        2.41515222e-05, 2.69832206e-05, 2.55443592e-04, 1.50391657e-03,\n",
       "        2.37691740e-06, 3.56537610e-06, 9.94470915e-04, 3.98175195e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.19034318e-04,\n",
       "        2.75480428e-05, 4.88000028e-05, 4.02176425e-04, 1.36889862e-03,\n",
       "        2.37691740e-06, 4.75383481e-06, 4.87731944e-04, 1.23320992e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.23191769e-05, 2.88604595e-04,\n",
       "        2.54229102e-05, 2.80652756e-05, 5.53121525e-04, 1.44189022e-03,\n",
       "        2.37691740e-06, 2.37691740e-06, 5.86548949e-04, 6.63653832e-04]),\n",
       " 'rank_test_balanced_accuracy': array([205, 239,  93,  29, 201, 235,  97,  25, 201, 235,  95,  27, 166,\n",
       "        171, 158, 153, 164, 175, 147, 159, 179, 172, 163, 148, 293, 264,\n",
       "        122,  44, 293, 264, 122,  44, 293, 264, 122,  44, 218, 183,  80,\n",
       "         13, 218, 183,  76,  10, 218, 183,  77,  10, 206, 238,  90,  23,\n",
       "        201, 235,  94,  24, 199, 239,  92,  26, 165, 192, 170, 155, 160,\n",
       "        336, 161, 150, 168, 287, 162, 167, 293, 264, 122,  50, 293, 264,\n",
       "        122,  44, 293, 264, 122,  44, 222, 180,  83,   5, 218, 181,  79,\n",
       "         12, 217, 181,  81,   9, 209, 244,  98,  19, 200, 242,  96,  28,\n",
       "        207, 241,  91,  21, 174, 279, 143, 114, 169, 335, 154, 152, 173,\n",
       "        277, 156, 116, 293, 264, 130,  43, 293, 264, 122,  44, 293, 264,\n",
       "        122,  52, 225, 188,  84,   6, 223, 186,  78,   8, 224, 187,  82,\n",
       "          7, 213, 248, 103,  20, 208, 245,  99,  22, 212, 247, 101,  18,\n",
       "        177, 261, 144,  68, 176, 197, 118,  71, 178, 214, 117,  61, 291,\n",
       "        275, 134,  56, 293, 264, 131,  51, 293, 264, 132,  53, 228, 191,\n",
       "         87,   3, 226, 189,  86,   1, 227, 190,  85,   2, 232, 246, 110,\n",
       "         37, 216, 251, 104,  31, 230, 250, 105,  32, 249, 211, 109,  34,\n",
       "        253, 196, 107,  33, 255, 233, 108,  30, 304, 282, 138,  58, 291,\n",
       "        275, 133,  54, 289, 278, 135,  57, 229, 193, 100,  15, 234, 194,\n",
       "         88,   4, 243, 195,  89,  14, 284, 281, 141,  67, 252, 231, 111,\n",
       "         38, 258, 259, 115,  39, 286, 262, 137,  66, 256, 210, 106,  17,\n",
       "        260, 215, 113,  35, 317, 314, 145,  65, 290, 280, 136,  55, 305,\n",
       "        285, 139,  60, 283, 263, 121,  63, 254, 198, 102,  16, 257, 204,\n",
       "        112,  36, 319, 319, 316,  75, 315, 311, 142,  42, 332, 318, 157,\n",
       "         69, 319, 319, 306,  73, 313, 308, 120,  41, 328, 332, 151,  64,\n",
       "        319, 319, 319,  72, 310, 288, 140,  59, 328, 334, 146,  70, 319,\n",
       "        319, 306,  73, 312, 309, 119,  40, 328, 328, 149,  62]),\n",
       " 'split0_test_f1': array([6.20618957e-03, 1.06397641e-02, 3.21150845e-01, 3.46611322e-01,\n",
       "        6.20618957e-03, 1.06945976e-02, 3.21119446e-01, 3.46594528e-01,\n",
       "        6.20618957e-03, 1.06945976e-02, 3.21119446e-01, 3.46597009e-01,\n",
       "        1.97704886e-01, 1.16477458e-01, 2.81093206e-01, 2.81197796e-01,\n",
       "        2.71397005e-01, 1.61087826e-01, 2.92327328e-01, 3.00195705e-01,\n",
       "        1.82612256e-01, 1.57769489e-01, 2.92822238e-01, 2.88594544e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        1.04247104e-02, 1.80061845e-02, 3.26287996e-01, 3.49321082e-01,\n",
       "        1.04247104e-02, 1.80061845e-02, 3.26300482e-01, 3.49321082e-01,\n",
       "        1.04247104e-02, 1.80061845e-02, 3.26292158e-01, 3.49321082e-01,\n",
       "        6.26142849e-03, 1.06945976e-02, 3.21400952e-01, 3.46643001e-01,\n",
       "        6.20618957e-03, 1.06945976e-02, 3.21115235e-01, 3.46599489e-01,\n",
       "        6.26142849e-03, 1.06397641e-02, 3.21185496e-01, 3.46637468e-01,\n",
       "        2.35496762e-01, 5.47367364e-02, 2.77893443e-01, 2.97408916e-01,\n",
       "        1.94337007e-01, 8.04903925e-02, 2.93050144e-01, 3.03452231e-01,\n",
       "        1.49862979e-01, 1.49705998e-01, 2.94844038e-01, 2.81122081e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        1.04247104e-02, 1.80066772e-02, 3.26301559e-01, 3.49342678e-01,\n",
       "        1.04247104e-02, 1.80066772e-02, 3.26304645e-01, 3.49321082e-01,\n",
       "        1.04247104e-02, 1.80066772e-02, 3.26262485e-01, 3.49323637e-01,\n",
       "        5.93044201e-03, 1.02052074e-02, 3.20940311e-01, 3.46658455e-01,\n",
       "        6.20618957e-03, 1.06400573e-02, 3.21154098e-01, 3.46601970e-01,\n",
       "        6.26142849e-03, 1.06397641e-02, 3.21512816e-01, 3.46639753e-01,\n",
       "        1.45813348e-02, 1.33645172e-03, 2.73759659e-01, 3.02896041e-01,\n",
       "        7.65135082e-02, 1.68299900e-02, 3.13354998e-01, 2.94510614e-01,\n",
       "        2.29766711e-02, 2.88776587e-03, 2.73445927e-01, 3.15926310e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39704502e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39702140e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88606312e-01, 3.39704502e-01,\n",
       "        1.02606537e-02, 1.79016232e-02, 3.26478018e-01, 3.49381903e-01,\n",
       "        1.03149650e-02, 1.80066772e-02, 3.26321832e-01, 3.49305749e-01,\n",
       "        1.03149650e-02, 1.80071700e-02, 3.26305182e-01, 3.49337567e-01,\n",
       "        6.48163537e-03, 1.01517241e-02, 3.19978872e-01, 3.46947423e-01,\n",
       "        6.15128845e-03, 1.03672659e-02, 3.21132452e-01, 3.46674110e-01,\n",
       "        5.93126386e-03, 9.82312850e-03, 3.20445920e-01, 3.46648719e-01,\n",
       "        5.76656501e-03, 5.71064231e-03, 2.88799735e-01, 3.41504440e-01,\n",
       "        5.37872907e-03, 3.20366133e-02, 3.30781897e-01, 3.33253423e-01,\n",
       "        4.88359832e-03, 2.88656450e-02, 3.22654819e-01, 3.38780552e-01,\n",
       "        5.57103064e-04, 1.78084479e-03, 2.88449780e-01, 3.39674329e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88610614e-01, 3.39702140e-01,\n",
       "        5.57103064e-04, 1.83644509e-03, 2.88614917e-01, 3.39695320e-01,\n",
       "        9.71570522e-03, 1.64963144e-02, 3.26126702e-01, 3.49277576e-01,\n",
       "        1.02054889e-02, 1.77926202e-02, 3.26166567e-01, 3.49406051e-01,\n",
       "        1.01520042e-02, 1.77945686e-02, 3.26360794e-01, 3.49409751e-01,\n",
       "        5.32254040e-03, 1.06538600e-02, 3.15692839e-01, 3.46323524e-01,\n",
       "        5.37977316e-03, 9.33134559e-03, 3.20659329e-01, 3.46532741e-01,\n",
       "        5.54492778e-03, 9.55484370e-03, 3.21510207e-01, 3.46778019e-01,\n",
       "        6.53703396e-03, 1.27178132e-02, 3.16316640e-01, 3.45831147e-01,\n",
       "        5.76544613e-03, 1.11365328e-02, 3.11948599e-01, 3.46380988e-01,\n",
       "        5.43312543e-03, 1.08143898e-02, 3.10591002e-01, 3.46232823e-01,\n",
       "        3.90026466e-04, 1.28033845e-03, 2.87435740e-01, 3.39712353e-01,\n",
       "        5.57103064e-04, 1.72524139e-03, 2.88594006e-01, 3.39688234e-01,\n",
       "        5.57103064e-04, 1.66963491e-03, 2.88429407e-01, 3.39649142e-01,\n",
       "        9.61379082e-03, 1.61254936e-02, 3.20610687e-01, 3.48802984e-01,\n",
       "        9.55378838e-03, 1.63988373e-02, 3.25654651e-01, 3.49228622e-01,\n",
       "        8.78768619e-03, 1.59130817e-02, 3.25334018e-01, 3.48881360e-01,\n",
       "        7.24355045e-04, 1.61478924e-03, 2.84277434e-01, 3.40466001e-01,\n",
       "        3.72253243e-03, 7.19663419e-03, 3.14899365e-01, 3.45958800e-01,\n",
       "        3.33435216e-03, 6.75787958e-03, 3.07510003e-01, 3.45313804e-01,\n",
       "        1.00270172e-03, 3.50116706e-03, 2.86195577e-01, 3.41996665e-01,\n",
       "        6.09806802e-03, 1.14698503e-02, 3.18315364e-01, 3.48002767e-01,\n",
       "        5.82185135e-03, 1.13016153e-02, 3.10533103e-01, 3.47296301e-01,\n",
       "        5.57506829e-05, 5.57413601e-05, 2.73022442e-01, 3.38601572e-01,\n",
       "        5.01420692e-04, 1.55845602e-03, 2.88300940e-01, 3.39675632e-01,\n",
       "        2.78613619e-04, 9.46574236e-04, 2.86647993e-01, 3.39581726e-01,\n",
       "        1.11411303e-03, 3.50126435e-03, 2.88226476e-01, 3.42175126e-01,\n",
       "        6.42801729e-03, 1.17948577e-02, 3.20845366e-01, 3.48146362e-01,\n",
       "        5.93307272e-03, 1.13013038e-02, 3.13218741e-01, 3.47110090e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.30691038e-01,\n",
       "        1.67205440e-04, 6.68560923e-04, 2.78512759e-01, 3.43887107e-01,\n",
       "        0.00000000e+00, 5.57522371e-05, 2.17544385e-01, 3.39362581e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.78667967e-04, 3.30952325e-01,\n",
       "        3.34410880e-04, 5.01532460e-04, 2.89232066e-01, 3.45172404e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.29617006e-01, 3.42102505e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.29382072e-01,\n",
       "        1.67210099e-04, 6.12813370e-04, 2.83651130e-01, 3.39476169e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.53616036e-01, 3.37102935e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.78667967e-04, 3.30952325e-01,\n",
       "        3.34401561e-04, 5.01532460e-04, 2.88908618e-01, 3.45393341e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.31900675e-01, 3.41855729e-01]),\n",
       " 'split1_test_f1': array([1.33944501e-02, 2.27948739e-03, 3.22440692e-01, 3.48538046e-01,\n",
       "        1.33944501e-02, 2.27942403e-03, 3.22411401e-01, 3.48567366e-01,\n",
       "        1.33944501e-02, 2.27942403e-03, 3.22411401e-01, 3.48557816e-01,\n",
       "        2.02130070e-01, 4.13743479e-02, 2.39123398e-01, 2.36083802e-01,\n",
       "        9.78146872e-02, 1.60405872e-01, 2.61604151e-01, 1.01644050e-01,\n",
       "        5.32328693e-02, 1.42821624e-01, 1.72512634e-01, 2.30084860e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40552787e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40552787e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40552787e-01,\n",
       "        1.08447331e-02, 1.91348434e-02, 3.24649198e-01, 3.49456697e-01,\n",
       "        1.08447331e-02, 1.91348434e-02, 3.24670263e-01, 3.49464373e-01,\n",
       "        1.08447331e-02, 1.91348434e-02, 3.24670263e-01, 3.49464373e-01,\n",
       "        1.33944501e-02, 2.27955076e-03, 3.22474050e-01, 3.48683439e-01,\n",
       "        1.33944501e-02, 2.27942403e-03, 3.22411401e-01, 3.48567366e-01,\n",
       "        1.33948177e-02, 2.27948739e-03, 3.22444758e-01, 3.48538046e-01,\n",
       "        2.43158619e-01, 3.34224599e-04, 9.82024520e-02, 2.63460014e-01,\n",
       "        2.43309806e-01, 1.52035015e-01, 2.07081177e-01, 2.00937406e-01,\n",
       "        2.05317479e-01, 1.66942228e-01, 1.91217289e-01, 1.67880730e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40550413e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40552787e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40552787e-01,\n",
       "        1.07902777e-02, 1.91353650e-02, 3.24602985e-01, 3.49468782e-01,\n",
       "        1.08447331e-02, 1.91348434e-02, 3.24645116e-01, 3.49459256e-01,\n",
       "        1.08450317e-02, 1.91348434e-02, 3.24645116e-01, 3.49459256e-01,\n",
       "        1.33962886e-02, 2.27936067e-03, 3.22807815e-01, 3.49349137e-01,\n",
       "        1.33948177e-02, 2.22395196e-03, 3.22334897e-01, 3.48545041e-01,\n",
       "        1.33948177e-02, 2.27942403e-03, 3.22371492e-01, 3.48736293e-01,\n",
       "        8.08687164e-02, 1.67205440e-04, 2.27117116e-01, 2.70693293e-01,\n",
       "        1.09764495e-01, 4.10063974e-02, 2.06220867e-01, 1.85902985e-01,\n",
       "        8.18502739e-02, 7.24032303e-04, 2.24371048e-01, 2.45945007e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88063427e-01, 3.40557232e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40552787e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080102e-01, 3.40545665e-01,\n",
       "        1.06268755e-02, 1.89224561e-02, 3.24594163e-01, 3.49442053e-01,\n",
       "        1.08453302e-02, 1.90278876e-02, 3.24611806e-01, 3.49469491e-01,\n",
       "        1.07908718e-02, 1.89744009e-02, 3.24599562e-01, 3.49468782e-01,\n",
       "        1.29032258e-02, 2.16853402e-03, 3.21894451e-01, 3.48600769e-01,\n",
       "        1.32319433e-02, 2.16841344e-03, 3.22272131e-01, 3.48620088e-01,\n",
       "        1.30718954e-02, 2.33488993e-03, 3.22557018e-01, 3.49211696e-01,\n",
       "        2.87715720e-02, 6.12898732e-04, 2.87690511e-01, 3.39448230e-01,\n",
       "        2.91123943e-02, 2.22909527e-04, 2.85807717e-01, 3.33670758e-01,\n",
       "        2.75836111e-02, 3.34401561e-04, 3.01864590e-01, 3.44730251e-01,\n",
       "        1.00247835e-03, 2.22506536e-03, 2.88047469e-01, 3.40492572e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88063427e-01, 3.40552787e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88021613e-01, 3.40557232e-01,\n",
       "        9.91844831e-03, 1.74705866e-02, 3.24407386e-01, 3.49594567e-01,\n",
       "        1.06822312e-02, 1.89234880e-02, 3.24454840e-01, 3.49440637e-01,\n",
       "        1.04654365e-02, 1.86553934e-02, 3.24577956e-01, 3.49419903e-01,\n",
       "        1.14455511e-02, 2.61212694e-03, 3.16732391e-01, 3.47883927e-01,\n",
       "        1.21495327e-02, 1.78034939e-03, 3.20934179e-01, 3.48216246e-01,\n",
       "        1.16573188e-02, 1.72437769e-03, 3.20946505e-01, 3.48249113e-01,\n",
       "        1.04178150e-02, 5.92338353e-03, 3.16170280e-01, 3.47797203e-01,\n",
       "        7.18947019e-03, 1.75405044e-02, 3.20981396e-01, 3.47698184e-01,\n",
       "        9.10495530e-03, 3.82918505e-03, 3.21466302e-01, 3.48249167e-01,\n",
       "        9.46916950e-04, 1.89172648e-03, 2.86999749e-01, 3.40531724e-01,\n",
       "        1.00247835e-03, 2.28062856e-03, 2.88080589e-01, 3.40545665e-01,\n",
       "        1.00250627e-03, 2.16949907e-03, 2.87981193e-01, 3.40483078e-01,\n",
       "        1.00372260e-02, 1.77967027e-02, 3.19948370e-01, 3.49447645e-01,\n",
       "        9.86497658e-03, 1.74250287e-02, 3.24226187e-01, 3.49521384e-01,\n",
       "        9.15508493e-03, 1.68379620e-02, 3.24299042e-01, 3.49259733e-01,\n",
       "        2.50278087e-03, 2.22971655e-04, 2.81161222e-01, 3.41325811e-01,\n",
       "        8.44883759e-03, 1.33995991e-02, 3.15612802e-01, 3.46987354e-01,\n",
       "        1.23188605e-02, 1.16959064e-03, 3.09513218e-01, 3.46805775e-01,\n",
       "        1.22518308e-03, 4.16192669e-03, 2.89010443e-01, 3.41444649e-01,\n",
       "        6.20378320e-03, 9.71007696e-03, 3.17854482e-01, 3.48548992e-01,\n",
       "        5.37738726e-03, 1.03075736e-02, 3.12502516e-01, 3.47826724e-01,\n",
       "        5.57444674e-05, 3.90102541e-04, 2.73863725e-01, 3.38986095e-01,\n",
       "        1.00253418e-03, 1.94731132e-03, 2.87689976e-01, 3.40542688e-01,\n",
       "        7.79966016e-04, 1.72509738e-03, 2.85824414e-01, 3.40281651e-01,\n",
       "        1.50317337e-03, 4.21800422e-03, 2.89111105e-01, 3.42184312e-01,\n",
       "        6.64194388e-03, 1.30194743e-02, 3.20430718e-01, 3.48570550e-01,\n",
       "        6.58841767e-03, 1.26390988e-02, 3.13935091e-01, 3.48041008e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.30313882e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.81063434e-01, 3.44285808e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.17179420e-01, 3.40839873e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.45893599e-04, 3.30313882e-01,\n",
       "        1.11479613e-04, 2.78629145e-04, 2.90509757e-01, 3.46163699e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.35926350e-01, 3.42479447e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.28712242e-01,\n",
       "        3.90113412e-04, 1.16933014e-03, 2.83079234e-01, 3.40325107e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.55966167e-01, 3.37673143e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.45893599e-04, 3.30313882e-01,\n",
       "        1.11479613e-04, 2.78621381e-04, 2.91638321e-01, 3.46250348e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.35934664e-01, 3.42544652e-01]),\n",
       " 'split2_test_f1': array([1.69677066e-02, 1.52483339e-02, 3.19257160e-01, 3.49234440e-01,\n",
       "        1.70219753e-02, 1.52483339e-02, 3.19266736e-01, 3.49258386e-01,\n",
       "        1.70219753e-02, 1.52483339e-02, 3.19275193e-01, 3.49251480e-01,\n",
       "        2.42906021e-01, 1.66112270e-01, 2.52032693e-01, 2.81437630e-01,\n",
       "        2.54423073e-01, 1.64335555e-01, 2.73160864e-01, 2.88377758e-01,\n",
       "        2.08179516e-01, 1.27878050e-01, 2.39395607e-01, 3.05413440e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42474534e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42474534e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42474534e-01,\n",
       "        1.10214091e-02, 2.09820699e-02, 3.27525425e-01, 3.50967192e-01,\n",
       "        1.10214091e-02, 2.09820699e-02, 3.27521272e-01, 3.50964613e-01,\n",
       "        1.10214091e-02, 2.09820699e-02, 3.27521272e-01, 3.50964613e-01,\n",
       "        1.65904673e-02, 1.51939005e-02, 3.19172056e-01, 3.49160532e-01,\n",
       "        1.70219753e-02, 1.52483339e-02, 3.19279422e-01, 3.49263453e-01,\n",
       "        1.69677066e-02, 1.52483339e-02, 3.19247582e-01, 3.49229374e-01,\n",
       "        7.68815622e-02, 4.98071360e-02, 2.07879210e-01, 2.85284386e-01,\n",
       "        2.72485334e-01, 1.25018845e-01, 2.11292116e-01, 2.88199916e-01,\n",
       "        2.27480429e-01, 1.08617250e-01, 2.54652460e-01, 2.52123860e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42472148e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42474534e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42474534e-01,\n",
       "        1.10214091e-02, 2.09820699e-02, 3.27461797e-01, 3.51006909e-01,\n",
       "        1.10214091e-02, 2.09820699e-02, 3.27525425e-01, 3.50967192e-01,\n",
       "        1.10214091e-02, 2.09820699e-02, 3.27525425e-01, 3.50967192e-01,\n",
       "        1.59487011e-02, 1.50342936e-02, 3.19072138e-01, 3.49227777e-01,\n",
       "        1.70219753e-02, 1.51943172e-02, 3.19322833e-01, 3.49239506e-01,\n",
       "        1.64818617e-02, 1.51947340e-02, 3.19111429e-01, 3.49184477e-01,\n",
       "        4.10475287e-02, 3.99467377e-03, 3.13655679e-01, 3.35283747e-01,\n",
       "        1.26462261e-01, 4.40061450e-02, 2.04024113e-01, 2.91601344e-01,\n",
       "        9.34995763e-02, 2.83356946e-03, 1.84924464e-01, 3.27216380e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86706541e-01, 3.42467376e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42474534e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42472148e-01,\n",
       "        1.08581822e-02, 2.08781924e-02, 3.27257506e-01, 3.50955849e-01,\n",
       "        1.09669064e-02, 2.09820699e-02, 3.27542040e-01, 3.50981891e-01,\n",
       "        1.09672086e-02, 2.09837853e-02, 3.27520830e-01, 3.50974930e-01,\n",
       "        1.46057544e-02, 1.40132989e-02, 3.19416499e-01, 3.49317359e-01,\n",
       "        1.66415766e-02, 1.48709084e-02, 3.19126668e-01, 3.49212795e-01,\n",
       "        1.52495886e-02, 1.46558349e-02, 3.18917914e-01, 3.49403015e-01,\n",
       "        4.03187251e-02, 8.12963168e-03, 2.40277189e-01, 3.48042658e-01,\n",
       "        4.46147747e-02, 8.89306231e-03, 2.36973992e-01, 3.40632766e-01,\n",
       "        4.08726578e-02, 3.77756791e-03, 2.39220454e-01, 3.47496063e-01,\n",
       "        7.79857398e-04, 1.83629180e-03, 2.86524612e-01, 3.42430344e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86693776e-01, 3.42469762e-01,\n",
       "        7.79835677e-04, 1.83624071e-03, 2.86706541e-01, 3.42457832e-01,\n",
       "        1.08081282e-02, 2.03491544e-02, 3.26957538e-01, 3.50978246e-01,\n",
       "        1.06940080e-02, 2.06078779e-02, 3.27427231e-01, 3.51103076e-01,\n",
       "        1.06948924e-02, 2.03981456e-02, 3.27387598e-01, 3.51088147e-01,\n",
       "        1.29780857e-02, 1.34076984e-02, 3.13877800e-01, 3.48466008e-01,\n",
       "        1.50293456e-02, 1.34150700e-02, 3.18300906e-01, 3.49078892e-01,\n",
       "        1.29809411e-02, 1.29334067e-02, 3.17334010e-01, 3.48886648e-01,\n",
       "        8.83660564e-03, 1.59956176e-02, 3.14106549e-01, 3.49033065e-01,\n",
       "        7.96107917e-03, 1.40063715e-02, 3.18393280e-01, 3.49079136e-01,\n",
       "        5.43342666e-03, 1.37415489e-02, 3.17496360e-01, 3.49114365e-01,\n",
       "        6.68560923e-04, 1.50275505e-03, 2.85684443e-01, 3.42164478e-01,\n",
       "        7.79857398e-04, 1.83629180e-03, 2.86630349e-01, 3.42460218e-01,\n",
       "        7.79857398e-04, 1.78069614e-03, 2.86410363e-01, 3.42428339e-01,\n",
       "        9.76821192e-03, 1.94413347e-02, 3.22014935e-01, 3.49435838e-01,\n",
       "        9.38604240e-03, 1.88493690e-02, 3.27285682e-01, 3.51001887e-01,\n",
       "        9.77252650e-03, 1.88003826e-02, 3.26633743e-01, 3.50789375e-01,\n",
       "        4.45819053e-04, 2.66985566e-03, 2.84761303e-01, 3.43694593e-01,\n",
       "        1.11882716e-02, 8.95151264e-03, 3.11762779e-01, 3.48492302e-01,\n",
       "        2.44675527e-03, 9.44151506e-03, 3.04477110e-01, 3.47783086e-01,\n",
       "        1.28065926e-03, 3.94334907e-03, 2.88182951e-01, 3.43676251e-01,\n",
       "        5.76496674e-03, 1.29294930e-02, 3.18834487e-01, 3.49807375e-01,\n",
       "        5.54431292e-03, 1.16736874e-02, 3.11466984e-01, 3.48872424e-01,\n",
       "        0.00000000e+00, 1.11492042e-04, 2.70944123e-01, 3.40581558e-01,\n",
       "        7.79922565e-04, 1.61389059e-03, 2.86164785e-01, 3.42438365e-01,\n",
       "        5.57196189e-04, 1.33600534e-03, 2.84468158e-01, 3.42156221e-01,\n",
       "        1.39198218e-03, 3.61101081e-03, 2.88598592e-01, 3.43963776e-01,\n",
       "        6.53649079e-03, 1.33593557e-02, 3.20466892e-01, 3.49939835e-01,\n",
       "        5.59773874e-03, 1.23266564e-02, 3.14003246e-01, 3.48612150e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.22946799e-04, 3.30745172e-01,\n",
       "        1.67247387e-04, 2.22984085e-04, 2.79796307e-01, 3.46794556e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.22119799e-01, 3.42764218e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.45781790e-04, 3.30876894e-01,\n",
       "        2.22971655e-04, 3.90102541e-04, 2.89425918e-01, 3.48557886e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.31090338e-01, 3.42846915e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.29751678e-01,\n",
       "        3.90135154e-04, 8.91215953e-04, 2.81967611e-01, 3.42299582e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.53338279e-01, 3.39412120e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.45781790e-04, 3.30876894e-01,\n",
       "        2.78706800e-04, 3.90102541e-04, 2.90112652e-01, 3.48467085e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.32710447e-01, 3.43273020e-01]),\n",
       " 'mean_test_f1': array([1.21894488e-02, 9.38919512e-03, 3.20949565e-01, 3.48127936e-01,\n",
       "        1.22075383e-02, 9.40745184e-03, 3.20932528e-01, 3.48140093e-01,\n",
       "        1.22075383e-02, 9.40745184e-03, 3.20935347e-01, 3.48135435e-01,\n",
       "        2.14246992e-01, 1.07988025e-01, 2.57416433e-01, 2.66239743e-01,\n",
       "        2.07878255e-01, 1.61943085e-01, 2.75697448e-01, 2.30072504e-01,\n",
       "        1.48008214e-01, 1.42823054e-01, 2.34910159e-01, 2.74697615e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40909820e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40909820e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40909820e-01,\n",
       "        1.07636176e-02, 1.93743659e-02, 3.26154206e-01, 3.49914990e-01,\n",
       "        1.07636176e-02, 1.93743659e-02, 3.26164006e-01, 3.49916689e-01,\n",
       "        1.07636176e-02, 1.93743659e-02, 3.26161231e-01, 3.49916689e-01,\n",
       "        1.20821153e-02, 9.38934961e-03, 3.21015686e-01, 3.48162324e-01,\n",
       "        1.22075383e-02, 9.40745184e-03, 3.20935353e-01, 3.48143436e-01,\n",
       "        1.22079843e-02, 9.38919512e-03, 3.20959279e-01, 3.48134963e-01,\n",
       "        1.85178981e-01, 3.49593657e-02, 1.94658368e-01, 2.82051105e-01,\n",
       "        2.36710716e-01, 1.19181418e-01, 2.37141146e-01, 2.64196517e-01,\n",
       "        1.94220295e-01, 1.41755158e-01, 2.46904596e-01, 2.33708890e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40908234e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40909820e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40909820e-01,\n",
       "        1.07454658e-02, 1.93747040e-02, 3.26122114e-01, 3.49939456e-01,\n",
       "        1.07636176e-02, 1.93745302e-02, 3.26158395e-01, 3.49915843e-01,\n",
       "        1.07637171e-02, 1.93745302e-02, 3.26144342e-01, 3.49916695e-01,\n",
       "        1.17584772e-02, 9.17295388e-03, 3.20940088e-01, 3.48411789e-01,\n",
       "        1.22076609e-02, 9.35277550e-03, 3.20937276e-01, 3.48128839e-01,\n",
       "        1.20460360e-02, 9.37130734e-03, 3.20998579e-01, 3.48186841e-01,\n",
       "        4.54991933e-02, 1.83277698e-03, 2.71510818e-01, 3.02957694e-01,\n",
       "        1.04246755e-01, 3.39475108e-02, 2.41199993e-01, 2.57338314e-01,\n",
       "        6.61088404e-02, 2.14845588e-03, 2.27580480e-01, 2.96362566e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87792093e-01, 3.40909703e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40909820e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87793397e-01, 3.40907438e-01,\n",
       "        1.05819038e-02, 1.92340906e-02, 3.26109896e-01, 3.49926602e-01,\n",
       "        1.07090672e-02, 1.93388782e-02, 3.26158559e-01, 3.49919044e-01,\n",
       "        1.06910151e-02, 1.93217854e-02, 3.26141858e-01, 3.49927093e-01,\n",
       "        1.13302052e-02, 8.77785235e-03, 3.20429941e-01, 3.48288517e-01,\n",
       "        1.20082695e-02, 9.13552927e-03, 3.20843750e-01, 3.48168998e-01,\n",
       "        1.14175826e-02, 8.93795111e-03, 3.20640284e-01, 3.48421143e-01,\n",
       "        2.49522874e-02, 4.81772424e-03, 2.72255811e-01, 3.42998443e-01,\n",
       "        2.63686327e-02, 1.37175284e-02, 2.84521202e-01, 3.35852315e-01,\n",
       "        2.44466224e-02, 1.09925382e-02, 2.87913287e-01, 3.43668955e-01,\n",
       "        7.79812937e-04, 1.94740065e-03, 2.87673954e-01, 3.40865748e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87789272e-01, 3.40908230e-01,\n",
       "        7.79805697e-04, 1.98443812e-03, 2.87781024e-01, 3.40903461e-01,\n",
       "        1.01474272e-02, 1.81053518e-02, 3.25830542e-01, 3.49950130e-01,\n",
       "        1.05272427e-02, 1.91079954e-02, 3.26016213e-01, 3.49983255e-01,\n",
       "        1.04374444e-02, 1.89493692e-02, 3.26108782e-01, 3.49972600e-01,\n",
       "        9.91539241e-03, 8.89122844e-03, 3.15434343e-01, 3.47557820e-01,\n",
       "        1.08528838e-02, 8.17558832e-03, 3.19964805e-01, 3.47942626e-01,\n",
       "        1.00610626e-02, 8.07087604e-03, 3.19930241e-01, 3.47971260e-01,\n",
       "        8.59715154e-03, 1.15456048e-02, 3.15531156e-01, 3.47553805e-01,\n",
       "        6.97199850e-03, 1.42278029e-02, 3.17107758e-01, 3.47719436e-01,\n",
       "        6.65716913e-03, 9.46170792e-03, 3.16517888e-01, 3.47865452e-01,\n",
       "        6.68501446e-04, 1.55827333e-03, 2.86706644e-01, 3.40802852e-01,\n",
       "        7.79812937e-04, 1.94738725e-03, 2.87768315e-01, 3.40898039e-01,\n",
       "        7.79822242e-04, 1.87327671e-03, 2.87606988e-01, 3.40853520e-01,\n",
       "        9.80640957e-03, 1.77878437e-02, 3.20857998e-01, 3.49228822e-01,\n",
       "        9.60160245e-03, 1.75577450e-02, 3.25722173e-01, 3.49917298e-01,\n",
       "        9.23843254e-03, 1.71838087e-02, 3.25422268e-01, 3.49643489e-01,\n",
       "        1.22431832e-03, 1.50253885e-03, 2.83399986e-01, 3.41828802e-01,\n",
       "        7.78654721e-03, 9.84924865e-03, 3.14091649e-01, 3.47146152e-01,\n",
       "        6.03332265e-03, 5.78966176e-03, 3.07166777e-01, 3.46634222e-01,\n",
       "        1.16951469e-03, 3.86881427e-03, 2.87796324e-01, 3.42372522e-01,\n",
       "        6.02227265e-03, 1.13698067e-02, 3.18334778e-01, 3.48786378e-01,\n",
       "        5.58118384e-03, 1.10942921e-02, 3.11500868e-01, 3.47998483e-01,\n",
       "        3.71650501e-05, 1.85778648e-04, 2.72610096e-01, 3.39389742e-01,\n",
       "        7.61292480e-04, 1.70655264e-03, 2.87385234e-01, 3.40885562e-01,\n",
       "        5.38591941e-04, 1.33589232e-03, 2.85646855e-01, 3.40673199e-01,\n",
       "        1.33642286e-03, 3.77675979e-03, 2.88645391e-01, 3.42774405e-01,\n",
       "        6.53548399e-03, 1.27245625e-02, 3.20580992e-01, 3.48885582e-01,\n",
       "        6.03974304e-03, 1.20890197e-02, 3.13719026e-01, 3.47921083e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.43155998e-05, 3.30583364e-01,\n",
       "        1.11484276e-04, 2.97181669e-04, 2.79790834e-01, 3.44989157e-01,\n",
       "        0.00000000e+00, 1.85840790e-05, 2.18947868e-01, 3.40988891e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.90114452e-04, 3.30714367e-01,\n",
       "        2.22954049e-04, 3.90088049e-04, 2.89722581e-01, 3.46631330e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.32211231e-01, 3.42476289e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.29281998e-01,\n",
       "        3.15819555e-04, 8.91119821e-04, 2.82899325e-01, 3.40700286e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.54306827e-01, 3.38062733e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.90114452e-04, 3.30714367e-01,\n",
       "        2.41529325e-04, 3.90085461e-04, 2.90219864e-01, 3.46703591e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.33515262e-01, 3.42557800e-01]),\n",
       " 'std_test_f1': array([4.47523433e-03, 5.36784793e-03, 1.30744136e-03, 1.10945342e-03,\n",
       "        4.49458007e-03, 5.37219472e-03, 1.29059014e-03, 1.12870319e-03,\n",
       "        4.49458007e-03, 5.37219472e-03, 1.28695227e-03, 1.12408735e-03,\n",
       "        2.03453599e-02, 5.12766362e-02, 1.75519171e-02, 2.13236947e-02,\n",
       "        7.81345861e-02, 1.71448756e-03, 1.26702827e-02, 9.09407018e-02,\n",
       "        6.78242614e-02, 1.22031289e-02, 4.92184900e-02, 3.22845909e-02,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15963992e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15963992e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15963992e-03,\n",
       "        2.50262700e-04, 1.22664902e-03, 1.17801982e-03, 7.46076110e-04,\n",
       "        2.50262700e-04, 1.22664902e-03, 1.16791357e-03, 7.43299251e-04,\n",
       "        2.50262700e-04, 1.22664902e-03, 1.16759587e-03, 7.43299251e-04,\n",
       "        4.31770987e-03, 5.35243630e-03, 1.37528522e-03, 1.09183692e-03,\n",
       "        4.49458007e-03, 5.37219472e-03, 1.28493600e-03, 1.12811434e-03,\n",
       "        4.45065749e-03, 5.36784793e-03, 1.31500673e-03, 1.09585650e-03,\n",
       "        7.66416954e-02, 2.45662442e-02, 7.39518133e-02, 1.40468875e-02,\n",
       "        3.22433583e-02, 2.94981913e-02, 3.95709915e-02, 4.51622609e-02,\n",
       "        3.26443230e-02, 2.44657944e-02, 4.26587063e-02, 4.80293952e-02,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15881059e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15963992e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15963992e-03,\n",
       "        2.45653427e-04, 1.22643187e-03, 1.17398236e-03, 7.56556461e-04,\n",
       "        2.50262700e-04, 1.22646582e-03, 1.18042011e-03, 7.45552971e-04,\n",
       "        2.50294988e-04, 1.22646582e-03, 1.17884522e-03, 7.44874055e-04,\n",
       "        4.25074097e-03, 5.25808829e-03, 1.52508377e-03, 1.24078436e-03,\n",
       "        4.49461244e-03, 5.37279658e-03, 1.23919099e-03, 1.11626417e-03,\n",
       "        4.28008747e-03, 5.34839817e-03, 1.37969364e-03, 1.10915219e-03,\n",
       "        2.72441704e-02, 1.60148525e-03, 3.53649894e-02, 2.63689782e-02,\n",
       "        2.07613996e-02, 1.21657103e-02, 5.10291748e-02, 5.05263671e-02,\n",
       "        3.08676161e-02, 1.00746256e-03, 3.62099221e-02, 3.59473141e-02,\n",
       "        1.81823700e-04, 2.09438286e-04, 7.98957285e-04, 1.15514645e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15963992e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06679842e-04, 1.15848356e-03,\n",
       "        2.46003961e-04, 1.23499745e-03, 1.11802937e-03, 7.28202132e-04,\n",
       "        2.83057836e-04, 1.23444365e-03, 1.20182103e-03, 7.54513634e-04,\n",
       "        2.75480131e-04, 1.23977593e-03, 1.19818110e-03, 7.42866783e-04,\n",
       "        3.49820173e-03, 4.93222442e-03, 1.06071014e-03, 9.92396255e-04,\n",
       "        4.36917762e-03, 5.25840446e-03, 1.30025540e-03, 1.08438701e-03,\n",
       "        3.97997966e-03, 5.06879838e-03, 1.49200113e-03, 1.25572495e-03,\n",
       "        1.43620598e-02, 3.13297468e-03, 2.26168350e-02, 3.66424910e-03,\n",
       "        1.61351168e-02, 1.34284410e-02, 3.83077197e-02, 3.38457982e-03,\n",
       "        1.48589731e-02, 1.27161265e-02, 3.54617391e-02, 3.63637131e-03,\n",
       "        1.81823702e-04, 1.97639168e-04, 8.29137430e-04, 1.15566734e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 8.06200102e-04, 1.15749371e-03,\n",
       "        1.81823700e-04, 2.09438286e-04, 7.97449036e-04, 1.15405781e-03,\n",
       "        4.74461432e-04, 1.63570356e-03, 1.06194868e-03, 7.38416301e-04,\n",
       "        2.27565091e-04, 1.15670544e-03, 1.21812169e-03, 7.91959281e-04,\n",
       "        2.22515261e-04, 1.08304192e-03, 1.16079126e-03, 7.88821485e-04,\n",
       "        3.30735370e-03, 4.58011960e-03, 1.17962953e-03, 9.04550900e-04,\n",
       "        4.04471144e-03, 4.81964811e-03, 1.18189274e-03, 1.05731490e-03,\n",
       "        3.23878569e-03, 4.69483425e-03, 1.85018033e-03, 8.82979986e-04,\n",
       "        1.59334430e-03, 4.19468151e-03, 1.00911987e-03, 1.31845941e-03,\n",
       "        9.09458287e-04, 2.61909486e-03, 3.79800665e-03, 1.10161674e-03,\n",
       "        1.73084620e-03, 4.15820880e-03, 4.49340942e-03, 1.20726948e-03,\n",
       "        2.27349592e-04, 2.52666497e-04, 7.44398276e-04, 1.01926831e-03,\n",
       "        1.81823702e-04, 2.39958852e-04, 8.31514048e-04, 1.15876337e-03,\n",
       "        1.81835097e-04, 2.14311947e-04, 8.65700763e-04, 1.16444636e-03,\n",
       "        1.74964051e-04, 1.35370093e-03, 8.61604787e-04, 3.01151662e-04,\n",
       "        1.98425679e-04, 1.00481726e-03, 1.24994575e-03, 7.76177859e-04,\n",
       "        4.06355944e-04, 1.20383668e-03, 9.55178287e-04, 8.24856447e-04,\n",
       "        9.11133153e-04, 1.00208464e-03, 1.59532295e-03, 1.36521114e-03,\n",
       "        3.08364360e-03, 2.61070117e-03, 1.67231886e-03, 1.04037529e-03,\n",
       "        4.45929342e-03, 3.44569951e-03, 2.07025754e-03, 1.01535277e-03,\n",
       "        1.20109188e-04, 2.74854388e-04, 1.18123600e-03, 9.49021464e-04,\n",
       "        1.86991398e-04, 1.31622350e-03, 4.00320786e-04, 7.55608400e-04,\n",
       "        1.83315177e-04, 5.76659299e-04, 8.04366487e-04, 6.54811017e-04,\n",
       "        2.62796591e-05, 1.46260554e-04, 1.22706726e-03, 8.57237440e-04,\n",
       "        2.05002428e-04, 1.71739733e-04, 8.98309633e-04, 1.15364502e-03,\n",
       "        2.05098586e-04, 3.17830754e-04, 8.98726690e-04, 1.08688805e-03,\n",
       "        1.63619713e-04, 3.15207398e-04, 3.62661121e-04, 8.41020772e-04,\n",
       "        8.73380648e-05, 6.71884517e-04, 1.87523154e-04, 7.65319301e-04,\n",
       "        4.11416340e-04, 5.71417697e-04, 3.54847489e-04, 6.19048937e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.05098129e-04, 1.91829760e-04,\n",
       "        7.88312891e-05, 2.77935710e-04, 1.04131583e-03, 1.28694452e-03,\n",
       "        0.00000000e+00, 2.62818566e-05, 2.24783708e-03, 1.39270455e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.88045783e-05, 2.84854953e-04,\n",
       "        9.10113092e-05, 9.09998981e-05, 5.62215940e-04, 1.42112210e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.69496530e-03, 3.03912368e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.30207712e-04,\n",
       "        1.05082754e-04, 2.27197030e-04, 6.98968119e-04, 1.18278876e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.17879665e-03, 9.82147032e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.88045783e-05, 2.84854953e-04,\n",
       "        9.47282857e-05, 9.10030677e-05, 1.11697243e-03, 1.29513130e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.74242405e-03, 5.78681429e-04]),\n",
       " 'rank_test_f1': array([207, 242,  93,  29, 204, 238,  98,  25, 204, 238,  97,  26, 165,\n",
       "        175, 151, 149, 166, 170, 144, 162, 171, 172, 158, 145, 294, 268,\n",
       "        122,  51, 294, 268, 122,  51, 294, 268, 122,  51, 221, 187,  80,\n",
       "         13, 221, 187,  76,  10, 221, 187,  77,  10, 209, 241,  90,  23,\n",
       "        204, 238,  96,  24, 202, 242,  92,  27, 169, 179, 167, 142, 157,\n",
       "        174, 156, 150, 168, 173, 154, 159, 294, 268, 122,  58, 294, 268,\n",
       "        122,  51, 294, 268, 122,  51, 225, 184,  83,   4, 221, 185,  79,\n",
       "         12, 220, 185,  81,   9, 212, 247,  94,  19, 203, 245,  95,  28,\n",
       "        210, 244,  91,  21, 178, 282, 148, 115, 176, 180, 155, 152, 177,\n",
       "        267, 163, 116, 294, 268, 130,  57, 294, 268, 122,  51, 294, 268,\n",
       "        122,  60, 228, 192,  84,   6, 226, 190,  78,   7, 227, 191,  82,\n",
       "          5, 216, 251, 103,  20, 211, 248, 100,  22, 214, 249, 101,  18,\n",
       "        182, 264, 147,  44, 181, 200, 139,  71, 183, 218, 120,  43, 292,\n",
       "        279, 134,  64, 294, 268, 131,  59, 294, 268, 132,  61, 231, 195,\n",
       "         87,   3, 229, 193,  86,   1, 230, 194,  85,   2, 233, 250, 110,\n",
       "         36, 219, 253, 104,  32, 232, 254, 105,  31, 252, 213, 109,  37,\n",
       "        256, 199, 107,  35, 257, 237, 108,  34, 306, 284, 137,  66, 292,\n",
       "        280, 133,  62, 291, 281, 135,  65, 235, 196,  99,  15, 236, 197,\n",
       "         88,   8, 246, 198,  89,  14, 288, 285, 140,  49, 255, 234, 111,\n",
       "         38, 260, 262, 114,  40, 289, 265, 121,  48, 261, 215, 106,  17,\n",
       "        263, 217, 113,  30, 319, 316, 146,  69, 305, 283, 136,  63, 307,\n",
       "        287, 138,  68, 286, 266, 119,  45, 258, 201, 102,  16, 259, 208,\n",
       "        112,  33, 321, 321, 318,  74, 317, 313, 143,  42, 321, 320, 164,\n",
       "         50, 321, 321, 308,  72, 315, 310, 118,  41, 321, 321, 161,  47,\n",
       "        321, 321, 321,  75, 312, 290, 141,  67, 321, 321, 153,  70, 321,\n",
       "        321, 308,  72, 314, 311, 117,  39, 321, 321, 160,  46]),\n",
       " 'split0_test_average_precision': array([0.27933491, 0.278243  , 0.27893923, 0.27813131, 0.2793336 ,\n",
       "        0.27824557, 0.27893568, 0.27812857, 0.27933399, 0.2782453 ,\n",
       "        0.27893638, 0.27812884, 0.18314597, 0.20655428, 0.17425156,\n",
       "        0.19319194, 0.22114106, 0.20569816, 0.19114892, 0.20834795,\n",
       "        0.15526206, 0.17927669, 0.18451528, 0.18302412, 0.2689018 ,\n",
       "        0.26950104, 0.27116202, 0.26805205, 0.26890173, 0.26950109,\n",
       "        0.27116198, 0.2680521 , 0.26890182, 0.26950105, 0.271162  ,\n",
       "        0.26805209, 0.28116973, 0.28114916, 0.28048798, 0.28009187,\n",
       "        0.28117031, 0.28114975, 0.28048785, 0.28009183, 0.28117033,\n",
       "        0.28114974, 0.28048791, 0.28009193, 0.27934474, 0.27824434,\n",
       "        0.27896683, 0.2781514 , 0.27933382, 0.27824561, 0.27893592,\n",
       "        0.27812916, 0.27933644, 0.27824179, 0.27894062, 0.27812822,\n",
       "        0.20869774, 0.20092938, 0.21807506, 0.21256921, 0.17387609,\n",
       "        0.19778628, 0.2114641 , 0.22480739, 0.16705824, 0.20381663,\n",
       "        0.21740026, 0.1930795 , 0.26890127, 0.26950056, 0.27116195,\n",
       "        0.26805175, 0.26890173, 0.26950108, 0.27116198, 0.2680521 ,\n",
       "        0.26890178, 0.26950101, 0.27116203, 0.26805202, 0.28116804,\n",
       "        0.28114876, 0.28048667, 0.28009072, 0.28117039, 0.28115019,\n",
       "        0.28048805, 0.28009145, 0.28116985, 0.28115001, 0.28048789,\n",
       "        0.28009162, 0.27938339, 0.27860782, 0.27902358, 0.27816905,\n",
       "        0.27933774, 0.27824821, 0.27893893, 0.27812606, 0.2793539 ,\n",
       "        0.27826217, 0.27898155, 0.27815672, 0.26056181, 0.25788629,\n",
       "        0.25995927, 0.22397596, 0.23559327, 0.23351355, 0.24798447,\n",
       "        0.20860189, 0.2556318 , 0.24941962, 0.25529262, 0.24788773,\n",
       "        0.26889545, 0.26949663, 0.2711606 , 0.26804947, 0.26890179,\n",
       "        0.26950109, 0.271162  , 0.26805207, 0.26890108, 0.26950023,\n",
       "        0.27116176, 0.26805165, 0.28116392, 0.28113443, 0.28049229,\n",
       "        0.28008963, 0.28116857, 0.28115206, 0.28048945, 0.28009331,\n",
       "        0.2811665 , 0.28115066, 0.2804893 , 0.28009249, 0.27953971,\n",
       "        0.27883621, 0.27898231, 0.2782768 , 0.27936236, 0.27827148,\n",
       "        0.27895999, 0.27815184, 0.27947616, 0.27879764, 0.27904807,\n",
       "        0.27821395, 0.27439995, 0.27010669, 0.27351048, 0.26117509,\n",
       "        0.27271752, 0.27990106, 0.26643861, 0.25957138, 0.27379854,\n",
       "        0.28040296, 0.26736385, 0.26729608, 0.2688483 , 0.26945251,\n",
       "        0.27114251, 0.26802478, 0.26890177, 0.26950106, 0.2711619 ,\n",
       "        0.26805188, 0.26889255, 0.26949439, 0.27115943, 0.26804798,\n",
       "        0.28107619, 0.28105725, 0.28048989, 0.28007787, 0.28117819,\n",
       "        0.28116807, 0.28051211, 0.2801155 , 0.28118518, 0.28114571,\n",
       "        0.28051429, 0.28011084, 0.27769299, 0.27794997, 0.27875939,\n",
       "        0.27800026, 0.27937317, 0.27826351, 0.27902249, 0.27824062,\n",
       "        0.27948582, 0.27883397, 0.27922038, 0.27829682, 0.27845574,\n",
       "        0.27815618, 0.2788354 , 0.27787015, 0.28032305, 0.28005776,\n",
       "        0.2800383 , 0.27767787, 0.28004733, 0.27985367, 0.28010693,\n",
       "        0.27805034, 0.26832495, 0.26899421, 0.27105842, 0.2678446 ,\n",
       "        0.26890101, 0.26950051, 0.27116087, 0.26805063, 0.26882066,\n",
       "        0.2694283 , 0.27113147, 0.26801076, 0.27869398, 0.27900247,\n",
       "        0.2797145 , 0.27936181, 0.28104331, 0.28104784, 0.28057661,\n",
       "        0.28020049, 0.28081505, 0.28085667, 0.28041793, 0.28002164,\n",
       "        0.26471827, 0.26480629, 0.26584068, 0.26449138, 0.27798827,\n",
       "        0.27677692, 0.27799617, 0.27734271, 0.27494388, 0.27485026,\n",
       "        0.27672118, 0.27627592, 0.26294827, 0.26337312, 0.26423366,\n",
       "        0.26545043, 0.27878655, 0.27894557, 0.2790733 , 0.2789161 ,\n",
       "        0.27464364, 0.27515513, 0.27695878, 0.27701103, 0.26045143,\n",
       "        0.26208756, 0.27042206, 0.26607313, 0.26889935, 0.26949896,\n",
       "        0.27114743, 0.26803614, 0.26800854, 0.26871706, 0.27101655,\n",
       "        0.26775113, 0.26311474, 0.26340987, 0.26554437, 0.26566979,\n",
       "        0.27904151, 0.27918908, 0.27940593, 0.27914084, 0.27567107,\n",
       "        0.2760163 , 0.27730286, 0.27723437, 0.15316954, 0.15316954,\n",
       "        0.24866   , 0.24866   , 0.27249201, 0.27154208, 0.27434264,\n",
       "        0.27369237, 0.26288172, 0.26314398, 0.26557849, 0.26404691,\n",
       "        0.15316954, 0.15316954, 0.24866   , 0.24866   , 0.27366411,\n",
       "        0.27403379, 0.27500088, 0.27480284, 0.25904748, 0.25926507,\n",
       "        0.26382353, 0.26511724, 0.18645107, 0.18645107, 0.24983627,\n",
       "        0.24866   , 0.26888479, 0.26949285, 0.27100696, 0.26789359,\n",
       "        0.25484538, 0.25708828, 0.27000574, 0.26479669, 0.15316954,\n",
       "        0.15316954, 0.24866   , 0.24866   , 0.27363312, 0.27394192,\n",
       "        0.27499183, 0.27477084, 0.26181213, 0.26235693, 0.26512035,\n",
       "        0.26530102]),\n",
       " 'split1_test_average_precision': array([0.27636168, 0.27331009, 0.27377954, 0.27401636, 0.27636142,\n",
       "        0.27330522, 0.27377851, 0.27401225, 0.27636139, 0.273306  ,\n",
       "        0.27377875, 0.27401286, 0.20923742, 0.20305088, 0.19435102,\n",
       "        0.18341748, 0.18295121, 0.13941953, 0.1809257 , 0.14091082,\n",
       "        0.19354279, 0.17385471, 0.19171197, 0.16792257, 0.26317761,\n",
       "        0.26391351, 0.26652306, 0.26448682, 0.26317767, 0.26391355,\n",
       "        0.26652307, 0.26448688, 0.26317766, 0.26391356, 0.26652307,\n",
       "        0.26448686, 0.27698274, 0.27688259, 0.27653285, 0.27606019,\n",
       "        0.27698274, 0.27688247, 0.27653294, 0.27606031, 0.27698284,\n",
       "        0.27688241, 0.2765329 , 0.27606032, 0.27636287, 0.27334848,\n",
       "        0.27379552, 0.27404487, 0.27636119, 0.27330553, 0.27377848,\n",
       "        0.27401251, 0.27636218, 0.27331311, 0.27377959, 0.27401786,\n",
       "        0.21752492, 0.17010082, 0.17942781, 0.15309398, 0.21276473,\n",
       "        0.13137892, 0.18689852, 0.15691239, 0.21753288, 0.1350888 ,\n",
       "        0.16840903, 0.15159463, 0.26317715, 0.26391311, 0.26652286,\n",
       "        0.26448662, 0.26317767, 0.26391355, 0.26652307, 0.26448688,\n",
       "        0.26317759, 0.26391349, 0.26652306, 0.26448682, 0.27698116,\n",
       "        0.27688162, 0.27653306, 0.27606053, 0.27698219, 0.27688254,\n",
       "        0.27653252, 0.27606023, 0.27698197, 0.27688266, 0.27653277,\n",
       "        0.27605998, 0.27635701, 0.27382452, 0.27394928, 0.27432386,\n",
       "        0.27635943, 0.27330698, 0.27377659, 0.274013  , 0.27636286,\n",
       "        0.27336705, 0.27380222, 0.27406307, 0.24375285, 0.23675363,\n",
       "        0.23959456, 0.2389062 , 0.23249445, 0.15071399, 0.19375605,\n",
       "        0.17123505, 0.24241807, 0.22338458, 0.23918448, 0.20810052,\n",
       "        0.26317285, 0.26390993, 0.26652161, 0.26448418, 0.26317768,\n",
       "        0.26391357, 0.26652306, 0.26448686, 0.26317685, 0.26391293,\n",
       "        0.26652281, 0.26448642, 0.27697157, 0.27686541, 0.27653438,\n",
       "        0.27606276, 0.27697922, 0.27688155, 0.27653357, 0.27606098,\n",
       "        0.27697918, 0.27687837, 0.27653335, 0.27606134, 0.27626367,\n",
       "        0.2741845 , 0.27413038, 0.27422714, 0.2763523 , 0.27332173,\n",
       "        0.2737638 , 0.27402158, 0.27633999, 0.27409895, 0.27402034,\n",
       "        0.27436479, 0.27137226, 0.26813373, 0.26858737, 0.2673983 ,\n",
       "        0.27034951, 0.2596755 , 0.26584766, 0.25687396, 0.27125659,\n",
       "        0.2676989 , 0.26789222, 0.26904993, 0.26313187, 0.26387   ,\n",
       "        0.26650276, 0.26446473, 0.26317758, 0.26391351, 0.26652286,\n",
       "        0.2644867 , 0.26317073, 0.263908  , 0.2665203 , 0.26448287,\n",
       "        0.27684162, 0.27671609, 0.27644895, 0.27598285, 0.27696819,\n",
       "        0.27686793, 0.27653608, 0.27606638, 0.27694722, 0.27684204,\n",
       "        0.27653267, 0.27606253, 0.27444411, 0.27387584, 0.27395787,\n",
       "        0.2737938 , 0.27621271, 0.27332553, 0.27363238, 0.27407997,\n",
       "        0.27600147, 0.27444297, 0.27410779, 0.2741613 , 0.27497898,\n",
       "        0.27427901, 0.27416058, 0.27375329, 0.27626836, 0.27665353,\n",
       "        0.27373139, 0.27322085, 0.27606267, 0.27560016, 0.27428254,\n",
       "        0.2755011 , 0.26266753, 0.26346921, 0.26639183, 0.26431929,\n",
       "        0.2631769 , 0.26391297, 0.26652139, 0.26448543, 0.26310673,\n",
       "        0.26384661, 0.26648944, 0.26445643, 0.27512248, 0.27518939,\n",
       "        0.27589593, 0.27547995, 0.27677512, 0.27668641, 0.27648533,\n",
       "        0.27608106, 0.27656073, 0.2764814 , 0.27630908, 0.27592585,\n",
       "        0.26045531, 0.25941239, 0.26291884, 0.26277298, 0.27481922,\n",
       "        0.2741189 , 0.2726126 , 0.27325737, 0.27184037, 0.27164183,\n",
       "        0.27225569, 0.27225318, 0.26066037, 0.26031897, 0.26332519,\n",
       "        0.26284013, 0.27518155, 0.27558103, 0.27543083, 0.27512734,\n",
       "        0.27166579, 0.27262441, 0.27385132, 0.27363518, 0.25589597,\n",
       "        0.2578006 , 0.26605543, 0.26269768, 0.26317269, 0.26390698,\n",
       "        0.26650656, 0.2644709 , 0.26237655, 0.26326933, 0.26634242,\n",
       "        0.26422745, 0.26142635, 0.26165822, 0.26338767, 0.26334029,\n",
       "        0.27536756, 0.27536033, 0.27560599, 0.27530879, 0.27259958,\n",
       "        0.27282597, 0.27408752, 0.27386763, 0.15316954, 0.15316954,\n",
       "        0.24812149, 0.24812149, 0.26861749, 0.26976425, 0.26826759,\n",
       "        0.26900048, 0.25939172, 0.25682781, 0.26214656, 0.26220796,\n",
       "        0.15316954, 0.15316954, 0.24812149, 0.24812149, 0.27022296,\n",
       "        0.27061763, 0.27169443, 0.27159387, 0.25809088, 0.25963991,\n",
       "        0.2631243 , 0.26299338, 0.18504067, 0.18504067, 0.24923406,\n",
       "        0.24812149, 0.26314346, 0.26385724, 0.26635667, 0.26431017,\n",
       "        0.25110769, 0.25372499, 0.26636793, 0.26170269, 0.15316954,\n",
       "        0.15316954, 0.24812149, 0.24812149, 0.27032425, 0.27051312,\n",
       "        0.27177207, 0.27160613, 0.26025687, 0.26071558, 0.26323421,\n",
       "        0.26324916]),\n",
       " 'split2_test_average_precision': array([0.27822596, 0.27909777, 0.27794852, 0.27740869, 0.27822438,\n",
       "        0.27910254, 0.27794728, 0.27741053, 0.27822516, 0.27910226,\n",
       "        0.27794743, 0.27741066, 0.17605138, 0.17708481, 0.19245685,\n",
       "        0.18508084, 0.18557655, 0.16417774, 0.19631428, 0.18755255,\n",
       "        0.1564508 , 0.15590346, 0.1786977 , 0.20132222, 0.26548851,\n",
       "        0.26629218, 0.26856979, 0.26600029, 0.26548856, 0.26629222,\n",
       "        0.2685698 , 0.26600033, 0.26548855, 0.26629223, 0.2685698 ,\n",
       "        0.26600033, 0.2800102 , 0.27993392, 0.27906378, 0.27857157,\n",
       "        0.28000991, 0.27993398, 0.27906476, 0.27857158, 0.2800099 ,\n",
       "        0.27993399, 0.27906484, 0.27857153, 0.27823793, 0.27907684,\n",
       "        0.27794997, 0.27740797, 0.27822405, 0.27910262, 0.27794806,\n",
       "        0.27741074, 0.27822565, 0.27909752, 0.27794821, 0.27740862,\n",
       "        0.23371359, 0.18183244, 0.16320923, 0.20382757, 0.20145684,\n",
       "        0.15998131, 0.18596603, 0.20746839, 0.18115701, 0.15323905,\n",
       "        0.18452884, 0.18366357, 0.26548782, 0.26629162, 0.26856976,\n",
       "        0.26600007, 0.26548856, 0.26629222, 0.2685698 , 0.26600033,\n",
       "        0.26548847, 0.26629215, 0.26856979, 0.2660003 , 0.28001074,\n",
       "        0.27993202, 0.27906226, 0.27856932, 0.28000973, 0.27993412,\n",
       "        0.27906495, 0.27857181, 0.28000979, 0.27993357, 0.27906449,\n",
       "        0.27857161, 0.27836231, 0.27906751, 0.2779726 , 0.27738321,\n",
       "        0.27822155, 0.27910419, 0.27794772, 0.27741291, 0.27824857,\n",
       "        0.27906381, 0.27795502, 0.27740953, 0.25390829, 0.23939408,\n",
       "        0.22066625, 0.25251152, 0.22644214, 0.16850502, 0.20594933,\n",
       "        0.21174336, 0.24798746, 0.23154975, 0.25034155, 0.24566731,\n",
       "        0.26548281, 0.26628794, 0.26856825, 0.26599721, 0.26548856,\n",
       "        0.26629222, 0.26856978, 0.26600032, 0.26548766, 0.26629145,\n",
       "        0.26856941, 0.26599979, 0.28003322, 0.2799061 , 0.27905466,\n",
       "        0.27856473, 0.28000951, 0.27993145, 0.27906598, 0.27857249,\n",
       "        0.28001148, 0.2799295 , 0.27906321, 0.27856967, 0.27835974,\n",
       "        0.27905855, 0.27808089, 0.27729334, 0.27820535, 0.2791088 ,\n",
       "        0.27795917, 0.27743536, 0.27843945, 0.27909251, 0.2779941 ,\n",
       "        0.27737878, 0.27351515, 0.27505592, 0.26760693, 0.27030567,\n",
       "        0.27355842, 0.27256844, 0.26244639, 0.26199599, 0.2731899 ,\n",
       "        0.27519628, 0.26607434, 0.26948237, 0.26543558, 0.26624359,\n",
       "        0.26855555, 0.26596878, 0.26548842, 0.26629229, 0.26856957,\n",
       "        0.2660002 , 0.26547975, 0.26628584, 0.26856767, 0.2659958 ,\n",
       "        0.27995566, 0.27984352, 0.27910279, 0.27862471, 0.27999491,\n",
       "        0.27992058, 0.27907813, 0.27858591, 0.28002753, 0.27990737,\n",
       "        0.27908407, 0.27857319, 0.27575474, 0.27702833, 0.27736544,\n",
       "        0.27696177, 0.27794062, 0.27899436, 0.27797161, 0.27756324,\n",
       "        0.27774529, 0.27883633, 0.27790807, 0.27728761, 0.27642272,\n",
       "        0.27713   , 0.27740161, 0.27726788, 0.2795003 , 0.27934205,\n",
       "        0.27802032, 0.27752302, 0.27981923, 0.27896573, 0.27803302,\n",
       "        0.27761477, 0.26487724, 0.26577561, 0.26841593, 0.26578425,\n",
       "        0.26548853, 0.26629164, 0.26856737, 0.26599843, 0.26540478,\n",
       "        0.2662205 , 0.26854661, 0.26595249, 0.2774502 , 0.27750416,\n",
       "        0.27819147, 0.27775825, 0.2797839 , 0.27970926, 0.27911023,\n",
       "        0.27865686, 0.27947702, 0.27944023, 0.27905982, 0.27861953,\n",
       "        0.26085788, 0.26180102, 0.26470944, 0.26487619, 0.27589301,\n",
       "        0.27751488, 0.27705468, 0.2771474 , 0.2759792 , 0.27486975,\n",
       "        0.27545983, 0.27549209, 0.26008535, 0.26285453, 0.26352524,\n",
       "        0.26465423, 0.27791498, 0.27809221, 0.27813485, 0.27759101,\n",
       "        0.27382412, 0.27445868, 0.27602565, 0.27557631, 0.25762762,\n",
       "        0.2595613 , 0.26790474, 0.26388888, 0.26548673, 0.26628576,\n",
       "        0.26854933, 0.26598259, 0.26460498, 0.26552044, 0.26831199,\n",
       "        0.26567756, 0.26276973, 0.26325478, 0.26481958, 0.26513935,\n",
       "        0.27804141, 0.27805751, 0.27814785, 0.27796786, 0.27482616,\n",
       "        0.27506859, 0.27616255, 0.2761653 , 0.15317019, 0.15317019,\n",
       "        0.24959059, 0.24959059, 0.27383784, 0.27284848, 0.27281831,\n",
       "        0.27369479, 0.26326721, 0.25926663, 0.26418724, 0.26448508,\n",
       "        0.15317019, 0.15317019, 0.24959059, 0.24959059, 0.27182701,\n",
       "        0.27248002, 0.27364516, 0.273792  , 0.25966414, 0.2609905 ,\n",
       "        0.26315538, 0.26459171, 0.18630781, 0.18630781, 0.2506585 ,\n",
       "        0.24959059, 0.2654652 , 0.26622606, 0.26836567, 0.26581664,\n",
       "        0.2527155 , 0.25522001, 0.26813446, 0.26320506, 0.15317019,\n",
       "        0.15317019, 0.24959059, 0.24959059, 0.27225987, 0.27243179,\n",
       "        0.27362513, 0.27382742, 0.26117789, 0.2619808 , 0.26438581,\n",
       "        0.26483615]),\n",
       " 'mean_test_average_precision': array([0.27797418, 0.27688362, 0.2768891 , 0.27651879, 0.27797313,\n",
       "        0.27688444, 0.27688716, 0.27651712, 0.27797351, 0.27688452,\n",
       "        0.27688752, 0.27651746, 0.18947825, 0.19556333, 0.18701981,\n",
       "        0.18723009, 0.19655627, 0.16976514, 0.18946296, 0.17893711,\n",
       "        0.16841855, 0.16967829, 0.18497498, 0.18408964, 0.26585597,\n",
       "        0.26656891, 0.26875162, 0.26617972, 0.26585599, 0.26656895,\n",
       "        0.26875162, 0.26617977, 0.26585601, 0.26656894, 0.26875162,\n",
       "        0.26617976, 0.27938756, 0.27932189, 0.27869487, 0.27824121,\n",
       "        0.27938765, 0.27932207, 0.27869518, 0.27824124, 0.27938769,\n",
       "        0.27932204, 0.27869522, 0.27824126, 0.27798185, 0.27688988,\n",
       "        0.27690411, 0.27653474, 0.27797302, 0.27688459, 0.27688748,\n",
       "        0.27651747, 0.27797476, 0.27688414, 0.27688948, 0.27651823,\n",
       "        0.21997875, 0.18428755, 0.18690404, 0.18983026, 0.19603255,\n",
       "        0.16304884, 0.19477622, 0.19639606, 0.18858271, 0.16404816,\n",
       "        0.19011271, 0.17611257, 0.26585541, 0.26656843, 0.26875152,\n",
       "        0.26617948, 0.26585599, 0.26656895, 0.26875162, 0.26617977,\n",
       "        0.26585595, 0.26656889, 0.26875163, 0.26617971, 0.27938665,\n",
       "        0.2793208 , 0.278694  , 0.27824019, 0.27938743, 0.27932228,\n",
       "        0.27869517, 0.27824116, 0.2793872 , 0.27932208, 0.27869505,\n",
       "        0.27824107, 0.27803424, 0.27716661, 0.27698182, 0.27662538,\n",
       "        0.2779729 , 0.27688646, 0.27688775, 0.27651732, 0.27798844,\n",
       "        0.27689768, 0.27691293, 0.27654311, 0.25274098, 0.244678  ,\n",
       "        0.24007336, 0.23846456, 0.23150996, 0.18424419, 0.21589661,\n",
       "        0.19719343, 0.24867911, 0.23478465, 0.24827288, 0.23388519,\n",
       "        0.26585037, 0.26656484, 0.26875016, 0.26617696, 0.26585601,\n",
       "        0.26656896, 0.26875161, 0.26617975, 0.2658552 , 0.26656821,\n",
       "        0.26875132, 0.26617929, 0.27938957, 0.27930198, 0.27869378,\n",
       "        0.27823904, 0.27938577, 0.27932168, 0.27869633, 0.27824226,\n",
       "        0.27938572, 0.27931951, 0.27869529, 0.27824117, 0.27805437,\n",
       "        0.27735975, 0.27706453, 0.27659909, 0.27797333, 0.27690067,\n",
       "        0.27689432, 0.27653626, 0.2780852 , 0.2773297 , 0.27702084,\n",
       "        0.27665251, 0.27309578, 0.27109878, 0.26990159, 0.26629302,\n",
       "        0.27220848, 0.270715  , 0.26491089, 0.25948044, 0.27274835,\n",
       "        0.27443271, 0.26711014, 0.26860946, 0.26580525, 0.26652203,\n",
       "        0.26873361, 0.26615276, 0.26585592, 0.26656895, 0.26875144,\n",
       "        0.26617959, 0.26584768, 0.26656274, 0.26874913, 0.26617555,\n",
       "        0.27929116, 0.27920562, 0.27868055, 0.27822848, 0.27938043,\n",
       "        0.27931886, 0.27870877, 0.27825593, 0.27938664, 0.27929837,\n",
       "        0.27871035, 0.27824885, 0.27596395, 0.27628471, 0.27669423,\n",
       "        0.27625194, 0.27784217, 0.27686113, 0.27687549, 0.27662794,\n",
       "        0.27774419, 0.27737109, 0.27707875, 0.27658191, 0.27661915,\n",
       "        0.27652173, 0.2767992 , 0.27629711, 0.27869724, 0.27868444,\n",
       "        0.27726334, 0.27614058, 0.27864308, 0.27813985, 0.27747416,\n",
       "        0.2770554 , 0.26528991, 0.26607968, 0.26862206, 0.26598271,\n",
       "        0.26585548, 0.26656837, 0.26874988, 0.26617816, 0.26577739,\n",
       "        0.26649847, 0.26872251, 0.2661399 , 0.27708889, 0.27723201,\n",
       "        0.27793396, 0.27753334, 0.27920078, 0.27914784, 0.27872406,\n",
       "        0.2783128 , 0.27895093, 0.2789261 , 0.27859561, 0.27818901,\n",
       "        0.26201049, 0.26200656, 0.26448965, 0.26404685, 0.2762335 ,\n",
       "        0.2761369 , 0.27588782, 0.27591583, 0.27425448, 0.27378728,\n",
       "        0.27481223, 0.27467373, 0.26123133, 0.26218221, 0.2636947 ,\n",
       "        0.26431493, 0.27729436, 0.27753961, 0.27754633, 0.27721148,\n",
       "        0.27337785, 0.27407941, 0.27561192, 0.27540751, 0.25799167,\n",
       "        0.25981649, 0.26812741, 0.2642199 , 0.26585292, 0.2665639 ,\n",
       "        0.26873444, 0.26616321, 0.26499669, 0.26583561, 0.26855698,\n",
       "        0.26588538, 0.26243694, 0.26277429, 0.26458388, 0.26471648,\n",
       "        0.27748349, 0.27753564, 0.27771992, 0.2774725 , 0.2743656 ,\n",
       "        0.27463696, 0.27585098, 0.27575577, 0.15316976, 0.15316976,\n",
       "        0.2487907 , 0.2487907 , 0.27164911, 0.27138494, 0.27180951,\n",
       "        0.27212921, 0.26184688, 0.25974614, 0.26397077, 0.26357998,\n",
       "        0.15316976, 0.15316976, 0.2487907 , 0.2487907 , 0.27190469,\n",
       "        0.27237714, 0.27344682, 0.27339623, 0.25893416, 0.25996516,\n",
       "        0.26336774, 0.26423411, 0.18593318, 0.18593318, 0.24990961,\n",
       "        0.2487907 , 0.26583115, 0.26652538, 0.26857643, 0.2660068 ,\n",
       "        0.25288953, 0.25534443, 0.26816938, 0.26323481, 0.15316976,\n",
       "        0.15316976, 0.2487907 , 0.2487907 , 0.27207241, 0.27229561,\n",
       "        0.27346301, 0.27340146, 0.26108229, 0.26168444, 0.26424679,\n",
       "        0.26446211]),\n",
       " 'std_test_average_precision': array([1.22680230e-03, 2.55084783e-03, 2.23567716e-03, 1.79390830e-03,\n",
       "        1.22632120e-03, 2.55496194e-03, 2.23487391e-03, 1.79529999e-03,\n",
       "        1.22653329e-03, 2.55447026e-03, 2.23499945e-03, 1.79511909e-03,\n",
       "        1.42688863e-02, 1.31443277e-02, 9.06157466e-03, 4.27001017e-03,\n",
       "        1.74170766e-02, 2.73450608e-02, 6.39447301e-03, 2.81970570e-02,\n",
       "        1.77721481e-02, 9.98862242e-03, 5.32299197e-03, 1.36561516e-02,\n",
       "        2.35129414e-03, 2.28947903e-03, 1.89820536e-03, 1.46101772e-03,\n",
       "        2.35123496e-03, 2.28948024e-03, 1.89818331e-03, 1.46101475e-03,\n",
       "        2.35128189e-03, 2.28946176e-03, 1.89819379e-03, 1.46101708e-03,\n",
       "        1.76512260e-03, 1.79477767e-03, 1.63561129e-03, 1.66241880e-03,\n",
       "        1.76528339e-03, 1.79503557e-03, 1.63559662e-03, 1.66235477e-03,\n",
       "        1.76524235e-03, 1.79506457e-03, 1.63564314e-03, 1.66238577e-03,\n",
       "        1.23073506e-03, 2.52710844e-03, 2.23696172e-03, 1.78657744e-03,\n",
       "        1.22648442e-03, 2.55484691e-03, 2.23508503e-03, 1.79539471e-03,\n",
       "        1.22712480e-03, 2.54915477e-03, 2.23602984e-03, 1.79227478e-03,\n",
       "        1.03590246e-02, 1.27048747e-02, 2.30142733e-02, 2.62204704e-02,\n",
       "        1.63329660e-02, 2.71973239e-02, 1.18062517e-02, 2.88025509e-02,\n",
       "        2.12646497e-02, 2.90804218e-02, 2.03865957e-02, 1.77578511e-02,\n",
       "        2.35127317e-03, 2.28945190e-03, 1.89825891e-03, 1.46097691e-03,\n",
       "        2.35123489e-03, 2.28947487e-03, 1.89818391e-03, 1.46101554e-03,\n",
       "        2.35129099e-03, 2.28947462e-03, 1.89821153e-03, 1.46100590e-03,\n",
       "        1.76533276e-03, 1.79486663e-03, 1.63492471e-03, 1.66169449e-03,\n",
       "        1.76553861e-03, 1.79517394e-03, 1.63586774e-03, 1.66226199e-03,\n",
       "        1.76546081e-03, 1.79499633e-03, 1.63566483e-03, 1.66242399e-03,\n",
       "        1.25710325e-03, 2.37065936e-03, 2.18683286e-03, 1.65873608e-03,\n",
       "        1.22853449e-03, 2.55508565e-03, 2.23683038e-03, 1.79459959e-03,\n",
       "        1.23485969e-03, 2.51788804e-03, 2.23916938e-03, 1.77998616e-03,\n",
       "        6.91169325e-03, 9.40167264e-03, 1.60448839e-02, 1.16537812e-02,\n",
       "        3.80023818e-03, 3.55877606e-02, 2.32291726e-02, 1.84000976e-02,\n",
       "        5.41660609e-03, 1.08721111e-02, 6.73684490e-03, 1.82550329e-02,\n",
       "        2.35065549e-03, 2.28915082e-03, 1.89822229e-03, 1.46106358e-03,\n",
       "        2.35125876e-03, 2.28947717e-03, 1.89820136e-03, 1.46101143e-03,\n",
       "        2.35131302e-03, 2.28938439e-03, 1.89820634e-03, 1.46102401e-03,\n",
       "        1.77100029e-03, 1.79440982e-03, 1.63583268e-03, 1.66001696e-03,\n",
       "        1.76625018e-03, 1.79595353e-03, 1.63599862e-03, 1.66267280e-03,\n",
       "        1.76580308e-03, 1.79669798e-03, 1.63583094e-03, 1.66202351e-03,\n",
       "        1.35475821e-03, 2.24707675e-03, 2.10714159e-03, 1.72461039e-03,\n",
       "        1.23975354e-03, 2.55367516e-03, 2.25100408e-03, 1.80204448e-03,\n",
       "        1.30461030e-03, 2.28765446e-03, 2.16486021e-03, 1.65319863e-03,\n",
       "        1.27112312e-03, 2.91174075e-03, 2.58306719e-03, 3.80859561e-03,\n",
       "        1.35858505e-03, 8.36041528e-03, 1.75927963e-03, 2.09205046e-03,\n",
       "        1.08370112e-03, 5.21444127e-03, 7.63524515e-04, 9.45331347e-04,\n",
       "        2.34832037e-03, 2.28753800e-03, 1.89835114e-03, 1.45919356e-03,\n",
       "        2.35129336e-03, 2.28948237e-03, 1.89824146e-03, 1.46099348e-03,\n",
       "        2.35036537e-03, 2.28902699e-03, 1.89825747e-03, 1.46098927e-03,\n",
       "        1.79147506e-03, 1.82877143e-03, 1.67650849e-03, 1.69510122e-03,\n",
       "        1.77279748e-03, 1.80634831e-03, 1.64408289e-03, 1.66943262e-03,\n",
       "        1.78850443e-03, 1.80896823e-03, 1.64683061e-03, 1.66855342e-03,\n",
       "        1.33457480e-03, 1.74439112e-03, 2.01685226e-03, 1.78912758e-03,\n",
       "        1.29212718e-03, 2.51779161e-03, 2.33301387e-03, 1.82278942e-03,\n",
       "        1.42248040e-03, 2.07049480e-03, 2.16801943e-03, 1.76051871e-03,\n",
       "        1.42616032e-03, 1.64024435e-03, 1.95544729e-03, 1.81548053e-03,\n",
       "        1.75001073e-03, 1.46550000e-03, 2.62983316e-03, 2.06553000e-03,\n",
       "        1.82699830e-03, 1.83205720e-03, 2.41041086e-03, 1.11334676e-03,\n",
       "        2.32799244e-03, 2.26579714e-03, 1.91069742e-03, 1.44602991e-03,\n",
       "        2.35121938e-03, 2.28947947e-03, 1.89845230e-03, 1.46102534e-03,\n",
       "        2.34753450e-03, 2.28717461e-03, 1.89917983e-03, 1.45708738e-03,\n",
       "        1.48027369e-03, 1.56853493e-03, 1.56951906e-03, 1.59272503e-03,\n",
       "        1.79060200e-03, 1.82426478e-03, 1.69243412e-03, 1.69925588e-03,\n",
       "        1.77620886e-03, 1.82281787e-03, 1.70924495e-03, 1.69958525e-03,\n",
       "        1.92173638e-03, 2.20684134e-03, 1.20292039e-03, 9.14356750e-04,\n",
       "        1.31597040e-03, 1.45839748e-03, 2.34760525e-03, 1.88150020e-03,\n",
       "        1.75858305e-03, 1.51708293e-03, 1.87965856e-03, 1.74124319e-03,\n",
       "        1.23654692e-03, 1.33441141e-03, 3.89757455e-04, 1.09232367e-03,\n",
       "        1.53577089e-03, 1.42806775e-03, 1.54416733e-03, 1.56986196e-03,\n",
       "        1.25598933e-03, 1.06740180e-03, 1.30191149e-03, 1.38334516e-03,\n",
       "        1.87748788e-03, 1.75942291e-03, 1.78960896e-03, 1.39775805e-03,\n",
       "        2.35219247e-03, 2.29137353e-03, 1.89914541e-03, 1.46109458e-03,\n",
       "        2.31587343e-03, 2.23516191e-03, 1.91605244e-03, 1.44602418e-03,\n",
       "        7.28345361e-04, 7.91716794e-04, 8.96104692e-04, 9.96915646e-04,\n",
       "        1.55090002e-03, 1.60605078e-03, 1.58055187e-03, 1.60316524e-03,\n",
       "        1.29552853e-03, 1.33773028e-03, 1.33101874e-03, 1.40464176e-03,\n",
       "        3.08308022e-07, 3.08308022e-07, 6.06832910e-04, 6.06832910e-04,\n",
       "        2.21297045e-03, 1.26402474e-03, 2.58067010e-03, 2.21234749e-03,\n",
       "        1.74318042e-03, 2.60076369e-03, 1.40941773e-03, 9.86520480e-04,\n",
       "        3.08308022e-07, 3.08308022e-07, 6.06832910e-04, 6.06832910e-04,\n",
       "        1.40591902e-03, 1.39653888e-03, 1.35711931e-03, 1.33961343e-03,\n",
       "        6.47258953e-04, 7.40994164e-04, 3.22544936e-04, 9.03183447e-04,\n",
       "        6.33803698e-04, 6.33803698e-04, 5.83836171e-04, 6.06832910e-04,\n",
       "        2.35812861e-03, 2.31044226e-03, 1.90431104e-03, 1.46909182e-03,\n",
       "        1.53085897e-03, 1.37587280e-03, 1.48533608e-03, 1.26329257e-03,\n",
       "        3.08308022e-07, 3.08308022e-07, 6.06832910e-04, 6.06832910e-04,\n",
       "        1.35732725e-03, 1.40310663e-03, 1.31945287e-03, 1.32663165e-03,\n",
       "        6.38517609e-04, 7.02079319e-04, 7.76261271e-04, 8.78430792e-04]),\n",
       " 'rank_test_average_precision': array([ 67, 115, 105, 130,  70, 113, 109, 135,  68, 112, 107, 133, 312,\n",
       "        308, 316, 315, 305, 326, 313, 324, 328, 327, 320, 323, 238, 203,\n",
       "        177, 219, 237, 200, 179, 215, 234, 202, 178, 217,   4,  16,  39,\n",
       "         52,   3,  14,  36,  51,   2,  15,  35,  50,  65, 103,  99, 128,\n",
       "         71, 111, 108, 132,  66, 114, 104, 131, 302, 321, 317, 311, 307,\n",
       "        330, 309, 306, 314, 329, 310, 325, 242, 205, 182, 222, 236, 201,\n",
       "        180, 216, 239, 204, 176, 220,   7,  18,  40,  56,   5,  12,  37,\n",
       "         54,   6,  13,  38,  55,  63,  91,  97, 122,  72, 110, 106, 134,\n",
       "         64, 101,  98, 126, 285, 296, 297, 298, 301, 322, 303, 304, 294,\n",
       "        299, 295, 300, 245, 208, 185, 225, 235, 198, 181, 218, 243, 207,\n",
       "        184, 223,   1,  21,  41,  57,   9,  17,  33,  49,  10,  19,  34,\n",
       "         53,  62,  85,  94, 124,  69, 100, 102, 127,  61,  86,  96, 120,\n",
       "        162, 173, 175, 214, 166, 174, 253, 280, 163, 152, 197, 192, 249,\n",
       "        212, 189, 228, 240, 199, 183, 221, 246, 210, 187, 226,  23,  24,\n",
       "         43,  58,  11,  20,  31,  47,   8,  22,  30,  48, 142, 137, 119,\n",
       "        138,  74, 117, 116, 121,  75,  84,  93, 125, 123, 129, 118, 136,\n",
       "         32,  42,  88, 140,  44,  60,  82,  95, 251, 230, 191, 232, 241,\n",
       "        206, 186, 224, 250, 213, 190, 229,  92,  89,  73,  80,  25,  26,\n",
       "         29,  46,  27,  28,  45,  59, 271, 272, 256, 262, 139, 141, 144,\n",
       "        143, 154, 156, 149, 150, 275, 270, 264, 258,  87,  78,  77,  90,\n",
       "        161, 155, 147, 148, 282, 278, 196, 261, 244, 209, 188, 227, 252,\n",
       "        247, 194, 233, 269, 268, 255, 254,  81,  79,  76,  83, 153, 151,\n",
       "        145, 146, 331, 331, 287, 287, 171, 172, 170, 167, 273, 279, 263,\n",
       "        265, 331, 331, 287, 287, 169, 164, 158, 160, 281, 277, 266, 260,\n",
       "        318, 318, 286, 287, 248, 211, 193, 231, 284, 283, 195, 267, 331,\n",
       "        331, 287, 287, 168, 165, 157, 159, 276, 274, 259, 257]),\n",
       " 'split0_test_roc_auc': array([0.69087526, 0.69019927, 0.69058965, 0.68990353, 0.6908751 ,\n",
       "        0.69020049, 0.69058818, 0.68990241, 0.69087512, 0.69020029,\n",
       "        0.69058841, 0.68990256, 0.57037415, 0.60161502, 0.56248594,\n",
       "        0.59147325, 0.62796629, 0.60205283, 0.59396885, 0.6205674 ,\n",
       "        0.5110968 , 0.5560208 , 0.58905295, 0.58192096, 0.68035005,\n",
       "        0.68113831, 0.68278172, 0.68016279, 0.68035008, 0.68113834,\n",
       "        0.68278172, 0.68016281, 0.68035007, 0.68113833, 0.68278172,\n",
       "        0.68016281, 0.6922184 , 0.69216375, 0.69202229, 0.69187819,\n",
       "        0.69221876, 0.69216448, 0.69202252, 0.69187842, 0.69221868,\n",
       "        0.69216435, 0.69202245, 0.6918784 , 0.69087707, 0.69019727,\n",
       "        0.69060126, 0.68991264, 0.69087516, 0.69020046, 0.69058805,\n",
       "        0.68990225, 0.6908754 , 0.69019858, 0.6905902 , 0.68990399,\n",
       "        0.60848604, 0.58766736, 0.61252408, 0.61224894, 0.54575172,\n",
       "        0.58555013, 0.6142116 , 0.63006234, 0.53656469, 0.59754371,\n",
       "        0.61838799, 0.58104556, 0.68034975, 0.68113807, 0.68278167,\n",
       "        0.68016261, 0.68035008, 0.68113834, 0.68278172, 0.68016281,\n",
       "        0.68035004, 0.68113829, 0.68278172, 0.68016278, 0.69221602,\n",
       "        0.6921615 , 0.69202047, 0.69187633, 0.6922186 , 0.6921644 ,\n",
       "        0.69202237, 0.69187823, 0.69221815, 0.69216361, 0.692022  ,\n",
       "        0.69187801, 0.69087431, 0.69041038, 0.6906416 , 0.68990759,\n",
       "        0.69087498, 0.69019987, 0.69058667, 0.68990063, 0.69087762,\n",
       "        0.69020511, 0.69060518, 0.6899115 , 0.67306565, 0.67158435,\n",
       "        0.67310228, 0.6442003 , 0.64456691, 0.64419704, 0.66146596,\n",
       "        0.60880144, 0.66795402, 0.66076149, 0.66733464, 0.65793004,\n",
       "        0.68034669, 0.68113557, 0.68278106, 0.68016085, 0.68035008,\n",
       "        0.68113833, 0.68278171, 0.6801628 , 0.68034961, 0.68113792,\n",
       "        0.68278163, 0.6801625 , 0.6922071 , 0.69214603, 0.69201149,\n",
       "        0.6918659 , 0.692217  , 0.69216333, 0.69202097, 0.69187677,\n",
       "        0.69221292, 0.69215954, 0.69201895, 0.69187397, 0.69071871,\n",
       "        0.69047056, 0.69055984, 0.68989634, 0.6908702 , 0.69019238,\n",
       "        0.6905707 , 0.68988436, 0.69091429, 0.69054394, 0.69063905,\n",
       "        0.68989725, 0.68582069, 0.68283642, 0.6856942 , 0.67700623,\n",
       "        0.68428455, 0.68978315, 0.68242293, 0.67366782, 0.68521912,\n",
       "        0.69039805, 0.67937172, 0.68000392, 0.68031645, 0.6811112 ,\n",
       "        0.68277398, 0.68014391, 0.68035004, 0.68113829, 0.6827816 ,\n",
       "        0.68016266, 0.68034502, 0.68113423, 0.68278056, 0.68015986,\n",
       "        0.69208098, 0.69204761, 0.69192886, 0.69178664, 0.69219932,\n",
       "        0.69214811, 0.69200656, 0.69186289, 0.69219279, 0.69212494,\n",
       "        0.6919932 , 0.69184757, 0.68870931, 0.68917855, 0.68971065,\n",
       "        0.68897481, 0.69066923, 0.68996787, 0.69038803, 0.68970553,\n",
       "        0.69056313, 0.69022255, 0.69043952, 0.68967591, 0.68955378,\n",
       "        0.68940854, 0.68971601, 0.68872565, 0.69152108, 0.69123581,\n",
       "        0.69095077, 0.68919824, 0.69127823, 0.69114002, 0.6909652 ,\n",
       "        0.68926993, 0.6799781 , 0.68083819, 0.68278718, 0.68002134,\n",
       "        0.68034982, 0.68113797, 0.68278031, 0.68016134, 0.68029922,\n",
       "        0.68109763, 0.68276835, 0.68013326, 0.6898866 , 0.69006589,\n",
       "        0.69086016, 0.69080085, 0.69187822, 0.69186207, 0.69183076,\n",
       "        0.69169789, 0.69166564, 0.69168106, 0.69169165, 0.69156516,\n",
       "        0.67874587, 0.67891521, 0.67963377, 0.6783805 , 0.68890083,\n",
       "        0.68805469, 0.68888441, 0.68823454, 0.68621813, 0.68656043,\n",
       "        0.68775247, 0.68717691, 0.67796027, 0.67829152, 0.67864661,\n",
       "        0.67955597, 0.68968249, 0.68979542, 0.69023186, 0.68997549,\n",
       "        0.68651923, 0.68697282, 0.68856777, 0.6885552 , 0.67384447,\n",
       "        0.67581483, 0.68318096, 0.67877892, 0.68034801, 0.68113455,\n",
       "        0.68276746, 0.68014806, 0.67975643, 0.68065338, 0.68279959,\n",
       "        0.6799571 , 0.67810589, 0.67834522, 0.679725  , 0.67988326,\n",
       "        0.68983312, 0.68991273, 0.69036249, 0.69031678, 0.68738801,\n",
       "        0.68764324, 0.68878449, 0.6888757 , 0.5       , 0.5       ,\n",
       "        0.66500535, 0.66500535, 0.68368621, 0.68369106, 0.68505643,\n",
       "        0.68457249, 0.67683124, 0.67733632, 0.67876516, 0.67731508,\n",
       "        0.5       , 0.5       , 0.66500535, 0.66500535, 0.68513701,\n",
       "        0.68544578, 0.6864585 , 0.68642246, 0.67433785, 0.67458526,\n",
       "        0.67821458, 0.67916643, 0.55539665, 0.55539665, 0.66543077,\n",
       "        0.66500535, 0.68033199, 0.68110171, 0.68263708, 0.68001617,\n",
       "        0.66850806, 0.67133875, 0.68320681, 0.67789857, 0.5       ,\n",
       "        0.5       , 0.66500535, 0.66500535, 0.68511231, 0.68536227,\n",
       "        0.68642243, 0.6863812 , 0.67664526, 0.67708454, 0.67925986,\n",
       "        0.67935467]),\n",
       " 'split1_test_roc_auc': array([0.69066941, 0.6880681 , 0.68931237, 0.68896157, 0.69066806,\n",
       "        0.68806431, 0.68931022, 0.68895871, 0.69066826, 0.68806491,\n",
       "        0.68931058, 0.68895918, 0.6137942 , 0.60166794, 0.59426253,\n",
       "        0.56843949, 0.56328764, 0.46800411, 0.57548622, 0.47114785,\n",
       "        0.59187104, 0.54141238, 0.58875577, 0.54116126, 0.6785343 ,\n",
       "        0.67926844, 0.68119768, 0.67903777, 0.67853433, 0.67926848,\n",
       "        0.6811977 , 0.6790378 , 0.67853432, 0.67926847, 0.68119769,\n",
       "        0.67903779, 0.69119662, 0.69118941, 0.69109451, 0.69087865,\n",
       "        0.69119659, 0.69118944, 0.69109446, 0.69087864, 0.69119659,\n",
       "        0.69118944, 0.69109447, 0.69087865, 0.69067275, 0.68809609,\n",
       "        0.68933265, 0.68898772, 0.69066782, 0.68806428, 0.68931003,\n",
       "        0.6889586 , 0.69066984, 0.68806995, 0.68931346, 0.68896286,\n",
       "        0.62391291, 0.54634892, 0.56894172, 0.51503345, 0.61128969,\n",
       "        0.44554292, 0.58335642, 0.5166546 , 0.61722394, 0.45969863,\n",
       "        0.55294119, 0.50886992, 0.67853401, 0.67926813, 0.6811976 ,\n",
       "        0.67903757, 0.67853433, 0.67926848, 0.6811977 , 0.67903779,\n",
       "        0.67853429, 0.67926843, 0.68119767, 0.67903776, 0.69119664,\n",
       "        0.69118872, 0.69109454, 0.69087863, 0.69119644, 0.69118934,\n",
       "        0.69109439, 0.69087855, 0.6911965 , 0.69118918, 0.69109441,\n",
       "        0.69087849, 0.69067182, 0.68837995, 0.68950344, 0.68921292,\n",
       "        0.6906659 , 0.68806392, 0.68930826, 0.68895745, 0.69067234,\n",
       "        0.68810696, 0.68934223, 0.68900075, 0.66076671, 0.64805428,\n",
       "        0.6557757 , 0.64957144, 0.64032038, 0.50620791, 0.60067228,\n",
       "        0.54770702, 0.65919662, 0.62968076, 0.65321891, 0.616205  ,\n",
       "        0.67853081, 0.6792655 , 0.68119675, 0.67903545, 0.67853433,\n",
       "        0.67926847, 0.68119768, 0.67903778, 0.67853385, 0.679268  ,\n",
       "        0.68119754, 0.67903743, 0.69119047, 0.69118117, 0.69109335,\n",
       "        0.69087758, 0.69119416, 0.69118744, 0.69109332, 0.69087759,\n",
       "        0.69119421, 0.69118668, 0.69109349, 0.69087765, 0.69058105,\n",
       "        0.6887193 , 0.68966698, 0.68911489, 0.69064502, 0.68805699,\n",
       "        0.68929254, 0.68894782, 0.6906395 , 0.68855511, 0.68957197,\n",
       "        0.68920599, 0.6868513 , 0.68298037, 0.68401098, 0.67814269,\n",
       "        0.68567827, 0.67582524, 0.68157964, 0.6707976 , 0.68687598,\n",
       "        0.68239662, 0.68364767, 0.6823635 , 0.67849957, 0.67923869,\n",
       "        0.6811887 , 0.67902012, 0.67853429, 0.6792684 , 0.68119755,\n",
       "        0.67903764, 0.67852903, 0.67926392, 0.68119612, 0.67903415,\n",
       "        0.69109722, 0.6910678 , 0.69103491, 0.69080968, 0.69117374,\n",
       "        0.69116636, 0.69108209, 0.69086689, 0.69116496, 0.69115375,\n",
       "        0.69108107, 0.69086277, 0.68848042, 0.68754346, 0.68882781,\n",
       "        0.68834241, 0.690367  , 0.68789661, 0.68911269, 0.68882319,\n",
       "        0.6901332 , 0.68866255, 0.68955836, 0.68891406, 0.68886585,\n",
       "        0.68824721, 0.68886057, 0.68802626, 0.69017148, 0.69072932,\n",
       "        0.68917306, 0.68820097, 0.69026288, 0.68982668, 0.68961789,\n",
       "        0.68942335, 0.67815352, 0.67896852, 0.6811742 , 0.67890807,\n",
       "        0.67853386, 0.67926788, 0.68119614, 0.67903605, 0.67848167,\n",
       "        0.67922285, 0.68118273, 0.67901266, 0.68898736, 0.68912403,\n",
       "        0.69002313, 0.68982203, 0.69088916, 0.69089618, 0.69093447,\n",
       "        0.69073536, 0.69067877, 0.69069061, 0.69080899, 0.69060998,\n",
       "        0.67663132, 0.67575242, 0.67846581, 0.6787582 , 0.68853026,\n",
       "        0.6883109 , 0.68767229, 0.6876209 , 0.6853371 , 0.68549451,\n",
       "        0.68684027, 0.686756  , 0.67688423, 0.67588975, 0.67927945,\n",
       "        0.67879447, 0.68886078, 0.68920441, 0.68956917, 0.68941815,\n",
       "        0.68581653, 0.68671961, 0.6880534 , 0.68794026, 0.6723976 ,\n",
       "        0.6745541 , 0.68167316, 0.6776228 , 0.67852976, 0.67926192,\n",
       "        0.68118244, 0.67902107, 0.67792143, 0.67884989, 0.6811729 ,\n",
       "        0.67883404, 0.67761705, 0.67779918, 0.67936394, 0.67939547,\n",
       "        0.68904265, 0.68911668, 0.6896521 , 0.68953822, 0.68665809,\n",
       "        0.68692539, 0.68816931, 0.6881195 , 0.5       , 0.5       ,\n",
       "        0.66478626, 0.66478626, 0.68213898, 0.6827068 , 0.68295108,\n",
       "        0.68338528, 0.67543714, 0.67297605, 0.67738201, 0.67778859,\n",
       "        0.5       , 0.5       , 0.66478626, 0.66478626, 0.68400346,\n",
       "        0.68442413, 0.68554329, 0.68567344, 0.67406194, 0.67511287,\n",
       "        0.67876624, 0.67858548, 0.55345034, 0.55345034, 0.66506372,\n",
       "        0.66478626, 0.67849062, 0.67920214, 0.68104276, 0.67887223,\n",
       "        0.66732204, 0.67050546, 0.68206465, 0.67684092, 0.5       ,\n",
       "        0.5       , 0.66478626, 0.66478626, 0.68410964, 0.68431374,\n",
       "        0.68562085, 0.68565255, 0.67616969, 0.67658998, 0.67889866,\n",
       "        0.67898598]),\n",
       " 'split2_test_roc_auc': array([0.69221898, 0.69157173, 0.6917935 , 0.69121574, 0.69221994,\n",
       "        0.69157093, 0.69179251, 0.69121521, 0.69221999, 0.69157111,\n",
       "        0.69179264, 0.69121532, 0.552353  , 0.53102098, 0.59229283,\n",
       "        0.57647661, 0.57266549, 0.50808546, 0.59973051, 0.58554963,\n",
       "        0.50942065, 0.49031839, 0.56547596, 0.60871436, 0.68052872,\n",
       "        0.68131687, 0.68382023, 0.68162007, 0.68052878, 0.68131691,\n",
       "        0.68382024, 0.68162009, 0.68052876, 0.68131691, 0.68382024,\n",
       "        0.68162009, 0.69344588, 0.69341788, 0.69313168, 0.69285734,\n",
       "        0.69344635, 0.69341792, 0.6931317 , 0.69285735, 0.69344624,\n",
       "        0.69341791, 0.6931317 , 0.69285734, 0.69221406, 0.6915732 ,\n",
       "        0.69180248, 0.69122216, 0.69221982, 0.6915708 , 0.69179245,\n",
       "        0.69121516, 0.69221853, 0.69157188, 0.69179393, 0.69121604,\n",
       "        0.64299299, 0.56663768, 0.53967603, 0.59151674, 0.59787735,\n",
       "        0.49557834, 0.57915127, 0.5998131 , 0.55968395, 0.48198789,\n",
       "        0.57464784, 0.55404169, 0.68052832, 0.68131661, 0.68382022,\n",
       "        0.68161989, 0.68052878, 0.68131691, 0.68382024, 0.68162009,\n",
       "        0.6805287 , 0.68131686, 0.68382023, 0.68162006, 0.69344541,\n",
       "        0.69341723, 0.69313121, 0.69285735, 0.69344617, 0.69341782,\n",
       "        0.69313165, 0.69285731, 0.69344538, 0.69341769, 0.69313166,\n",
       "        0.69285733, 0.69221785, 0.69158678, 0.69182933, 0.69127443,\n",
       "        0.69221818, 0.69156903, 0.6917915 , 0.69121499, 0.6922203 ,\n",
       "        0.69156903, 0.69180666, 0.69122531, 0.66723616, 0.63763964,\n",
       "        0.63669737, 0.66421711, 0.6295981 , 0.52980812, 0.60993445,\n",
       "        0.60370914, 0.65849724, 0.6280592 , 0.66195255, 0.65513872,\n",
       "        0.68052451, 0.68131385, 0.68381991, 0.68161805, 0.68052877,\n",
       "        0.68131691, 0.68382022, 0.68162007, 0.68052812, 0.68131645,\n",
       "        0.68382016, 0.68161978, 0.69344417, 0.6934099 , 0.69312701,\n",
       "        0.69284842, 0.69344479, 0.69341616, 0.69313132, 0.69285697,\n",
       "        0.69344394, 0.6934152 , 0.69313062, 0.69285662, 0.69215722,\n",
       "        0.6914592 , 0.69184291, 0.69119083, 0.69219907, 0.69155143,\n",
       "        0.6917818 , 0.69121295, 0.69223094, 0.69156007, 0.69182341,\n",
       "        0.69126694, 0.68834217, 0.68690932, 0.68240889, 0.68559462,\n",
       "        0.68794984, 0.68327694, 0.67815008, 0.67521124, 0.68826613,\n",
       "        0.689624  , 0.68323989, 0.68509407, 0.68048684, 0.68128556,\n",
       "        0.68381573, 0.68159968, 0.68052873, 0.68131686, 0.68382006,\n",
       "        0.68161994, 0.68052251, 0.68131222, 0.68381954, 0.68161694,\n",
       "        0.69326033, 0.69325525, 0.69307883, 0.69279967, 0.69342855,\n",
       "        0.6933985 , 0.69312608, 0.6928527 , 0.69342583, 0.69339101,\n",
       "        0.69311885, 0.69284073, 0.69011805, 0.68978546, 0.6908239 ,\n",
       "        0.69067179, 0.69192354, 0.69132976, 0.69164833, 0.69116039,\n",
       "        0.6917067 , 0.69116288, 0.69155779, 0.69100278, 0.69030088,\n",
       "        0.69030576, 0.69086545, 0.69084213, 0.69272859, 0.69197099,\n",
       "        0.69169871, 0.69092052, 0.69259906, 0.69159394, 0.69160672,\n",
       "        0.69115734, 0.68006762, 0.68096745, 0.68376875, 0.68147061,\n",
       "        0.68052856, 0.68131645, 0.68381851, 0.68161846, 0.68046501,\n",
       "        0.68126906, 0.68381204, 0.68158793, 0.69130327, 0.69135252,\n",
       "        0.69209621, 0.69184914, 0.69317859, 0.69315532, 0.69302396,\n",
       "        0.69277379, 0.69287742, 0.6928842 , 0.69289494, 0.69264709,\n",
       "        0.67778598, 0.67884602, 0.68086027, 0.68116717, 0.69000089,\n",
       "        0.68994388, 0.69044723, 0.69040791, 0.68886384, 0.68829945,\n",
       "        0.68897626, 0.68928358, 0.67663358, 0.67929229, 0.68007561,\n",
       "        0.68105499, 0.69129344, 0.6914688 , 0.69178571, 0.6917319 ,\n",
       "        0.68812512, 0.68868349, 0.69008914, 0.6900756 , 0.67357705,\n",
       "        0.67580883, 0.68401435, 0.68001335, 0.68052635, 0.68131226,\n",
       "        0.68380323, 0.68160407, 0.67986787, 0.68078803, 0.68372302,\n",
       "        0.68139144, 0.67966927, 0.68004291, 0.68130605, 0.68154028,\n",
       "        0.69157866, 0.69160131, 0.69192677, 0.69184622, 0.68919848,\n",
       "        0.68938347, 0.69036409, 0.69038198, 0.5       , 0.5       ,\n",
       "        0.66694794, 0.66694794, 0.68623197, 0.68640337, 0.6866489 ,\n",
       "        0.68741759, 0.6798435 , 0.67628556, 0.67972601, 0.68050323,\n",
       "        0.5       , 0.5       , 0.66694794, 0.66694794, 0.68653441,\n",
       "        0.68710392, 0.68809845, 0.68829909, 0.67614329, 0.67735278,\n",
       "        0.67940865, 0.68058626, 0.55385306, 0.55385306, 0.66731569,\n",
       "        0.66694794, 0.68050884, 0.68126941, 0.68364962, 0.68145707,\n",
       "        0.66826264, 0.67143693, 0.68430783, 0.67946511, 0.5       ,\n",
       "        0.5       , 0.66694794, 0.66694794, 0.68695996, 0.68706536,\n",
       "        0.68810058, 0.68828784, 0.67788182, 0.67856886, 0.68076014,\n",
       "        0.68110047]),\n",
       " 'mean_test_roc_auc': array([0.69125455, 0.68994637, 0.69056517, 0.69002695, 0.69125436,\n",
       "        0.68994524, 0.69056364, 0.69002544, 0.69125446, 0.68994544,\n",
       "        0.69056387, 0.69002569, 0.57884045, 0.57810131, 0.58301377,\n",
       "        0.57879645, 0.58797314, 0.52604747, 0.58972852, 0.55908829,\n",
       "        0.53746283, 0.52925052, 0.58109489, 0.57726553, 0.67980436,\n",
       "        0.68057454, 0.68259988, 0.68027354, 0.6798044 , 0.68057457,\n",
       "        0.68259989, 0.68027357, 0.67980439, 0.68057457, 0.68259988,\n",
       "        0.68027356, 0.69228697, 0.69225701, 0.69208282, 0.69187139,\n",
       "        0.69228723, 0.69225728, 0.69208289, 0.69187147, 0.69228717,\n",
       "        0.69225723, 0.69208287, 0.69187146, 0.69125463, 0.68995552,\n",
       "        0.69057879, 0.69004084, 0.69125427, 0.68994518, 0.69056351,\n",
       "        0.69002534, 0.69125459, 0.6899468 , 0.69056586, 0.69002763,\n",
       "        0.62513065, 0.56688465, 0.57371394, 0.57293304, 0.58497292,\n",
       "        0.50889046, 0.59223976, 0.58217668, 0.57115753, 0.51307674,\n",
       "        0.58199234, 0.54798572, 0.67980403, 0.68057427, 0.68259983,\n",
       "        0.68027336, 0.6798044 , 0.68057458, 0.68259988, 0.68027356,\n",
       "        0.67980434, 0.68057453, 0.68259987, 0.68027353, 0.69228602,\n",
       "        0.69225581, 0.69208207, 0.69187077, 0.69228707, 0.69225718,\n",
       "        0.6920828 , 0.69187136, 0.69228668, 0.69225683, 0.69208269,\n",
       "        0.69187128, 0.69125466, 0.6901257 , 0.69065812, 0.69013165,\n",
       "        0.69125302, 0.68994427, 0.69056214, 0.69002436, 0.69125675,\n",
       "        0.68996036, 0.69058469, 0.69004585, 0.66702284, 0.65242609,\n",
       "        0.65519178, 0.65266295, 0.6381618 , 0.56007102, 0.62402423,\n",
       "        0.5867392 , 0.66188263, 0.63950048, 0.66083537, 0.64309126,\n",
       "        0.67980067, 0.68057164, 0.68259924, 0.68027145, 0.67980439,\n",
       "        0.68057457, 0.68259987, 0.68027355, 0.67980386, 0.68057413,\n",
       "        0.68259978, 0.68027324, 0.69228058, 0.6922457 , 0.69207729,\n",
       "        0.69186396, 0.69228532, 0.69225564, 0.69208187, 0.69187044,\n",
       "        0.69228369, 0.69225381, 0.69208102, 0.69186941, 0.69115232,\n",
       "        0.69021635, 0.69068991, 0.69006735, 0.6912381 , 0.6899336 ,\n",
       "        0.69054835, 0.69001504, 0.69126158, 0.6902197 , 0.69067814,\n",
       "        0.69012339, 0.68700472, 0.68424203, 0.68403802, 0.68024785,\n",
       "        0.68597089, 0.68296177, 0.68071755, 0.67322556, 0.68678708,\n",
       "        0.68747289, 0.68208643, 0.68248716, 0.67976762, 0.68054515,\n",
       "        0.68259281, 0.68025457, 0.67980435, 0.68057452, 0.68259973,\n",
       "        0.68027341, 0.67979885, 0.68057012, 0.68259874, 0.68027031,\n",
       "        0.69214618, 0.69212355, 0.6920142 , 0.69179866, 0.69226721,\n",
       "        0.69223766, 0.69207158, 0.69186083, 0.69226119, 0.69222323,\n",
       "        0.69206437, 0.69185036, 0.68910259, 0.68883583, 0.68978745,\n",
       "        0.68932967, 0.69098659, 0.68973141, 0.69038302, 0.68989637,\n",
       "        0.69080101, 0.69001599, 0.69051856, 0.68986425, 0.6895735 ,\n",
       "        0.6893205 , 0.68981401, 0.68919801, 0.69147372, 0.69131204,\n",
       "        0.69060751, 0.68943991, 0.69138006, 0.69085355, 0.69072994,\n",
       "        0.68995021, 0.67939975, 0.68025805, 0.68257671, 0.68013334,\n",
       "        0.67980408, 0.6805741 , 0.68259832, 0.68027195, 0.67974863,\n",
       "        0.68052985, 0.68258771, 0.68024462, 0.69005908, 0.69018081,\n",
       "        0.69099317, 0.69082401, 0.69198199, 0.69197119, 0.69192973,\n",
       "        0.69173568, 0.69174061, 0.69175196, 0.69179852, 0.69160741,\n",
       "        0.67772105, 0.67783788, 0.67965328, 0.67943529, 0.689144  ,\n",
       "        0.68876982, 0.68900131, 0.68875445, 0.68680636, 0.68678479,\n",
       "        0.68785633, 0.68773883, 0.67715936, 0.67782452, 0.67933389,\n",
       "        0.67980181, 0.68994557, 0.69015621, 0.69052891, 0.69037518,\n",
       "        0.68682029, 0.68745864, 0.68890344, 0.68885702, 0.67327304,\n",
       "        0.67539259, 0.68295616, 0.67880502, 0.67980137, 0.68056958,\n",
       "        0.68258438, 0.68025773, 0.67918191, 0.6800971 , 0.68256517,\n",
       "        0.68006086, 0.67846407, 0.6787291 , 0.68013166, 0.68027301,\n",
       "        0.69015148, 0.69021024, 0.69064712, 0.69056707, 0.68774819,\n",
       "        0.68798403, 0.68910596, 0.68912573, 0.5       , 0.5       ,\n",
       "        0.66557985, 0.66557985, 0.68401905, 0.68426708, 0.68488547,\n",
       "        0.68512512, 0.67737063, 0.67553265, 0.67862439, 0.67853564,\n",
       "        0.5       , 0.5       , 0.66557985, 0.66557985, 0.68522496,\n",
       "        0.68565794, 0.68670008, 0.68679833, 0.67484769, 0.67568364,\n",
       "        0.67879649, 0.67944606, 0.55423335, 0.55423335, 0.66593673,\n",
       "        0.66557985, 0.67977715, 0.68052442, 0.68244315, 0.68011516,\n",
       "        0.66803091, 0.67109372, 0.6831931 , 0.6780682 , 0.5       ,\n",
       "        0.5       , 0.66557985, 0.66557985, 0.68539397, 0.68558046,\n",
       "        0.68671462, 0.68677386, 0.67689892, 0.67741446, 0.67963955,\n",
       "        0.67981371]),\n",
       " 'std_test_roc_auc': array([0.00068711, 0.00144149, 0.00101306, 0.00092439, 0.00068798,\n",
       "        0.0014429 , 0.00101354, 0.00092531, 0.00068794, 0.0014427 ,\n",
       "        0.00101345, 0.00092517, 0.02578778, 0.03329083, 0.01453762,\n",
       "        0.0095455 , 0.0285374 , 0.05617971, 0.01034188, 0.06380544,\n",
       "        0.0384785 , 0.02816777, 0.01104492, 0.02777421, 0.00090102,\n",
       "        0.00092642, 0.00107834, 0.00105712, 0.00090103, 0.00092642,\n",
       "        0.00107834, 0.00105712, 0.00090103, 0.00092642, 0.00107834,\n",
       "        0.00105712, 0.00091954, 0.00091215, 0.00083277, 0.00080781,\n",
       "        0.00091973, 0.00091213, 0.00083279, 0.00080782, 0.0009197 ,\n",
       "        0.00091214, 0.00083279, 0.00080782, 0.00068353, 0.00142978,\n",
       "        0.00100843, 0.0009167 , 0.00068798, 0.00144287, 0.00101359,\n",
       "        0.00092534, 0.00068675, 0.0014407 , 0.00101279, 0.000924  ,\n",
       "        0.0141137 , 0.01686909, 0.02993092, 0.04180692, 0.02826894,\n",
       "        0.05792763, 0.01563099, 0.04794866, 0.03391372, 0.06041635,\n",
       "        0.02721859, 0.02977512, 0.00090099, 0.00092645, 0.00107838,\n",
       "        0.00105713, 0.00090103, 0.00092642, 0.00107834, 0.00105712,\n",
       "        0.00090102, 0.00092642, 0.00107835, 0.00105712, 0.00091939,\n",
       "        0.00091223, 0.00083261, 0.00080782, 0.00091972, 0.00091214,\n",
       "        0.0008328 , 0.00080784, 0.00091938, 0.00091217, 0.00083281,\n",
       "        0.00080787, 0.00068608, 0.00132457, 0.00094961, 0.00085639,\n",
       "        0.00068779, 0.00144233, 0.00101393, 0.00092578, 0.00068647,\n",
       "        0.00142394, 0.0010062 , 0.00091313, 0.00502328, 0.01419848,\n",
       "        0.01486798, 0.00845917, 0.00629873, 0.06026128, 0.02674396,\n",
       "        0.0276781 , 0.00430261, 0.01504837, 0.00581661, 0.01904557,\n",
       "        0.00090086, 0.00092644, 0.00107859, 0.00105724, 0.00090103,\n",
       "        0.00092643, 0.00107834, 0.00105712, 0.00090098, 0.00092644,\n",
       "        0.00107838, 0.00105715, 0.00092154, 0.0009126 , 0.00083154,\n",
       "        0.00080459, 0.00092009, 0.00091221, 0.00083312, 0.00080809,\n",
       "        0.00091981, 0.00091222, 0.00083281, 0.00080792, 0.00071279,\n",
       "        0.00113291, 0.00089307, 0.00085608, 0.0006857 , 0.00143828,\n",
       "        0.00101636, 0.00092934, 0.00069456, 0.00124801, 0.00091956,\n",
       "        0.00085644, 0.00103509, 0.00188697, 0.00134136, 0.0038091 ,\n",
       "        0.00151058, 0.00570265, 0.00184783, 0.0018288 , 0.00124552,\n",
       "        0.00360335, 0.00192679, 0.00207988, 0.00089934, 0.00092655,\n",
       "        0.0010801 , 0.001056  , 0.00090103, 0.00092644, 0.00107833,\n",
       "        0.00105712, 0.00090082, 0.00092648, 0.00107869, 0.00105731,\n",
       "        0.00088429, 0.00089464, 0.0008366 , 0.00081245, 0.00092177,\n",
       "        0.00091346, 0.00083572, 0.0008107 , 0.00092426, 0.000916  ,\n",
       "        0.00083344, 0.0008075 , 0.00072409, 0.00094683, 0.00081671,\n",
       "        0.00098351, 0.00067391, 0.00141151, 0.00103518, 0.00096365,\n",
       "        0.00066404, 0.00103115, 0.00081817, 0.00086306, 0.00058602,\n",
       "        0.0008427 , 0.00082142, 0.00119711, 0.00104447, 0.00050977,\n",
       "        0.00105928, 0.00112332, 0.00095645, 0.00074938, 0.0008288 ,\n",
       "        0.00085587, 0.00088197, 0.00091336, 0.00106962, 0.00104915,\n",
       "        0.00090114, 0.00092651, 0.00107828, 0.00105716, 0.00089843,\n",
       "        0.00092683, 0.00108098, 0.0010543 , 0.0009533 , 0.0009134 ,\n",
       "        0.00085154, 0.00082773, 0.00093753, 0.00092551, 0.0008559 ,\n",
       "        0.00083261, 0.00089916, 0.00089693, 0.00085493, 0.00083218,\n",
       "        0.00086448, 0.00147492, 0.00097763, 0.0012343 , 0.00062452,\n",
       "        0.00083674, 0.00113588, 0.00119571, 0.00149866, 0.00115605,\n",
       "        0.0008751 , 0.00110574, 0.0005755 , 0.0014278 , 0.00058466,\n",
       "        0.00093908, 0.0010104 , 0.00095899, 0.00092896, 0.00098596,\n",
       "        0.00096622, 0.00087225, 0.00086432, 0.00089749, 0.00062858,\n",
       "        0.0005929 , 0.00096891, 0.00097611, 0.00090211, 0.00092749,\n",
       "        0.00107774, 0.00105735, 0.00089245, 0.00088362, 0.00105419,\n",
       "        0.00104663, 0.00087526, 0.00095537, 0.0008434 , 0.00091796,\n",
       "        0.00105951, 0.00103593, 0.00095019, 0.00095871, 0.00106793,\n",
       "        0.00103203, 0.0009244 , 0.00094042, 0.        , 0.        ,\n",
       "        0.00097151, 0.00097151, 0.00168745, 0.00156312, 0.00151446,\n",
       "        0.00169193, 0.00183888, 0.00185798, 0.0009621 , 0.00140466,\n",
       "        0.        , 0.        , 0.00097151, 0.00097151, 0.00103513,\n",
       "        0.00110426, 0.00105704, 0.00110437, 0.00092302, 0.00119976,\n",
       "        0.00048795, 0.00084041, 0.00083885, 0.00083885, 0.00098652,\n",
       "        0.00097151, 0.00091257, 0.0009375 , 0.00107305, 0.00105758,\n",
       "        0.00051116, 0.00041789, 0.00091583, 0.00107802, 0.        ,\n",
       "        0.        , 0.00097151, 0.00097151, 0.00118056, 0.00113389,\n",
       "        0.00103321, 0.0011111 , 0.00072162, 0.00084088, 0.00080598,\n",
       "        0.00092225]),\n",
       " 'rank_test_roc_auc': array([ 65, 122,  87, 111,  67, 125,  89, 113,  66, 124,  88, 112, 313,\n",
       "        315, 309, 314, 306, 328, 305, 322, 326, 327, 312, 316, 243, 203,\n",
       "        181, 219, 239, 200, 178, 215, 242, 202, 179, 217,   4,  15,  27,\n",
       "         42,   1,  12,  25,  40,   2,  13,  26,  41,  63, 119,  84, 109,\n",
       "         68, 126,  90, 114,  64, 121,  86, 110, 302, 320, 317, 318, 308,\n",
       "        330, 304, 310, 319, 329, 311, 325, 247, 206, 184, 222, 240, 199,\n",
       "        180, 216, 245, 204, 182, 220,   6,  17,  30,  45,   3,  14,  28,\n",
       "         43,   5,  16,  29,  44,  62, 104,  80, 103,  69, 127,  91, 115,\n",
       "         61, 118,  83, 108, 285, 298, 296, 297, 301, 321, 303, 307, 294,\n",
       "        300, 295, 299, 251, 209, 187, 226, 241, 201, 183, 218, 248, 207,\n",
       "        185, 223,   9,  20,  33,  48,   7,  18,  31,  46,   8,  19,  32,\n",
       "         47,  71,  98,  78, 106,  70, 128,  92, 117,  60,  97,  79, 105,\n",
       "        155, 172, 173, 231, 164, 176, 198, 282, 159, 153, 197, 195, 254,\n",
       "        212, 190, 230, 244, 205, 186, 221, 252, 210, 188, 227,  23,  24,\n",
       "         36,  51,  10,  21,  34,  49,  11,  22,  35,  50, 142, 146, 132,\n",
       "        136,  73, 133,  95, 129,  76, 116,  94, 130, 134, 137, 131, 138,\n",
       "         57,  59,  82, 135,  58,  74,  77, 120, 260, 228, 193, 233, 246,\n",
       "        208, 189, 225, 255, 213, 191, 232, 107, 100,  72,  75,  37,  38,\n",
       "         39,  55,  54,  53,  52,  56, 272, 270, 256, 259, 139, 147, 143,\n",
       "        148, 157, 160, 150, 152, 275, 271, 261, 249, 123, 101,  93,  96,\n",
       "        156, 154, 144, 145, 281, 279, 177, 263, 250, 211, 192, 229, 262,\n",
       "        236, 194, 237, 268, 265, 234, 224, 102,  99,  81,  85, 151, 149,\n",
       "        141, 140, 331, 331, 287, 287, 174, 171, 170, 169, 274, 278, 266,\n",
       "        267, 331, 331, 287, 287, 168, 165, 163, 158, 280, 277, 264, 258,\n",
       "        323, 323, 286, 287, 253, 214, 196, 235, 284, 283, 175, 269, 331,\n",
       "        331, 287, 287, 167, 166, 162, 161, 276, 273, 257, 238]),\n",
       " 'split0_test_loan_loss': array([-132421. , -132226.3, -115559.9, -122769.2, -132421. , -132222.6,\n",
       "        -115564.3, -122774.9, -132421. , -132222.6, -115564.3, -122773.9,\n",
       "        -135064.9, -130313.3, -165249.4, -137665.5, -126360.6, -129622.5,\n",
       "        -146150.8, -131018.4, -147046.8, -134137.6, -160737.7, -151664.5,\n",
       "        -132707.4, -132638.3, -118439.4, -125864.1, -132707.4, -132638.3,\n",
       "        -118439.4, -125864.1, -132707.4, -132638.3, -118439.4, -125864.1,\n",
       "        -132226.1, -131851.1, -115138.4, -121334.6, -132226.1, -131851.1,\n",
       "        -115135.4, -121334.6, -132226.1, -131851.1, -115137.4, -121334.6,\n",
       "        -132417.3, -132222.6, -115523.4, -122757.5, -132421. , -132222.6,\n",
       "        -115565.3, -122772.9, -132417.3, -132226.3, -115553.2, -122760.8,\n",
       "        -129262.9, -131558.4, -181510.3, -140232.6, -138012.7, -131246.5,\n",
       "        -160295.8, -128873.6, -137514.3, -129925.1, -158135.2, -156628.8,\n",
       "        -132707.4, -132638.3, -118439.4, -125864.1, -132707.4, -132638.3,\n",
       "        -118439.4, -125864.1, -132707.4, -132638.3, -118439.4, -125864.1,\n",
       "        -132226.1, -131850.1, -115138. , -121328.2, -132226.1, -131850.1,\n",
       "        -115134.4, -121334.6, -132226.1, -131850.1, -115143.1, -121333.6,\n",
       "        -132436.5, -132240.9, -115570.1, -122736.3, -132421. , -132225.3,\n",
       "        -115557.6, -122771.9, -132417.3, -132226.3, -115507.5, -122755.6,\n",
       "        -132089.2, -132657.6, -120441. , -125474.4, -130128. , -132236.4,\n",
       "        -120460.4, -151839.5, -131768.6, -132624. , -121042.3, -143021.3,\n",
       "        -132707.4, -132638.3, -118439.4, -125863.1, -132707.4, -132638.3,\n",
       "        -118439.4, -125864.1, -132707.4, -132638.3, -118439.4, -125863.1,\n",
       "        -132235.2, -131850.5, -115099.9, -121319. , -132233.5, -131850.1,\n",
       "        -115131.7, -121340.6, -132233.5, -131849.1, -115135.7, -121330.2,\n",
       "        -132406.5, -132239.6, -115639.2, -122568.9, -132422.7, -132238.8,\n",
       "        -115555.1, -122747.1, -132431.5, -132258.8, -115595.1, -122744.5,\n",
       "        -132435.6, -132443.3, -118158.6, -120803.1, -132466.5, -131202.9,\n",
       "        -116868.2, -130323.5, -132479.8, -131343.6, -117041.7, -127056.9,\n",
       "        -132707.4, -132642. , -118455.7, -125873.5, -132707.4, -132638.3,\n",
       "        -118438.4, -125864.1, -132707.4, -132638.3, -118437.4, -125865.8,\n",
       "        -132257.2, -131932.7, -115125.6, -121374.2, -132239.9, -131858.9,\n",
       "        -115154.7, -121311.6, -132238.6, -131854.9, -115115.2, -121312.2,\n",
       "        -132476.2, -132178.3, -115974. , -122718.7, -132459.5, -132282.1,\n",
       "        -115573.8, -122804.1, -132453.4, -132253.3, -115462.5, -122604.9,\n",
       "        -132401.8, -132095.7, -115918.8, -122905.7, -132442.6, -132182. ,\n",
       "        -116005.7, -122933. , -132468.8, -132181.2, -116150.6, -122810. ,\n",
       "        -132716.5, -132674.3, -118507.1, -125866.9, -132707.4, -132645.7,\n",
       "        -118437.8, -125868.8, -132707.4, -132649.4, -118451.5, -125880.6,\n",
       "        -132234.6, -131936.6, -115620.7, -121603.8, -132257.3, -131915.1,\n",
       "        -115184.2, -121390.3, -132294.1, -131939.4, -115183. , -121523.4,\n",
       "        -132687.3, -132636.1, -119362.8, -123760. , -132536.5, -132371.4,\n",
       "        -116101.2, -122961.4, -132561.4, -132387. , -116749.1, -123058.4,\n",
       "        -132672.8, -132546.3, -119597.1, -122875.2, -132414.4, -132145.8,\n",
       "        -115807. , -121872.3, -132431.9, -132168.9, -116553.8, -122052.1,\n",
       "        -132723.7, -132729.7, -119372.5, -126480. , -132710.1, -132655.8,\n",
       "        -118441.1, -125877.7, -132722.9, -132693.5, -118526.5, -125915.1,\n",
       "        -132663.4, -132545.3, -119239.8, -122901.1, -132401.2, -132135.6,\n",
       "        -115595.7, -121739.4, -132420.5, -132169.9, -116340. , -122123.5,\n",
       "        -132726.4, -132726.4, -132726.4, -125533.2, -132724.3, -132696. ,\n",
       "        -118895.6, -123530.2, -132726.4, -132722.7, -122952.8, -124562.8,\n",
       "        -132726.4, -132726.4, -132715.9, -125325.5, -132710.2, -132702.1,\n",
       "        -118134.8, -122726.9, -132726.4, -132726.4, -122519.4, -123042.8,\n",
       "        -132726.4, -132726.4, -132726.4, -129621.3, -132723.3, -132702.7,\n",
       "        -118694.4, -125978.9, -132726.4, -132726.4, -120596. , -127431.7,\n",
       "        -132726.4, -132726.4, -132715.9, -125325.5, -132711.2, -132702.1,\n",
       "        -118191.4, -122589.1, -132726.4, -132726.4, -122253. , -123211.7]),\n",
       " 'split1_test_loan_loss': array([-132140.6, -132634.7, -116063.3, -121538.9, -132140.6, -132635.7,\n",
       "        -116069. , -121529.5, -132140.6, -132635.7, -116069. , -121532.2,\n",
       "        -129289.6, -131983.9, -133394.3, -136252.2, -132835.9, -154573.4,\n",
       "        -142957.6, -143521.2, -132617.8, -134263.7, -132741.2, -146053.3,\n",
       "        -132680.8, -132616.7, -118855.7, -125502.5, -132680.8, -132616.7,\n",
       "        -118855.7, -125502.5, -132680.8, -132616.7, -118855.7, -125502.5,\n",
       "        -132259.5, -131891.7, -115709.4, -121270.3, -132259.5, -131891.7,\n",
       "        -115705.7, -121267.3, -132259.5, -131891.7, -115705.7, -121267.3,\n",
       "        -132140.6, -132633.7, -116056.6, -121490.3, -132140.6, -132635.7,\n",
       "        -116069. , -121529.5, -132139.6, -132634.7, -116062.3, -121538.9,\n",
       "        -127378.1, -132730.2, -133998.7, -176607.6, -127978. , -160730.9,\n",
       "        -134248. , -145726.3, -127841.5, -160439.6, -142283.8, -146726.6,\n",
       "        -132680.8, -132616.7, -118855.7, -125503.5, -132680.8, -132616.7,\n",
       "        -118855.7, -125502.5, -132680.8, -132616.7, -118855.7, -125502.5,\n",
       "        -132262.2, -131890.7, -115717.8, -121266.6, -132259.5, -131891.7,\n",
       "        -115710.4, -121269.3, -132258.5, -131891.7, -115710.4, -121269.3,\n",
       "        -132135.6, -132636.7, -115982.1, -121293.9, -132139.6, -132638.4,\n",
       "        -116081.8, -121537.2, -132139.6, -132635.7, -116072.8, -121474.8,\n",
       "        -130046.5, -132724.3, -124775.4, -122891.9, -129182.7, -136828.5,\n",
       "        -132677.5, -137184.5, -130028.7, -132703.3, -124964.7, -128441.6,\n",
       "        -132680.8, -132616.7, -118857.4, -125501.8, -132680.8, -132616.7,\n",
       "        -118855.7, -125502.5, -132680.8, -132616.7, -118855.7, -125505.5,\n",
       "        -132270.3, -131899.5, -115718.5, -121275. , -132257.5, -131897.1,\n",
       "        -115717.1, -121265.3, -132260.2, -131899.8, -115720.1, -121266.6,\n",
       "        -132174.9, -132640.1, -116066.2, -121571.4, -132148.7, -132642.1,\n",
       "        -116086.7, -121518.2, -132149.8, -132633. , -116016.7, -121346.6,\n",
       "        -131594.2, -132697.7, -118490.8, -126573.6, -131542. , -132724.6,\n",
       "        -118944.6, -120739.1, -131639.3, -132711.2, -117599.6, -120129.8,\n",
       "        -132680.8, -132620.4, -118852.2, -125522. , -132680.8, -132616.7,\n",
       "        -118857.4, -125502.5, -132680.8, -132616.7, -118862.8, -125501.8,\n",
       "        -132304.4, -131983.4, -115721.8, -121238.9, -132264.6, -131897.5,\n",
       "        -115742.4, -121277.6, -132271.4, -131912. , -115713.7, -121289.8,\n",
       "        -132222.8, -132619.5, -116434.8, -121818.5, -132195.7, -132652. ,\n",
       "        -116262.4, -121677.5, -132230. , -132663.7, -116173.6, -121717.8,\n",
       "        -132250.1, -132479.5, -116390.5, -122152. , -132407.4, -131946.7,\n",
       "        -116241.6, -121849.5, -132322.9, -132569.1, -116067.8, -122050.5,\n",
       "        -132680.5, -132640.6, -118923.8, -125510.2, -132680.8, -132616.7,\n",
       "        -118851.1, -125505.5, -132679.8, -132624.1, -118854.4, -125526. ,\n",
       "        -132264. , -131958.2, -116077.6, -121342.4, -132303.1, -131969.1,\n",
       "        -115736.8, -121262.4, -132338.2, -131990.8, -115673.5, -121369.9,\n",
       "        -132602.9, -132714.6, -119783.7, -123230.8, -132353.3, -132126.6,\n",
       "        -116708.1, -122133.7, -132168.6, -132665.7, -117079.4, -122176.6,\n",
       "        -132664. , -132542.9, -119435.7, -123051.3, -132435. , -132278.2,\n",
       "        -116215.4, -121596.9, -132475.5, -132259.5, -116648.6, -121788.5,\n",
       "        -132727.7, -132709.5, -119635. , -126297.5, -132678.8, -132636.9,\n",
       "        -118871.5, -125509.1, -132687.6, -132648.7, -119000.4, -125590.9,\n",
       "        -132651.5, -132533.2, -119424.5, -122871.2, -132424.4, -132147.5,\n",
       "        -115984.1, -121570.8, -132419.1, -132168.4, -116579.2, -121757.8,\n",
       "        -132726.4, -132726.4, -132726.4, -125533.8, -132728.4, -132728.4,\n",
       "        -119316.8, -123078.6, -132729.4, -132728.4, -123177.8, -123712.8,\n",
       "        -132726.4, -132726.4, -132699.8, -125533.8, -132726. , -132720.9,\n",
       "        -118235.8, -122337.8, -132728.4, -132729.4, -122142.9, -123016.6,\n",
       "        -132726.4, -132726.4, -132726.4, -129793.6, -132708.5, -132673.7,\n",
       "        -119141.3, -125617.3, -132728.4, -132730.4, -120598.2, -127128.2,\n",
       "        -132726.4, -132726.4, -132699.8, -125533.8, -132726. , -132721.9,\n",
       "        -118130.4, -122249.8, -132728.4, -132728.4, -122140.9, -122994.5]),\n",
       " 'split2_test_loan_loss': array([-131937.4, -132010.8, -115708.7, -121574.9, -131933.7, -132010.8,\n",
       "        -115708. , -121567.5, -131933.7, -132010.8, -115706. , -121569.2,\n",
       "        -143014.3, -132226.1, -135165.7, -145131. , -139628. , -136088. ,\n",
       "        -135443.1, -145017.2, -149708.7, -136500.6, -140568. , -135310.6,\n",
       "        -132693.6, -132642.3, -118839.1, -124868.5, -132693.6, -132642.3,\n",
       "        -118839.1, -124868.5, -132693.6, -132642.3, -118839.1, -124868.5,\n",
       "        -132207.4, -131742.9, -115016.6, -120696.5, -132207.4, -131742.9,\n",
       "        -115017.6, -120697.5, -132207.4, -131742.9, -115017.6, -120697.5,\n",
       "        -131957.3, -132014.5, -115716.3, -121601. , -131933.7, -132010.8,\n",
       "        -115705. , -121565.5, -131937.4, -132010.8, -115709.4, -121576.9,\n",
       "        -130253.5, -133479.3, -146787.5, -140881.9, -134559.3, -134328.5,\n",
       "        -135331.6, -136993. , -139124.4, -135676.7, -141478.1, -142376. ,\n",
       "        -132693.6, -132642.3, -118839.1, -124869.5, -132693.6, -132642.3,\n",
       "        -118839.1, -124868.5, -132693.6, -132642.3, -118839.1, -124868.5,\n",
       "        -132207.4, -131742.9, -115027.7, -120683.1, -132207.4, -131742.9,\n",
       "        -115016.6, -120696.5, -132207.4, -131742.9, -115016.6, -120696.5,\n",
       "        -131978.7, -132016.6, -115707. , -121561.1, -131933.7, -132013.5,\n",
       "        -115696.3, -121572.9, -131964.7, -132012.5, -115722.8, -121593.6,\n",
       "        -131149.1, -132564. , -138059.1, -121755.3, -128876.7, -133038.8,\n",
       "        -130098.9, -136202.8, -129083.7, -132611.7, -125449.2, -123394.6,\n",
       "        -132693.6, -132642.3, -118836.1, -124871.5, -132693.6, -132642.3,\n",
       "        -118839.1, -124868.5, -132693.6, -132642.3, -118839.1, -124869.5,\n",
       "        -132214.5, -131743.3, -115061.4, -120698.9, -132210.1, -131742.9,\n",
       "        -115012.6, -120691.8, -132209.1, -131739.9, -115016.3, -120693.5,\n",
       "        -132028.2, -132049.9, -115692.9, -121490.9, -131960.6, -132027.7,\n",
       "        -115728.6, -121585.5, -132007.8, -132035.5, -115729.3, -121487.9,\n",
       "        -130937.1, -132327.5, -121481.2, -119154.7, -130722.6, -132305.7,\n",
       "        -122052.8, -120779.6, -130913.4, -132536.8, -121701.8, -120144.2,\n",
       "        -132692.6, -132641.3, -118853.8, -124883.6, -132693.6, -132642.3,\n",
       "        -118839.1, -124870.5, -132693.6, -132642.3, -118836.1, -124875.5,\n",
       "        -132202.2, -131761.3, -115081.3, -120701.2, -132224.6, -131762.8,\n",
       "        -115031.8, -120654.8, -132221.6, -131766.6, -115034.3, -120656.6,\n",
       "        -132114.2, -132104.6, -116243.5, -121759.4, -132028.6, -132084.6,\n",
       "        -115802.5, -121645.6, -132106.2, -132089.9, -115898.6, -121679.3,\n",
       "        -132315.4, -131992. , -116229.7, -121423.7, -132353.6, -132067.9,\n",
       "        -115796.5, -121603.3, -132466.8, -132065.4, -115861.9, -121521.5,\n",
       "        -132696. , -132661.5, -118879.3, -124976.8, -132692.6, -132641.3,\n",
       "        -118844.9, -124874.5, -132692.6, -132645. , -118862.4, -124883.3,\n",
       "        -132262.5, -131804.2, -115553.2, -121245.7, -132279.4, -131838.9,\n",
       "        -115001. , -120708. , -132246.5, -131832.6, -115066.8, -120785.5,\n",
       "        -132705.8, -132585.8, -119368.4, -122561.7, -132188.3, -132288. ,\n",
       "        -116437.8, -121878.9, -132613.6, -132273.7, -117136.6, -121979. ,\n",
       "        -132665.3, -132530.7, -119380.2, -122203.9, -132445.6, -132100.9,\n",
       "        -115940.9, -121104.9, -132457.4, -132179. , -116672.6, -121362.7,\n",
       "        -132727.4, -132722. , -119875.9, -125819.8, -132689.6, -132656.1,\n",
       "        -118872.1, -124884.8, -132701.4, -132669.6, -118972.4, -124983.7,\n",
       "        -132656.9, -132549.9, -119295.3, -122381.3, -132404.8, -132091.3,\n",
       "        -115807.4, -121065.5, -132465.7, -132145.6, -116392.6, -121509.9,\n",
       "        -132726.4, -132726.4, -132718.6, -125485.2, -132715.3, -132712.6,\n",
       "        -118779.2, -122357.1, -132726.4, -132726.4, -122782.6, -123287.7,\n",
       "        -132726.4, -132726.4, -132708.8, -125318.4, -132714.6, -132709.5,\n",
       "        -118176.5, -121508.3, -132726.4, -132726.4, -122468.5, -122909.2,\n",
       "        -132726.4, -132726.4, -132726.4, -129449.2, -132706.5, -132685.2,\n",
       "        -119083.1, -124959. , -132726.4, -132726.4, -120855.3, -126673.9,\n",
       "        -132726.4, -132726.4, -132708.8, -125318.4, -132710.9, -132709.5,\n",
       "        -118126.2, -121569.8, -132726.4, -132726.4, -122315.8, -122804.7]),\n",
       " 'mean_test_loan_loss': array([-132166.33333333, -132290.6       , -115777.3       ,\n",
       "        -121961.        , -132165.1       , -132289.7       ,\n",
       "        -115780.43333333, -121957.3       , -132165.1       ,\n",
       "        -132289.7       , -115779.76666667, -121958.43333333,\n",
       "        -135789.6       , -131507.76666667, -144603.13333333,\n",
       "        -139682.9       , -132941.5       , -140094.63333333,\n",
       "        -141517.16666667, -139852.26666667, -143124.43333333,\n",
       "        -134967.3       , -144682.3       , -144342.8       ,\n",
       "        -132693.93333333, -132632.43333333, -118711.4       ,\n",
       "        -125411.7       , -132693.93333333, -132632.43333333,\n",
       "        -118711.4       , -125411.7       , -132693.93333333,\n",
       "        -132632.43333333, -118711.4       , -125411.7       ,\n",
       "        -132231.        , -131828.56666667, -115288.13333333,\n",
       "        -121100.46666667, -132231.        , -131828.56666667,\n",
       "        -115286.23333333, -121099.8       , -132231.        ,\n",
       "        -131828.56666667, -115286.9       , -121099.8       ,\n",
       "        -132171.73333333, -132290.26666667, -115765.43333333,\n",
       "        -121949.6       , -132165.1       , -132289.7       ,\n",
       "        -115779.76666667, -121955.96666667, -132164.76666667,\n",
       "        -132290.6       , -115774.96666667, -121958.86666667,\n",
       "        -128964.83333333, -132589.3       , -154098.83333333,\n",
       "        -152574.03333333, -133516.66666667, -142101.96666667,\n",
       "        -143291.8       , -137197.63333333, -134826.73333333,\n",
       "        -142013.8       , -147299.03333333, -148577.13333333,\n",
       "        -132693.93333333, -132632.43333333, -118711.4       ,\n",
       "        -125412.36666667, -132693.93333333, -132632.43333333,\n",
       "        -118711.4       , -125411.7       , -132693.93333333,\n",
       "        -132632.43333333, -118711.4       , -125411.7       ,\n",
       "        -132231.9       , -131827.9       , -115294.5       ,\n",
       "        -121092.63333333, -132231.        , -131828.23333333,\n",
       "        -115287.13333333, -121100.13333333, -132230.66666667,\n",
       "        -131828.23333333, -115290.03333333, -121099.8       ,\n",
       "        -132183.6       , -132298.06666667, -115753.06666667,\n",
       "        -121863.76666667, -132164.76666667, -132292.4       ,\n",
       "        -115778.56666667, -121960.66666667, -132173.86666667,\n",
       "        -132291.5       , -115767.7       , -121941.33333333,\n",
       "        -131094.93333333, -132648.63333333, -127758.5       ,\n",
       "        -123373.86666667, -129395.8       , -134034.56666667,\n",
       "        -127745.6       , -141742.26666667, -130293.66666667,\n",
       "        -132646.33333333, -123818.73333333, -131619.16666667,\n",
       "        -132693.93333333, -132632.43333333, -118710.96666667,\n",
       "        -125412.13333333, -132693.93333333, -132632.43333333,\n",
       "        -118711.4       , -125411.7       , -132693.93333333,\n",
       "        -132632.43333333, -118711.4       , -125412.7       ,\n",
       "        -132240.        , -131831.1       , -115293.26666667,\n",
       "        -121097.63333333, -132233.7       , -131830.03333333,\n",
       "        -115287.13333333, -121099.23333333, -132234.26666667,\n",
       "        -131829.6       , -115290.7       , -121096.76666667,\n",
       "        -132203.2       , -132309.86666667, -115799.43333333,\n",
       "        -121877.06666667, -132177.33333333, -132302.86666667,\n",
       "        -115790.13333333, -121950.26666667, -132196.36666667,\n",
       "        -132309.1       , -115780.36666667, -121859.66666667,\n",
       "        -131655.63333333, -132489.5       , -119376.86666667,\n",
       "        -122177.13333333, -131577.03333333, -132077.73333333,\n",
       "        -119288.53333333, -123947.4       , -131677.5       ,\n",
       "        -132197.2       , -118781.03333333, -122443.63333333,\n",
       "        -132693.6       , -132634.56666667, -118720.56666667,\n",
       "        -125426.36666667, -132693.93333333, -132632.43333333,\n",
       "        -118711.63333333, -125412.36666667, -132693.93333333,\n",
       "        -132632.43333333, -118712.1       , -125414.36666667,\n",
       "        -132254.6       , -131892.46666667, -115309.56666667,\n",
       "        -121104.76666667, -132243.03333333, -131839.73333333,\n",
       "        -115309.63333333, -121081.33333333, -132243.86666667,\n",
       "        -131844.5       , -115287.73333333, -121086.2       ,\n",
       "        -132271.06666667, -132300.8       , -116217.43333333,\n",
       "        -122098.86666667, -132227.93333333, -132339.56666667,\n",
       "        -115879.56666667, -122042.4       , -132263.2       ,\n",
       "        -132335.63333333, -115844.9       , -122000.66666667,\n",
       "        -132322.43333333, -132189.06666667, -116179.66666667,\n",
       "        -122160.46666667, -132401.2       , -132065.53333333,\n",
       "        -116014.6       , -122128.6       , -132419.5       ,\n",
       "        -132271.9       , -116026.76666667, -122127.33333333,\n",
       "        -132697.66666667, -132658.8       , -118770.06666667,\n",
       "        -125451.3       , -132693.6       , -132634.56666667,\n",
       "        -118711.26666667, -125416.26666667, -132693.26666667,\n",
       "        -132639.5       , -118722.76666667, -125429.96666667,\n",
       "        -132253.7       , -131899.66666667, -115750.5       ,\n",
       "        -121397.3       , -132279.93333333, -131907.7       ,\n",
       "        -115307.33333333, -121120.23333333, -132292.93333333,\n",
       "        -131920.93333333, -115307.76666667, -121226.26666667,\n",
       "        -132665.33333333, -132645.5       , -119504.96666667,\n",
       "        -123184.16666667, -132359.36666667, -132262.        ,\n",
       "        -116415.7       , -122324.66666667, -132447.86666667,\n",
       "        -132442.13333333, -116988.36666667, -122404.66666667,\n",
       "        -132667.36666667, -132539.96666667, -119471.        ,\n",
       "        -122710.13333333, -132431.66666667, -132174.96666667,\n",
       "        -115987.76666667, -121524.7       , -132454.93333333,\n",
       "        -132202.46666667, -116625.        , -121734.43333333,\n",
       "        -132726.26666667, -132720.4       , -119627.8       ,\n",
       "        -126199.1       , -132692.83333333, -132649.6       ,\n",
       "        -118728.23333333, -125423.86666667, -132703.96666667,\n",
       "        -132670.6       , -118833.1       , -125496.56666667,\n",
       "        -132657.26666667, -132542.8       , -119319.86666667,\n",
       "        -122717.86666667, -132410.13333333, -132124.8       ,\n",
       "        -115795.73333333, -121458.56666667, -132435.1       ,\n",
       "        -132161.3       , -116437.26666667, -121797.06666667,\n",
       "        -132726.4       , -132726.4       , -132723.8       ,\n",
       "        -125517.4       , -132722.66666667, -132712.33333333,\n",
       "        -118997.2       , -122988.63333333, -132727.4       ,\n",
       "        -132725.83333333, -122971.06666667, -123854.43333333,\n",
       "        -132726.4       , -132726.4       , -132708.16666667,\n",
       "        -125392.56666667, -132716.93333333, -132710.83333333,\n",
       "        -118182.36666667, -122191.        , -132727.06666667,\n",
       "        -132727.4       , -122376.93333333, -122989.53333333,\n",
       "        -132726.4       , -132726.4       , -132726.4       ,\n",
       "        -129621.36666667, -132712.76666667, -132687.2       ,\n",
       "        -118972.93333333, -125518.4       , -132727.06666667,\n",
       "        -132727.73333333, -120683.16666667, -127077.93333333,\n",
       "        -132726.4       , -132726.4       , -132708.16666667,\n",
       "        -125392.56666667, -132716.03333333, -132711.16666667,\n",
       "        -118149.33333333, -122136.23333333, -132727.06666667,\n",
       "        -132727.06666667, -122236.56666667, -123003.63333333]),\n",
       " 'std_test_loan_loss': array([1.98265636e+02, 2.58732384e+02, 2.11159276e+02, 5.71672651e+02,\n",
       "        1.99692280e+02, 2.59488998e+02, 2.12313390e+02, 5.78338609e+02,\n",
       "        1.99692280e+02, 2.59488998e+02, 2.12542801e+02, 5.76819824e+02,\n",
       "        5.62646959e+03, 8.50383511e+02, 1.46170154e+04, 3.89535616e+03,\n",
       "        5.41690805e+03, 1.05728169e+04, 4.48849239e+03, 6.27627300e+03,\n",
       "        7.50837026e+03, 1.08542832e+03, 1.17939703e+04, 6.78512437e+03,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11491855e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11491855e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11491855e+02,\n",
       "        2.15500967e+01, 6.28021939e+01, 3.02002222e+02, 2.86851208e+02,\n",
       "        2.15500967e+01, 6.28021939e+01, 3.00481196e+02, 2.85792804e+02,\n",
       "        2.15500967e+01, 6.28021939e+01, 3.00147841e+02, 2.85792804e+02,\n",
       "        1.89080165e+02, 2.57275784e+02, 2.20433094e+02, 5.73056385e+02,\n",
       "        1.99692280e+02, 2.59488998e+02, 2.12322025e+02, 5.77846031e+02,\n",
       "        1.96724890e+02, 2.58732384e+02, 2.12947479e+02, 5.67264667e+02,\n",
       "        1.19264806e+03, 7.90507774e+02, 2.00736931e+04, 1.69963652e+04,\n",
       "        4.16246035e+03, 1.32325999e+04, 1.20317790e+04, 6.88160739e+03,\n",
       "        4.98285186e+03, 1.32389024e+04, 7.66938368e+03, 5.96399934e+03,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11125490e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11491855e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11491855e+02,\n",
       "        2.27448163e+01, 6.23476276e+01, 3.02686516e+02, 2.90673704e+02,\n",
       "        2.15500967e+01, 6.26842529e+01, 3.03133868e+02, 2.86654174e+02,\n",
       "        2.11099239e+01, 6.26842529e+01, 3.01697048e+02, 2.86381785e+02,\n",
       "        1.89952994e+02, 2.56361754e+02, 1.71323482e+02, 6.26543307e+02,\n",
       "        1.99733728e+02, 2.59488998e+02, 2.21769044e+02, 5.73813712e+02,\n",
       "        1.86355115e+02, 2.58563777e+02, 2.32956405e+02, 5.77812549e+02,\n",
       "        8.34807899e+02, 6.57486291e+01, 7.49548869e+03, 1.55609449e+03,\n",
       "        5.32601558e+02, 2.00258317e+03, 5.25787655e+03, 7.15106167e+03,\n",
       "        1.11200345e+03, 4.05932945e+01, 1.97317369e+03, 8.32163989e+03,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92223417e+02, 4.09754173e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.11491855e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.92452401e+02, 4.10908846e+02,\n",
       "        2.30317173e+01, 6.52271927e+01, 3.01095891e+02, 2.82518676e+02,\n",
       "        1.93514857e+01, 6.45311983e+01, 3.07895765e+02, 2.89734319e+02,\n",
       "        2.08685303e+01, 6.67192626e+01, 3.07519495e+02, 2.86332258e+02,\n",
       "        1.55731371e+02, 2.46017700e+02, 1.89902191e+02, 4.90302684e+02,\n",
       "        1.89734909e+02, 2.54885913e+02, 2.21343479e+02, 5.64115735e+02,\n",
       "        1.76080973e+02, 2.46507782e+02, 1.75864519e+02, 6.28325257e+02,\n",
       "        6.13300418e+02, 1.54623931e+02, 1.49415601e+03, 3.18077506e+03,\n",
       "        7.12375043e+02, 6.41804396e+02, 2.13052996e+03, 4.50861386e+03,\n",
       "        6.40050347e+02, 6.07771097e+02, 2.07781480e+03, 3.26207744e+03,\n",
       "        1.08824017e+01, 1.00214215e+01, 1.87290155e+02, 4.09743662e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.93349534e+02, 4.10611944e+02,\n",
       "        1.08619622e+01, 1.12443566e+01, 1.94547835e+02, 4.08988153e+02,\n",
       "        4.17634609e+01, 9.50303226e+01, 2.92053492e+02, 2.90661388e+02,\n",
       "        1.64795496e+01, 5.66365214e+01, 3.10098203e+02, 3.01923846e+02,\n",
       "        2.06690321e+01, 5.98130978e+01, 3.03009244e+02, 3.03910689e+02,\n",
       "        1.51675648e+02, 2.27354628e+02, 1.89021627e+02, 4.38951951e+02,\n",
       "        1.77384598e+02, 2.35177243e+02, 2.86352863e+02, 5.38760658e+02,\n",
       "        1.43674725e+02, 2.41378946e+02, 2.92778153e+02, 4.27546491e+02,\n",
       "        6.21306330e+01, 2.09685579e+02, 1.95793639e+02, 6.05053586e+02,\n",
       "        3.65976320e+01, 9.60753986e+01, 1.81820259e+02, 5.77608939e+02,\n",
       "        6.83113949e+01, 2.15403946e+02, 1.21380183e+02, 5.28826111e+02,\n",
       "        1.47441137e+01, 1.38898044e+01, 1.86830874e+02, 3.65760769e+02,\n",
       "        1.08824017e+01, 1.27607036e+01, 1.93386700e+02, 4.10796027e+02,\n",
       "        1.12775096e+01, 1.10366058e+01, 1.91842302e+02, 4.12770011e+02,\n",
       "        1.35196154e+01, 6.80786473e+01, 2.32930419e+02, 1.51260063e+02,\n",
       "        1.87015745e+01, 5.34108603e+01, 3.12753122e+02, 2.96132677e+02,\n",
       "        3.74454566e+01, 6.58916956e+01, 2.62927447e+02, 3.17906698e+02,\n",
       "        4.47884162e+01, 5.30008176e+01, 1.97107489e+02, 4.90313997e+02,\n",
       "        1.42216767e+02, 1.01616140e+02, 2.48258212e+02, 4.62098664e+02,\n",
       "        1.98617913e+02, 1.64713414e+02, 1.70791029e+02, 4.69245418e+02,\n",
       "        3.87843038e+00, 6.69792671e+00, 9.19998913e+01, 3.65109064e+02,\n",
       "        1.29535924e+01, 7.52632860e+01, 1.69990202e+02, 3.17422242e+02,\n",
       "        1.78848788e+01, 4.05388976e+01, 5.12905449e+01, 2.84031094e+02,\n",
       "        1.81903515e+00, 8.32386128e+00, 2.05575242e+02, 2.78361791e+02,\n",
       "        1.29810974e+01, 8.98109125e+00, 2.03034075e+02, 4.09805762e+02,\n",
       "        1.45249976e+01, 1.83031873e+01, 2.17100084e+02, 3.86048851e+02,\n",
       "        4.86506823e+00, 7.04320003e+00, 7.73785644e+01, 2.38301410e+02,\n",
       "        1.01945519e+01, 2.41811221e+01, 1.58778091e+02, 2.86336100e+02,\n",
       "        2.16450148e+01, 1.11184531e+01, 1.02633631e+02, 2.52035240e+02,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.67695526e+00, 2.27701559e+01,\n",
       "        5.47133947e+00, 1.32285886e+01, 2.30933410e+02, 4.83122761e+02,\n",
       "        1.41421356e+00, 2.36126143e+00, 1.61855930e+02, 5.30103766e+02,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.58803629e+00, 9.99091031e+01,\n",
       "        6.65799436e+00, 7.73275860e+00, 4.14412301e+01, 5.08205451e+02,\n",
       "        9.42809042e-01, 1.41421356e+00, 1.66786097e+02, 5.78024990e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.40600719e+02,\n",
       "        7.49281137e+00, 1.19233664e+01, 1.98380818e+02, 4.22204461e+02,\n",
       "        9.42809042e-01, 1.88561808e+00, 1.21719961e+02, 3.11405698e+02,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.58803629e+00, 9.99091031e+01,\n",
       "        7.04856171e+00, 8.16877524e+00, 2.97950033e+01, 4.23805115e+02,\n",
       "        9.42809042e-01, 9.42809042e-01, 7.23419810e+01, 1.66282517e+02]),\n",
       " 'rank_test_loan_loss': array([184, 218,  20,  99, 181, 214,  25,  95, 181, 214,  22,  96, 319,\n",
       "        155, 331, 321, 314, 323, 324, 322, 328, 318, 332, 330, 272, 244,\n",
       "         44, 127, 272, 244,  44, 127, 272, 244,  44, 127, 197, 163,   6,\n",
       "         79, 197, 163,   1,  75, 197, 163,   2,  75, 185, 217,  17,  92,\n",
       "        181, 214,  22,  94, 180, 218,  19,  97, 150, 243, 336, 335, 315,\n",
       "        327, 329, 320, 317, 326, 333, 334, 272, 244,  44, 134, 272, 244,\n",
       "         44, 127, 272, 244,  44, 127, 201, 160,  10,  71, 197, 161,   3,\n",
       "         78, 196, 161,   7,  75, 189, 223,  16,  89, 179, 221,  21,  98,\n",
       "        186, 220,  18,  91, 154, 260, 149, 121, 151, 316, 148, 325, 153,\n",
       "        259, 122, 157, 272, 244,  42, 133, 272, 244,  44, 127, 272, 244,\n",
       "         44, 136, 204, 168,   9,  73, 202, 167,   3,  74, 203, 166,   8,\n",
       "         72, 194, 227,  28,  90, 188, 225,  26,  93, 191, 226,  24,  88,\n",
       "        158, 240,  64, 107, 156, 176,  62, 124, 159, 192,  58, 113, 270,\n",
       "        255,  54, 140, 272, 244,  52, 134, 272, 244,  53, 137, 208, 171,\n",
       "         13,  80, 205, 169,  14,  69, 206, 170,   5,  70, 211, 224,  35,\n",
       "        102, 195, 230,  30, 101, 210, 229,  29, 100, 228, 190,  34, 106,\n",
       "        232, 175,  32, 104, 234, 212,  33, 103, 283, 263,  57, 142, 270,\n",
       "        255,  43, 138, 269, 257,  55, 141, 207, 172,  15,  83, 213, 173,\n",
       "         11,  81, 222, 174,  12,  82, 264, 258,  66, 120, 231, 209,  36,\n",
       "        110, 238, 237,  39, 112, 265, 241,  65, 114, 235, 187,  31,  85,\n",
       "        239, 193,  38,  86, 297, 293,  67, 146, 268, 261,  56, 139, 284,\n",
       "        266,  59, 143, 262, 242,  63, 115, 233, 177,  27,  84, 236, 178,\n",
       "         37,  87, 298, 298, 295, 144, 294, 289,  61, 117, 311, 296, 116,\n",
       "        123, 298, 298, 285, 125, 292, 287,  41, 108, 307, 311, 111, 118,\n",
       "        298, 298, 298, 152, 290, 267,  60, 145, 307, 313,  68, 147, 298,\n",
       "        298, 285, 125, 291, 288,  40, 105, 307, 307, 109, 119]),\n",
       " 'split0_test_neg_brier_score': array([0.12191179, 0.12187572, 0.16266938, 0.22571521, 0.12191184,\n",
       "        0.12187539, 0.16266834, 0.22571675, 0.12191183, 0.12187544,\n",
       "        0.1626685 , 0.22571652, 0.23950364, 0.17131138, 0.61494255,\n",
       "        0.36679401, 0.24702991, 0.18624408, 0.47117658, 0.34374852,\n",
       "        0.30199754, 0.20816306, 0.61535998, 0.51480045, 0.1252374 ,\n",
       "        0.12642417, 0.17907263, 0.23304523, 0.12523739, 0.12642415,\n",
       "        0.17907258, 0.23304521, 0.12523739, 0.12642416, 0.17907259,\n",
       "        0.23304522, 0.12160793, 0.12178785, 0.16517973, 0.22324673,\n",
       "        0.12160792, 0.12178781, 0.16517981, 0.22324668, 0.12160792,\n",
       "        0.12178782, 0.1651798 , 0.22324669, 0.12191121, 0.12187709,\n",
       "        0.16268048, 0.22570005, 0.12191183, 0.12187538, 0.16266833,\n",
       "        0.22571667, 0.12191175, 0.12187588, 0.16266989, 0.2257144 ,\n",
       "        0.17585157, 0.13950555, 0.67367158, 0.376184  , 0.22258676,\n",
       "        0.15268968, 0.58710984, 0.31175318, 0.1910025 , 0.1591822 ,\n",
       "        0.56516989, 0.51345304, 0.12523756, 0.12642435, 0.17907313,\n",
       "        0.23304535, 0.1252374 , 0.12642417, 0.1790726 , 0.23304522,\n",
       "        0.12523743, 0.1264242 , 0.17907268, 0.23304524, 0.12160804,\n",
       "        0.12178798, 0.16517967, 0.22324724, 0.12160792, 0.1217878 ,\n",
       "        0.16517978, 0.22324669, 0.12160794, 0.12178784, 0.16517967,\n",
       "        0.22324676, 0.12191103, 0.12185075, 0.16257626, 0.22558082,\n",
       "        0.12191176, 0.12187528, 0.1626682 , 0.22571591, 0.12191096,\n",
       "        0.12187595, 0.16269083, 0.22567929, 0.12410729, 0.12598908,\n",
       "        0.14507206, 0.19681605, 0.12972823, 0.12870061, 0.19083749,\n",
       "        0.38017702, 0.12497367, 0.12628916, 0.14818661, 0.34343793,\n",
       "        0.12523917, 0.12642618, 0.17907815, 0.23304661, 0.12523756,\n",
       "        0.12642436, 0.17907282, 0.23304527, 0.1252378 , 0.12642463,\n",
       "        0.17907362, 0.23304547, 0.12160843, 0.12178913, 0.16517848,\n",
       "        0.2232495 , 0.12160791, 0.12178774, 0.16517954, 0.22324673,\n",
       "        0.12160811, 0.121788  , 0.16517933, 0.22324756, 0.12188423,\n",
       "        0.12184747, 0.16239708, 0.22500538, 0.12191148, 0.12187463,\n",
       "        0.16266705, 0.22570858, 0.12190568, 0.12183816, 0.16239011,\n",
       "        0.22559168, 0.12267309, 0.12260658, 0.15156712, 0.21347093,\n",
       "        0.12297271, 0.12211863, 0.17665722, 0.2527048 , 0.12279694,\n",
       "        0.12202177, 0.17060702, 0.24113731, 0.12525525, 0.12644448,\n",
       "        0.17912842, 0.23305913, 0.12523915, 0.12642622, 0.17907506,\n",
       "        0.23304573, 0.12524156, 0.12642896, 0.17908305, 0.23304774,\n",
       "        0.1216184 , 0.12179307, 0.16516773, 0.22326006, 0.12160828,\n",
       "        0.12178747, 0.1651772 , 0.22324739, 0.12160853, 0.12178946,\n",
       "        0.16517606, 0.22325095, 0.12210165, 0.12197389, 0.16260703,\n",
       "        0.22410642, 0.12192861, 0.12188608, 0.16266153, 0.22565605,\n",
       "        0.12192226, 0.12186131, 0.16297589, 0.22470535, 0.12194463,\n",
       "        0.12199939, 0.16273263, 0.22421167, 0.12175359, 0.12181617,\n",
       "        0.15878952, 0.22631775, 0.12176439, 0.12183371, 0.15871532,\n",
       "        0.22489087, 0.1254202 , 0.12663147, 0.17962588, 0.23319434,\n",
       "        0.12525503, 0.12644489, 0.17909741, 0.23305037, 0.12527926,\n",
       "        0.12647234, 0.1791775 , 0.23306996, 0.12184912, 0.12196063,\n",
       "        0.16508799, 0.22347146, 0.12163139, 0.12180142, 0.16516216,\n",
       "        0.22326614, 0.12165295, 0.12181373, 0.16515546, 0.22330229,\n",
       "        0.12333687, 0.12325151, 0.16350227, 0.22668898, 0.12213798,\n",
       "        0.12205928, 0.16259722, 0.22560684, 0.12241143, 0.12227359,\n",
       "        0.16254073, 0.22455246, 0.12326183, 0.12326625, 0.16610394,\n",
       "        0.22557299, 0.12186022, 0.12196779, 0.16375758, 0.22371984,\n",
       "        0.12222519, 0.12228087, 0.16400596, 0.22346311, 0.12737823,\n",
       "        0.12880358, 0.18464712, 0.23541171, 0.12541544, 0.12663272,\n",
       "        0.17932038, 0.23309698, 0.12567053, 0.1269197 , 0.18011757,\n",
       "        0.23332665, 0.12324207, 0.12325777, 0.16617552, 0.2259741 ,\n",
       "        0.1218374 , 0.12195218, 0.16506798, 0.22352231, 0.12213939,\n",
       "        0.12220497, 0.16513261, 0.2239055 , 0.12972362, 0.12987441,\n",
       "        0.1704085 , 0.2404848 , 0.12329602, 0.12302891, 0.16262749,\n",
       "        0.22642553, 0.12511534, 0.1246982 , 0.16349776, 0.22909859,\n",
       "        0.12970863, 0.12989059, 0.17286575, 0.23907682, 0.12284757,\n",
       "        0.12277008, 0.16498781, 0.22540125, 0.12488382, 0.12469828,\n",
       "        0.16649051, 0.22807881, 0.14550872, 0.14901134, 0.2161211 ,\n",
       "        0.2474175 , 0.12714603, 0.12859919, 0.18148926, 0.23357971,\n",
       "        0.13047377, 0.13220361, 0.18934709, 0.23775145, 0.12970863,\n",
       "        0.1298903 , 0.17287211, 0.23909139, 0.12284616, 0.12276463,\n",
       "        0.16500609, 0.22501026, 0.12475425, 0.12452936, 0.16632217,\n",
       "        0.22805065]),\n",
       " 'split1_test_neg_brier_score': array([0.12195093, 0.12214107, 0.16470001, 0.22483187, 0.12195098,\n",
       "        0.12214135, 0.16470078, 0.22483396, 0.12195097, 0.12214131,\n",
       "        0.16470067, 0.22483365, 0.19999589, 0.15804956, 0.26879625,\n",
       "        0.2869935 , 0.17621996, 0.32805548, 0.37434756, 0.23363272,\n",
       "        0.16269709, 0.20215708, 0.21167393, 0.34986879, 0.12544756,\n",
       "        0.12666913, 0.17940045, 0.23317684, 0.12544754, 0.12666911,\n",
       "        0.17940039, 0.23317683, 0.12544754, 0.12666911, 0.1794004 ,\n",
       "        0.23317683, 0.12182618, 0.12204132, 0.16584907, 0.22353988,\n",
       "        0.12182619, 0.12204132, 0.16584909, 0.22353988, 0.12182619,\n",
       "        0.12204132, 0.16584909, 0.22353988, 0.12195083, 0.12213883,\n",
       "        0.16469326, 0.22481399, 0.12195099, 0.12214136, 0.16470069,\n",
       "        0.22483406, 0.12195091, 0.12214094, 0.16469954, 0.22483094,\n",
       "        0.18194587, 0.13956697, 0.16076966, 0.52949049, 0.19820151,\n",
       "        0.33254095, 0.23176779, 0.29607516, 0.16506877, 0.33687448,\n",
       "        0.26004705, 0.27593315, 0.12544771, 0.12666931, 0.17940094,\n",
       "        0.23317699, 0.12544756, 0.12666913, 0.17940042, 0.23317683,\n",
       "        0.12544758, 0.12666915, 0.17940049, 0.23317685, 0.1218261 ,\n",
       "        0.12204131, 0.16584886, 0.2235399 , 0.12182619, 0.12204132,\n",
       "        0.16584905, 0.22353986, 0.12182618, 0.12204133, 0.16584902,\n",
       "        0.22353987, 0.12195166, 0.12210945, 0.1646401 , 0.22482068,\n",
       "        0.12195105, 0.12214144, 0.1646998 , 0.22483507, 0.12195087,\n",
       "        0.12213803, 0.16468869, 0.22480572, 0.12814345, 0.12813838,\n",
       "        0.1459391 , 0.16943247, 0.13254873, 0.15322007, 0.16736943,\n",
       "        0.18099967, 0.12851332, 0.12921358, 0.14646006, 0.17221377,\n",
       "        0.12544927, 0.12667109, 0.17940587, 0.23317849, 0.12544771,\n",
       "        0.12666931, 0.17940064, 0.23317688, 0.12544795, 0.12666958,\n",
       "        0.17940142, 0.23317712, 0.12182612, 0.12204135, 0.16584729,\n",
       "        0.22353954, 0.12182627, 0.12204135, 0.16584866, 0.22353971,\n",
       "        0.12182616, 0.12204133, 0.16584839, 0.22353976, 0.12196636,\n",
       "        0.12208445, 0.16437035, 0.22500096, 0.12195181, 0.12214247,\n",
       "        0.16469108, 0.2248452 , 0.12195222, 0.12209316, 0.164596  ,\n",
       "        0.22486091, 0.12274792, 0.12281781, 0.15484211, 0.24158617,\n",
       "        0.12281336, 0.12361293, 0.15437388, 0.20198214, 0.12273942,\n",
       "        0.12292642, 0.15886038, 0.20741806, 0.12546488, 0.12668901,\n",
       "        0.17945526, 0.23319267, 0.12544927, 0.12667115, 0.17940283,\n",
       "        0.23317733, 0.12545161, 0.12667382, 0.17941068, 0.23317975,\n",
       "        0.12183351, 0.12204428, 0.16583132, 0.22354218, 0.12182723,\n",
       "        0.12204184, 0.165845  , 0.22353842, 0.12182754, 0.12204229,\n",
       "        0.16584338, 0.2235258 , 0.12211574, 0.12221365, 0.16359606,\n",
       "        0.22482667, 0.12196719, 0.12215996, 0.16462347, 0.22494531,\n",
       "        0.12198575, 0.12209627, 0.16428443, 0.22513294, 0.12203434,\n",
       "        0.12218658, 0.1635623 , 0.2256624 , 0.12189724, 0.12207387,\n",
       "        0.16453772, 0.22535227, 0.1219049 , 0.12203022, 0.16420925,\n",
       "        0.22467536, 0.1256249 , 0.12687035, 0.17994334, 0.23332748,\n",
       "        0.12546484, 0.12668952, 0.17942476, 0.23318188, 0.12548834,\n",
       "        0.12671639, 0.17950349, 0.23320438, 0.1220222 , 0.12218183,\n",
       "        0.16569731, 0.22368245, 0.12184619, 0.12205274, 0.16581969,\n",
       "        0.22354198, 0.12186502, 0.12206388, 0.16581106, 0.22356433,\n",
       "        0.12335887, 0.12362201, 0.16337141, 0.226551  , 0.12210508,\n",
       "        0.12223334, 0.1643255 , 0.22554635, 0.12233593, 0.12249846,\n",
       "        0.1636161 , 0.22554421, 0.12335934, 0.12351262, 0.16640773,\n",
       "        0.22624457, 0.12202426, 0.1221278 , 0.16430815, 0.22411223,\n",
       "        0.12235246, 0.12238865, 0.16470639, 0.22406408, 0.12750809,\n",
       "        0.1289527 , 0.18481509, 0.23559099, 0.1256222 , 0.12687444,\n",
       "        0.17964355, 0.23322758, 0.12586984, 0.12714944, 0.18042   ,\n",
       "        0.2334246 , 0.12329428, 0.1233463 , 0.16669422, 0.22619418,\n",
       "        0.1220029 , 0.12215705, 0.16567453, 0.22374828, 0.12226898,\n",
       "        0.12237496, 0.16568434, 0.22406267, 0.12971539, 0.12987441,\n",
       "        0.17008739, 0.24002607, 0.12319808, 0.12334827, 0.16390834,\n",
       "        0.22735155, 0.12467348, 0.12520628, 0.16329474, 0.22911154,\n",
       "        0.12970863, 0.12989059, 0.17284055, 0.23922973, 0.12293931,\n",
       "        0.12288952, 0.16524565, 0.22563048, 0.12488225, 0.12471163,\n",
       "        0.16660574, 0.2282829 , 0.14547158, 0.14902538, 0.21606657,\n",
       "        0.24743807, 0.12732483, 0.12881371, 0.18177403, 0.233702  ,\n",
       "        0.13054323, 0.13228048, 0.18929855, 0.23788797, 0.12970863,\n",
       "        0.1298903 , 0.17285802, 0.23924698, 0.12292638, 0.1228804 ,\n",
       "        0.1655067 , 0.22518976, 0.12473844, 0.12454302, 0.16657822,\n",
       "        0.22817678]),\n",
       " 'split2_test_neg_brier_score': array([0.12182822, 0.12177857, 0.1616055 , 0.22408074, 0.12182866,\n",
       "        0.12177851, 0.16160986, 0.22407799, 0.12182853, 0.12177852,\n",
       "        0.1616092 , 0.22407841, 0.34148283, 0.20387076, 0.29695119,\n",
       "        0.43438661, 0.33244151, 0.22514402, 0.33118326, 0.4512605 ,\n",
       "        0.34015697, 0.20819597, 0.32147357, 0.40156747, 0.1253312 ,\n",
       "        0.12653829, 0.17917002, 0.23299029, 0.12533118, 0.12653827,\n",
       "        0.17916997, 0.23299027, 0.12533118, 0.12653828, 0.17916998,\n",
       "        0.23299027, 0.12159506, 0.12178918, 0.16545835, 0.22323073,\n",
       "        0.12159501, 0.12178918, 0.16545838, 0.22323074, 0.12159501,\n",
       "        0.12178918, 0.16545838, 0.22323074, 0.12182605, 0.12177935,\n",
       "        0.16157391, 0.2241021 , 0.12182866, 0.12177851, 0.16160981,\n",
       "        0.22407798, 0.12182799, 0.1217786 , 0.1616033 , 0.22408205,\n",
       "        0.13042153, 0.14217339, 0.27707288, 0.35643605, 0.27556465,\n",
       "        0.1810732 , 0.2353952 , 0.35365642, 0.24111438, 0.17730615,\n",
       "        0.32009823, 0.32834948, 0.12533136, 0.12653847, 0.17917052,\n",
       "        0.23299044, 0.1253312 , 0.12653829, 0.17916999, 0.23299027,\n",
       "        0.12533122, 0.12653832, 0.17917007, 0.2329903 , 0.121595  ,\n",
       "        0.12178923, 0.16545808, 0.22323068, 0.12159501, 0.12178918,\n",
       "        0.16545833, 0.22323072, 0.12159509, 0.12178919, 0.16545828,\n",
       "        0.2232307 , 0.12181651, 0.12177828, 0.16147679, 0.22409458,\n",
       "        0.12182869, 0.12177849, 0.16160938, 0.22407786, 0.12182415,\n",
       "        0.12177994, 0.16155629, 0.22410962, 0.12586098, 0.12740462,\n",
       "        0.26896844, 0.22079371, 0.13567115, 0.1416491 , 0.15748555,\n",
       "        0.25621904, 0.12933249, 0.12781904, 0.13558888, 0.21991284,\n",
       "        0.12533297, 0.1265403 , 0.17917548, 0.23299202, 0.12533135,\n",
       "        0.12653848, 0.17917022, 0.23299032, 0.1253316 , 0.12653875,\n",
       "        0.17917101, 0.23299058, 0.12159404, 0.12178996, 0.16545624,\n",
       "        0.22322895, 0.12159509, 0.12178923, 0.16545776, 0.22323053,\n",
       "        0.12159503, 0.1217893 , 0.16545744, 0.2232306 , 0.1218071 ,\n",
       "        0.12178194, 0.16177585, 0.22390801, 0.12182924, 0.12177859,\n",
       "        0.16160528, 0.22407683, 0.12180154, 0.12177894, 0.16148218,\n",
       "        0.22402114, 0.12282534, 0.12230167, 0.1469506 , 0.21210791,\n",
       "        0.12294214, 0.12263312, 0.14591138, 0.21129229, 0.12287771,\n",
       "        0.12199838, 0.14926904, 0.21627481, 0.12534912, 0.1265586 ,\n",
       "        0.17922506, 0.23300838, 0.12533293, 0.12654033, 0.17917244,\n",
       "        0.2329908 , 0.12533535, 0.12654307, 0.17918033, 0.23299335,\n",
       "        0.12160753, 0.12179689, 0.16542961, 0.22323237, 0.12159615,\n",
       "        0.12178986, 0.1654524 , 0.22322883, 0.12159485, 0.12179018,\n",
       "        0.16544914, 0.22322401, 0.12199038, 0.12198009, 0.16176275,\n",
       "        0.22377609, 0.12184515, 0.12178948, 0.16157602, 0.22407986,\n",
       "        0.12183206, 0.12181168, 0.16156618, 0.22404683, 0.12191713,\n",
       "        0.12200941, 0.16182275, 0.22314419, 0.12165137, 0.1218002 ,\n",
       "        0.16158596, 0.22371991, 0.12166785, 0.12185449, 0.16151419,\n",
       "        0.22363811, 0.12551468, 0.1267458 , 0.17972346, 0.23314797,\n",
       "        0.12534866, 0.12655888, 0.17919468, 0.23299559, 0.12537303,\n",
       "        0.12658637, 0.17927357, 0.23302277, 0.12183158, 0.12197377,\n",
       "        0.16533144, 0.22342514, 0.12161739, 0.1218029 , 0.16541502,\n",
       "        0.22323014, 0.12164753, 0.12182371, 0.16539659, 0.22325158,\n",
       "        0.12331069, 0.1232729 , 0.16367748, 0.22680232, 0.12199519,\n",
       "        0.1219216 , 0.16144344, 0.22427129, 0.12214191, 0.12215516,\n",
       "        0.16142892, 0.22406507, 0.12339666, 0.12325781, 0.16646423,\n",
       "        0.22533095, 0.12180636, 0.12194316, 0.16450708, 0.22356286,\n",
       "        0.12217398, 0.12224102, 0.16478151, 0.22357855, 0.12744615,\n",
       "        0.12888766, 0.18469069, 0.23554531, 0.12550761, 0.1267456 ,\n",
       "        0.17941653, 0.23304371, 0.1257611 , 0.12703157, 0.18021277,\n",
       "        0.2332614 , 0.12319454, 0.1232016 , 0.16635471, 0.22600622,\n",
       "        0.12179258, 0.1219218 , 0.16525501, 0.22345681, 0.12209188,\n",
       "        0.12217248, 0.16531929, 0.22383358, 0.12971626, 0.12987485,\n",
       "        0.16995719, 0.24199592, 0.12271399, 0.12279777, 0.1616454 ,\n",
       "        0.22543234, 0.12448917, 0.12466047, 0.16352838, 0.22930469,\n",
       "        0.12970909, 0.12989103, 0.17281753, 0.23957713, 0.12285344,\n",
       "        0.12274533, 0.16509457, 0.22492165, 0.12479696, 0.12457138,\n",
       "        0.16662669, 0.22827609, 0.14550206, 0.14906735, 0.21601923,\n",
       "        0.24755052, 0.12722541, 0.12870137, 0.18157468, 0.23354004,\n",
       "        0.13050394, 0.13225214, 0.18925586, 0.23795288, 0.12970909,\n",
       "        0.12989073, 0.17281303, 0.23959171, 0.12281313, 0.1227409 ,\n",
       "        0.16521889, 0.22503401, 0.12472466, 0.12449646, 0.16643868,\n",
       "        0.2281618 ]),\n",
       " 'mean_test_neg_brier_score': array([0.12189698, 0.12193179, 0.16299163, 0.22487594, 0.12189716,\n",
       "        0.12193175, 0.16299299, 0.22487624, 0.12189711, 0.12193175,\n",
       "        0.16299279, 0.22487619, 0.26032745, 0.1777439 , 0.39356333,\n",
       "        0.36272471, 0.25189713, 0.2464812 , 0.3922358 , 0.34288058,\n",
       "        0.26828386, 0.20617203, 0.38283583, 0.4220789 , 0.12533872,\n",
       "        0.12654386, 0.17921437, 0.23307079, 0.1253387 , 0.12654385,\n",
       "        0.17921431, 0.23307077, 0.12533871, 0.12654385, 0.17921432,\n",
       "        0.23307077, 0.12167639, 0.12187278, 0.16549572, 0.22333911,\n",
       "        0.12167637, 0.12187277, 0.16549576, 0.2233391 , 0.12167637,\n",
       "        0.12187277, 0.16549575, 0.2233391 , 0.12189603, 0.12193176,\n",
       "        0.16298255, 0.22487205, 0.12189716, 0.12193175, 0.16299294,\n",
       "        0.22487624, 0.12189688, 0.12193181, 0.16299091, 0.2248758 ,\n",
       "        0.16273966, 0.14041531, 0.37050471, 0.42070351, 0.23211764,\n",
       "        0.22210128, 0.35142427, 0.32049492, 0.19906188, 0.22445428,\n",
       "        0.38177172, 0.37257856, 0.12533888, 0.12654404, 0.17921486,\n",
       "        0.23307093, 0.12533872, 0.12654386, 0.17921434, 0.23307077,\n",
       "        0.12533874, 0.12654389, 0.17921442, 0.2330708 , 0.12167638,\n",
       "        0.12187284, 0.16549554, 0.22333927, 0.12167638, 0.12187277,\n",
       "        0.16549572, 0.22333909, 0.1216764 , 0.12187279, 0.16549566,\n",
       "        0.22333911, 0.12189307, 0.12191283, 0.16289772, 0.22483203,\n",
       "        0.12189717, 0.12193174, 0.16299246, 0.22487628, 0.12189533,\n",
       "        0.12193131, 0.1629786 , 0.22486488, 0.12603724, 0.12717736,\n",
       "        0.18665987, 0.19568075, 0.13264937, 0.14118993, 0.17189749,\n",
       "        0.27246524, 0.12760649, 0.12777393, 0.14341185, 0.24518818,\n",
       "        0.12534047, 0.12654586, 0.17921983, 0.23307238, 0.12533888,\n",
       "        0.12654405, 0.17921456, 0.23307082, 0.12533912, 0.12654432,\n",
       "        0.17921535, 0.23307105, 0.1216762 , 0.12187348, 0.165494  ,\n",
       "        0.22333933, 0.12167642, 0.12187277, 0.16549532, 0.22333899,\n",
       "        0.12167644, 0.12187288, 0.16549505, 0.22333931, 0.1218859 ,\n",
       "        0.12190462, 0.16284776, 0.22463812, 0.12189751, 0.1219319 ,\n",
       "        0.1629878 , 0.22487687, 0.12188648, 0.12190342, 0.16282277,\n",
       "        0.22482457, 0.12274878, 0.12257535, 0.15111994, 0.22238834,\n",
       "        0.12290941, 0.12278823, 0.15898083, 0.22199308, 0.12280469,\n",
       "        0.12231552, 0.15957881, 0.22161006, 0.12535642, 0.12656403,\n",
       "        0.17926958, 0.23308673, 0.12534045, 0.1265459 , 0.17921678,\n",
       "        0.23307129, 0.12534284, 0.12654861, 0.17922469, 0.23307362,\n",
       "        0.12168648, 0.12187808, 0.16547622, 0.22334487, 0.12167722,\n",
       "        0.12187306, 0.16549153, 0.22333821, 0.12167697, 0.12187398,\n",
       "        0.16548953, 0.22333359, 0.12206926, 0.12205588, 0.16265528,\n",
       "        0.22423639, 0.12191365, 0.12194517, 0.16295368, 0.22489374,\n",
       "        0.12191336, 0.12192309, 0.16294217, 0.22462837, 0.12196537,\n",
       "        0.12206513, 0.16270589, 0.22433942, 0.1217674 , 0.12189675,\n",
       "        0.16163773, 0.22512997, 0.12177905, 0.12190614, 0.16147959,\n",
       "        0.22440145, 0.12551993, 0.12674921, 0.17976423, 0.23322326,\n",
       "        0.12535617, 0.12656443, 0.17923895, 0.23307595, 0.12538021,\n",
       "        0.1265917 , 0.17931819, 0.23309904, 0.12190097, 0.12203874,\n",
       "        0.16537225, 0.22352635, 0.12169833, 0.12188568, 0.16546562,\n",
       "        0.22334609, 0.12172183, 0.12190044, 0.16545437, 0.22337274,\n",
       "        0.12333548, 0.12338214, 0.16351705, 0.22668077, 0.12207942,\n",
       "        0.12207141, 0.16278872, 0.22514149, 0.12229643, 0.12230907,\n",
       "        0.16252858, 0.22472058, 0.12333928, 0.12334556, 0.1663253 ,\n",
       "        0.22571617, 0.12189695, 0.12201292, 0.16419094, 0.22379831,\n",
       "        0.12225054, 0.12230351, 0.16449796, 0.22370192, 0.12744416,\n",
       "        0.12888131, 0.18471763, 0.235516  , 0.12551508, 0.12675092,\n",
       "        0.17946015, 0.23312276, 0.12576715, 0.12703357, 0.18025012,\n",
       "        0.23333755, 0.12324363, 0.12326856, 0.16640815, 0.22605816,\n",
       "        0.12187763, 0.12201034, 0.1653325 , 0.2235758 , 0.12216675,\n",
       "        0.1222508 , 0.16537874, 0.22393392, 0.12971842, 0.12987456,\n",
       "        0.17015103, 0.24083559, 0.12306936, 0.12305832, 0.16272708,\n",
       "        0.22640314, 0.12475933, 0.12485498, 0.16344029, 0.22917161,\n",
       "        0.12970878, 0.12989074, 0.17284128, 0.23929456, 0.12288011,\n",
       "        0.12280165, 0.16510934, 0.22531779, 0.12485434, 0.12466043,\n",
       "        0.16657431, 0.2282126 , 0.14549412, 0.14903469, 0.21606897,\n",
       "        0.24746869, 0.12723209, 0.12870475, 0.18161266, 0.23360725,\n",
       "        0.13050698, 0.13224541, 0.1893005 , 0.2378641 , 0.12970878,\n",
       "        0.12989044, 0.17284772, 0.23931003, 0.12286189, 0.12279531,\n",
       "        0.16524389, 0.22507801, 0.12473912, 0.12452295, 0.16644636,\n",
       "        0.22812975]),\n",
       " 'std_test_neg_brier_score': array([5.11794495e-05, 1.53208078e-04, 1.28371478e-03, 6.67998257e-04,\n",
       "        5.10047496e-05, 1.53396860e-04, 1.28257822e-03, 6.69686516e-04,\n",
       "        5.10585024e-05, 1.53368631e-04, 1.28274846e-03, 6.69430509e-04,\n",
       "        5.96090744e-02, 1.92514685e-02, 1.56960175e-01, 6.02417442e-02,\n",
       "        6.38699772e-02, 5.98279465e-02, 5.85350276e-02, 8.88482863e-02,\n",
       "        7.62690895e-02, 2.83903387e-03, 1.70420270e-01, 6.88774352e-02,\n",
       "        8.59606451e-05, 1.00080175e-04, 1.37454103e-04, 7.82760611e-05,\n",
       "        8.59608532e-05, 1.00080338e-04, 1.37454444e-04, 7.82760647e-05,\n",
       "        8.59608220e-05, 1.00080314e-04, 1.37454393e-04, 7.82760642e-05,\n",
       "        1.06045936e-04, 1.19175364e-04, 2.74529838e-04, 1.42114257e-04,\n",
       "        1.06067136e-04, 1.19183981e-04, 2.74507107e-04, 1.42119204e-04,\n",
       "        1.06064019e-04, 1.19182477e-04, 2.74510641e-04, 1.42118487e-04,\n",
       "        5.20598635e-05, 1.51764013e-04, 1.29125678e-03, 6.53648896e-04,\n",
       "        5.10047470e-05, 1.53402434e-04, 1.28255477e-03, 6.69657399e-04,\n",
       "        5.12701841e-05, 1.53120058e-04, 1.28425482e-03, 6.67155393e-04,\n",
       "        2.29874044e-02, 1.24340908e-03, 2.19566581e-01, 7.73453315e-02,\n",
       "        3.22943967e-02, 7.89476491e-02, 1.66661439e-01, 2.43065701e-02,\n",
       "        3.15642114e-02, 7.98366898e-02, 1.31979041e-01, 1.01885830e-01,\n",
       "        8.59586457e-05, 1.00078584e-04, 1.37450857e-04, 7.82759363e-05,\n",
       "        8.59607275e-05, 1.00080218e-04, 1.37454266e-04, 7.82759710e-05,\n",
       "        8.59604153e-05, 1.00079973e-04, 1.37453754e-04, 7.82759656e-05,\n",
       "        1.06002810e-04, 1.19128358e-04, 2.74475439e-04, 1.42025130e-04,\n",
       "        1.06068761e-04, 1.19186036e-04, 2.74502640e-04, 1.42115966e-04,\n",
       "        1.06038493e-04, 1.19176319e-04, 2.74535421e-04, 1.42108389e-04,\n",
       "        5.66166828e-05, 1.42146375e-04, 1.31126573e-03, 6.06806837e-04,\n",
       "        5.10047156e-05, 1.53457765e-04, 1.28232073e-03, 6.69366976e-04,\n",
       "        5.29018346e-05, 1.51338137e-04, 1.29488419e-03, 6.42177117e-04,\n",
       "        1.65245999e-03, 8.92039261e-04, 5.82020270e-02, 2.09835018e-02,\n",
       "        2.42723196e-03, 1.00152923e-02, 1.39872647e-02, 8.21212842e-02,\n",
       "        1.89148654e-03, 1.19431384e-03, 5.57640378e-03, 7.21505848e-02,\n",
       "        8.59386139e-05, 1.00062670e-04, 1.37417288e-04, 7.82752791e-05,\n",
       "        8.59594703e-05, 1.00079015e-04, 1.37452482e-04, 7.82750336e-05,\n",
       "        8.59563475e-05, 1.00076564e-04, 1.37447369e-04, 7.82749862e-05,\n",
       "        1.06174892e-04, 1.18704427e-04, 2.74345960e-04, 1.41817982e-04,\n",
       "        1.06084462e-04, 1.19205987e-04, 2.74458148e-04, 1.42083684e-04,\n",
       "        1.06004654e-04, 1.19115557e-04, 2.74435444e-04, 1.41911075e-04,\n",
       "        6.50285486e-05, 1.29942937e-04, 1.10609854e-03, 5.16267030e-04,\n",
       "        5.10035357e-05, 1.53973134e-04, 1.28002960e-03, 6.66536612e-04,\n",
       "        6.29943325e-05, 1.36327349e-04, 1.30750733e-03, 6.41685153e-04,\n",
       "        6.21612966e-05, 2.11865924e-04, 3.23717761e-03, 1.35863155e-02,\n",
       "        6.90479505e-05, 6.19824853e-04, 1.29677729e-02, 2.20465770e-02,\n",
       "        5.67202125e-05, 4.32074793e-04, 8.72599131e-03, 1.42734196e-02,\n",
       "        8.57371564e-05, 9.99029428e-05, 1.37094419e-04, 7.77264153e-05,\n",
       "        8.59468990e-05, 1.00066985e-04, 1.37434644e-04, 7.82656608e-05,\n",
       "        8.59155971e-05, 1.00042448e-04, 1.37380610e-04, 7.82636197e-05,\n",
       "        1.04059101e-04, 1.17530262e-04, 2.72904149e-04, 1.39975850e-04,\n",
       "        1.06190904e-04, 1.19350616e-04, 2.74029704e-04, 1.41770575e-04,\n",
       "        1.06613347e-04, 1.19018618e-04, 2.73925516e-04, 1.36357496e-04,\n",
       "        5.60692802e-05, 1.11589454e-04, 7.49225577e-04, 4.38631661e-04,\n",
       "        5.09317502e-05, 1.56913323e-04, 1.26114975e-03, 6.44505852e-04,\n",
       "        6.30566339e-05, 1.24126555e-04, 1.10997841e-03, 4.46731902e-04,\n",
       "        5.00481289e-05, 8.59778855e-05, 7.10421307e-04, 1.03201436e-03,\n",
       "        1.00848558e-04, 1.25416505e-04, 2.34697664e-03, 1.07214887e-03,\n",
       "        9.73281241e-05, 8.81495244e-05, 2.24302085e-03, 5.46885643e-04,\n",
       "        8.36532045e-05, 9.75537468e-05, 1.32770374e-04, 7.60837533e-05,\n",
       "        8.58212929e-05, 9.99467636e-05, 1.37256425e-04, 7.81720353e-05,\n",
       "        8.55064618e-05, 9.97037498e-05, 1.36770572e-04, 7.69407571e-05,\n",
       "        8.60246111e-05, 1.01319248e-04, 2.50423222e-04, 1.11987416e-04,\n",
       "        1.04715725e-04, 1.18127832e-04, 2.70813176e-04, 1.39294306e-04,\n",
       "        1.01272528e-04, 1.15639530e-04, 2.70745666e-04, 1.37053530e-04,\n",
       "        1.96951764e-05, 1.69840474e-04, 1.25390544e-04, 1.02763976e-04,\n",
       "        6.10526474e-05, 1.27553753e-04, 1.18436353e-03, 6.15822052e-04,\n",
       "        1.13520814e-04, 1.42382693e-04, 8.92955101e-04, 6.15445958e-04,\n",
       "        5.68433659e-05, 1.18182054e-04, 1.58216197e-04, 3.86479584e-04,\n",
       "        9.26692617e-05, 8.18536655e-05, 3.17008255e-04, 2.31043385e-04,\n",
       "        7.50361999e-05, 6.23620058e-05, 3.49242378e-04, 2.60388377e-04,\n",
       "        5.30314591e-05, 6.10431385e-05, 7.11701259e-05, 7.60704585e-05,\n",
       "        8.45759083e-05, 9.87524421e-05, 1.35490073e-04, 7.72459388e-05,\n",
       "        8.14801388e-05, 9.38042871e-05, 1.26258714e-04, 6.70688859e-05,\n",
       "        4.07358329e-05, 5.95639314e-05, 2.15105746e-04, 9.70674118e-05,\n",
       "        9.04523186e-05, 1.04478614e-04, 2.53614783e-04, 1.24859342e-04,\n",
       "        7.48460346e-05, 8.87899800e-05, 2.29132208e-04, 9.56565145e-05,\n",
       "        3.69030442e-06, 2.05921940e-07, 1.89663173e-04, 8.41574915e-04,\n",
       "        2.54449133e-04, 2.25701851e-04, 9.26521834e-04, 7.83674917e-04,\n",
       "        2.62741554e-04, 2.48882194e-04, 1.03678010e-04, 9.42502657e-05,\n",
       "        2.14061275e-07, 2.05517107e-07, 1.96904945e-05, 2.09331672e-04,\n",
       "        4.19308116e-05, 6.29548942e-05, 1.05779284e-04, 2.95333802e-04,\n",
       "        4.05821770e-05, 6.32040236e-05, 5.98746986e-05, 9.46448372e-05,\n",
       "        1.61675950e-05, 2.37958100e-05, 4.16229821e-05, 5.84642738e-05,\n",
       "        7.31497831e-05, 8.76089585e-05, 1.19318091e-04, 6.89286022e-05,\n",
       "        2.84394550e-05, 3.17415460e-05, 3.72719759e-05, 8.39465589e-05,\n",
       "        2.13844740e-07, 2.05550262e-07, 2.51922679e-05, 2.09066582e-04,\n",
       "        4.75510683e-05, 6.09427766e-05, 2.05138817e-04, 7.96095230e-05,\n",
       "        1.20895627e-05, 1.95414117e-05, 1.04670378e-04, 5.62598542e-05]),\n",
       " 'rank_test_neg_brier_score': array([298, 277, 158,  61, 296, 280, 154,  59, 297, 279, 156,  60,  15,\n",
       "        122,   3,   9,  16,  18,   4,  11,  14,  98,   5,   1, 231, 214,\n",
       "        118,  39, 234, 216, 121,  42, 233, 215, 120,  41, 331, 316, 134,\n",
       "         85, 335, 319, 131,  88, 334, 318, 132,  87, 302, 278, 161,  63,\n",
       "        295, 281, 155,  58, 300, 276, 159,  62, 169, 183,   8,   2,  43,\n",
       "         94,  10,  12,  99,  70,   6,   7, 228, 211, 115,  36, 232, 213,\n",
       "        119,  40, 230, 212, 117,  38, 332, 314, 136,  84, 333, 320, 133,\n",
       "         89, 330, 315, 135,  86, 304, 287, 165,  65, 294, 282, 157,  57,\n",
       "        303, 283, 162,  64, 217, 199, 102, 100, 184, 182, 125,  13, 196,\n",
       "        195, 181,  19, 225, 208, 112,  33, 229, 210, 116,  37, 227, 209,\n",
       "        114,  35, 336, 311, 139,  82, 329, 317, 137,  90, 328, 313, 138,\n",
       "         83, 306, 289, 166,  68, 293, 275, 160,  56, 305, 290, 167,  66,\n",
       "        256, 257, 178,  93, 249, 255, 177,  95, 252, 258, 176,  96, 222,\n",
       "        205, 109,  30, 226, 207, 113,  34, 224, 206, 111,  32, 325, 308,\n",
       "        142,  81, 326, 312, 140,  91, 327, 310, 141,  92, 267, 269, 172,\n",
       "         73, 285, 274, 163,  55, 286, 284, 164,  69, 273, 268, 171,  72,\n",
       "        322, 301, 174,  53, 321, 288, 175,  71, 219, 202, 106,  27, 223,\n",
       "        204, 110,  31, 221, 203, 108,  29, 291, 270, 146,  78, 324, 307,\n",
       "        143,  80, 323, 292, 144,  79, 244, 241, 152,  47, 265, 266, 168,\n",
       "         52, 261, 259, 173,  67, 243, 242, 130,  50, 299, 271, 151,  75,\n",
       "        263, 260, 150,  76, 197, 193, 103,  24, 220, 201, 107,  28, 218,\n",
       "        200, 105,  26, 246, 245, 129,  49, 309, 272, 147,  77, 264, 262,\n",
       "        145,  74, 190, 189, 126,  20, 247, 248, 170,  48, 237, 235, 153,\n",
       "         44, 191, 187, 124,  22, 250, 253, 149,  51, 236, 239, 127,  45,\n",
       "        180, 179,  97,  17, 198, 194, 104,  25, 186, 185, 101,  23, 192,\n",
       "        188, 123,  21, 251, 254, 148,  54, 238, 240, 128,  46])}"
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full results\n",
    "results_Grid_LR1_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__alpha': 1e-07, 'model__learning_rate': 'adaptive', 'model__penalty': 'l2', 'under': None}\n",
      "ROC-AUC: 0.692287233989569\n",
      "F1: 0.010763617553287516\n",
      "Accuracy: 0.8466936050409449\n",
      "Balanced Accuracy: 0.5021495121015661\n",
      "Loan Loss: -132231.0\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on AUC:\n",
    "ind = np.argmin(results_Grid_LR1_AUC['rank_test_roc_auc'])\n",
    "print('Parameters:', results_Grid_LR1_AUC['params'][ind])\n",
    "print('ROC-AUC:', results_Grid_LR1_AUC['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid_LR1_AUC['mean_test_f1'][ind])\n",
    "#print('Average Precision:', results_Grid_LR1_AUC['mean_test_average_precision'][ind])\n",
    "print('Accuracy:', results_Grid_LR1_AUC['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid_LR1_AUC['mean_test_balanced_accuracy'][ind])\n",
    "#print('Brier Score:', results_Grid_LR1_AUC['mean_test_neg_brier_score'][ind])\n",
    "print('Loan Loss:', results_Grid_LR1_AUC['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__alpha': 1e-07, 'model__learning_rate': 'adaptive', 'model__penalty': 'l2', 'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)}\n",
      "ROC-AUC: 0.6920828911182128\n",
      "F1: 0.32616400566065984\n",
      "Accuracy: 0.7729040862324874\n",
      "Balanced Accuracy: 0.6033114742725855\n",
      "Loan Loss: -115286.23333333334\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan loss:\n",
    "ind = np.argmin(results_Grid_LR1_AUC['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid_LR1_AUC['params'][ind])\n",
    "print('ROC-AUC:', results_Grid_LR1_AUC['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid_LR1_AUC['mean_test_f1'][ind])\n",
    "#print('Average Precision:', results_Grid_LR1_AUC['mean_test_average_precision'][ind])\n",
    "print('Accuracy:', results_Grid_LR1_AUC['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid_LR1_AUC['mean_test_balanced_accuracy'][ind])\n",
    "#print('Brier Score:', results_Grid_LR1_AUC['mean_test_neg_brier_score'][ind])\n",
    "print('Loan Loss:', results_Grid_LR1_AUC['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR: Undersampling and class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup learner.\n",
    "sgdLearner2 = SGDClassifier(loss='log', penalty='l2', n_jobs=-1, random_state=rs, learning_rate='adaptive', \n",
    "                            early_stopping=True, n_iter_no_change=20, eta0 = 0.001, alpha=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "steps = [('under', RandomUnderSampler()), ('model', sgdLearner2)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter values for gridsearch\n",
    "param_grid = [{'under': [None, \n",
    "                         RandomUnderSampler(sampling_strategy=0.6, random_state=rs),\n",
    "                         RandomUnderSampler(sampling_strategy=0.3, random_state=rs),\n",
    "                         RandomUnderSampler(sampling_strategy=0.4, random_state=rs),\n",
    "                         RandomUnderSampler(sampling_strategy=0.5, random_state=rs),\n",
    "                         RandomUnderSampler(sampling_strategy=0.7, random_state=rs),\n",
    "                         RandomUnderSampler(sampling_strategy=0.8, random_state=rs),\n",
    "                         RandomUnderSampler(sampling_strategy=0.9, random_state=rs)]},\n",
    "              {'under': [None],\n",
    "               'model__class_weight': [None, 'balanced']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'under': [None,\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.3),\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.4),\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.5),\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.7),\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.8),\n",
       "   RandomUnderSampler(random_state=0, sampling_strategy=0.9)]},\n",
       " {'under': [None], 'model__class_weight': [None, 'balanced']}]"
      ]
     },
     "execution_count": 1175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch object.\n",
    "gridLR3 = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoringDict2, cv = kFold, verbose=3,\n",
    "                                        refit='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('under', RandomUnderSampler()),\n",
       "                                       ('model',\n",
       "                                        SGDClassifier(alpha=1e-07,\n",
       "                                                      early_stopping=True,\n",
       "                                                      eta0=0.001,\n",
       "                                                      learning_rate='adaptive',\n",
       "                                                      loss='log',\n",
       "                                                      n_iter_no_change=20,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'under': [None,\n",
       "                                    RandomUnderSampler(random_state=0...\n",
       "                      'average_precision': make_scorer(average_precision_score, needs_proba=True),\n",
       "                      'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
       "                      'f1': make_scorer(f1_score),\n",
       "                      'loan_loss': make_scorer(calcCostRev, greater_is_better=False, needs_proba=True, cost=(1, 3.7), returnThreshold=False),\n",
       "                      'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True),\n",
       "                      'roc_auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 1177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the gridsearch.\n",
    "gridLR3.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning gridsearch results\n",
    "results_Grid2_LR_AUC = gridLR3.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'under': None}\n",
      "ROC-AUC: 0.6923093033781349\n",
      "F1: 0.010925930647811893\n",
      "Accuracy: 0.8466779487668094\n",
      "Balanced Accuracy: 0.502174520083923\n",
      "Loan Loss: -132226.56666666668\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on AUC:\n",
    "ind = np.argmin(results_Grid2_LR_AUC['rank_test_roc_auc'])\n",
    "print('Parameters:', results_Grid2_LR_AUC['params'][ind])\n",
    "print('ROC-AUC:', results_Grid2_LR_AUC['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid2_LR_AUC['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid2_LR_AUC['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid2_LR_AUC['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid2_LR_AUC['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'under': RandomUnderSampler(random_state=0, sampling_strategy=0.7)}\n",
      "ROC-AUC: 0.6920429621421148\n",
      "F1: 0.34547757741940766\n",
      "Accuracy: 0.7347596696015607\n",
      "Balanced Accuracy: 0.6210052795740183\n",
      "Loan Loss: -114709.26666666666\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan loss:\n",
    "ind = np.argmin(results_Grid2_LR_AUC['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid2_LR_AUC['params'][ind])\n",
    "print('ROC-AUC:', results_Grid2_LR_AUC['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid2_LR_AUC['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid2_LR_AUC['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid2_LR_AUC['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid2_LR_AUC['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR: eta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup learner.\n",
    "sgdLearner2 = SGDClassifier(loss='log', penalty='l2', n_jobs=-1, random_state=rs, learning_rate='adaptive', \n",
    "                            early_stopping=True, n_iter_no_change=20, eta0 = 0.001, alpha=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "steps = [('under', RandomUnderSampler(sampling_strategy=0.7, random_state=rs)), ('model', sgdLearner2)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eta0 values for gridsearch\n",
    "eta0 = [10**i for i in range(-5,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-05, 0.0001, 0.001, 0.01, 0.1]"
      ]
     },
     "execution_count": 1277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for gridsearch\n",
    "param_grid = [{'model__eta0': eta0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model__eta0': [1e-05, 0.0001, 0.001, 0.01, 0.1]}]"
      ]
     },
     "execution_count": 1426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch object\n",
    "gridLR3 = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoringDict2, cv = kFold, verbose=3,\n",
    "                                        refit='loan_loss', n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  15 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('under',\n",
       "                                        RandomUnderSampler(random_state=0,\n",
       "                                                           sampling_strategy=0.7)),\n",
       "                                       ('model',\n",
       "                                        SGDClassifier(alpha=1e-07,\n",
       "                                                      early_stopping=True,\n",
       "                                                      eta0=0.001,\n",
       "                                                      learning_rate='adaptive',\n",
       "                                                      loss='log',\n",
       "                                                      n_iter_no_change=20,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      random_state=0))]),\n",
       "             n_jobs=3,\n",
       "             param_grid=[{'model__eta0...\n",
       "                      'loan_loss3': make_scorer(calcCost, greater_is_better=False, needs_proba=True, cost=(1, 3), returnThreshold=False),\n",
       "                      'loan_loss4': make_scorer(calcCost, greater_is_better=False, needs_proba=True, cost=(1, 4), returnThreshold=False),\n",
       "                      'loan_loss5': make_scorer(calcCost, greater_is_better=False, needs_proba=True, cost=(1, 5), returnThreshold=False),\n",
       "                      'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True),\n",
       "                      'roc_auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 1428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the gridsearch.\n",
    "gridLR3.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning results of the gridsearch.\n",
    "results_Grid3_LR = gridLR3.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__eta0': 0.1, 'model__max_iter': 100}\n",
      "ROC-AUC: 0.6919465578589947\n",
      "F1: 0.34476554755730904\n",
      "Accuracy: 0.7381727391802123\n",
      "Balanced Accuracy: 0.6200329441390753\n",
      "Loan Loss: -114616.43333333335\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan loss:\n",
    "ind = np.argmin(results_Grid3_LR['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid3_LR['params'][ind])\n",
    "print('ROC-AUC:', results_Grid3_LR['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid3_LR['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid3_LR['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid3_LR['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid3_LR['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__eta0': 0.1}\n",
      "ROC-AUC: 0.6920234345868245\n",
      "F1: 0.34548723923662344\n",
      "Accuracy: 0.7347440132970385\n",
      "Balanced Accuracy: 0.621015064432354\n",
      "Loan Loss: -114708.43333333333\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan loss:\n",
    "ind = np.argmin(results_Grid3_LR['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid3_LR['params'][ind])\n",
    "print('ROC-AUC:', results_Grid3_LR['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid3_LR['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid3_LR['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid3_LR['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid3_LR['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup learner.\n",
    "sgdLogLearnerFinal = SGDClassifier(loss='log', penalty='l2', n_jobs=-1, random_state=rs, learning_rate='adaptive', \n",
    "                            early_stopping=True, n_iter_no_change=20, eta0 = 0.1, alpha=0.0000001, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "stepsLog = [('under', RandomUnderSampler(sampling_strategy=0.7, random_state=rs)), ('model', sgdLogLearnerFinal)]\n",
    "pipelineLog = Pipeline(steps=stepsLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('under',\n",
       "                 RandomUnderSampler(random_state=0, sampling_strategy=0.7)),\n",
       "                ('model',\n",
       "                 SGDClassifier(alpha=1e-07, early_stopping=True, eta0=0.1,\n",
       "                               learning_rate='adaptive', loss='log',\n",
       "                               n_iter_no_change=20, n_jobs=-1,\n",
       "                               random_state=0))])"
      ]
     },
     "execution_count": 1442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model using pipeline.\n",
    "pipelineLog.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model_LR_Final = pipelineLog[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15321994, -0.10608688, -0.06394024, -0.00440388, -0.01858805,\n",
       "        0.12388084, -0.1041584 ,  0.05336857,  0.10666981,  0.02358108,\n",
       "       -0.03570565, -0.06334034, -0.06419003,  0.06665411,  0.02254944,\n",
       "       -0.07983814,  0.02864404, -0.00344223,  0.02030868, -0.10533221,\n",
       "        0.15146144, -0.20662868,  0.27996585, -0.1527876 , -0.21482195,\n",
       "       -0.12353167,  0.77843557, -0.05847721, -0.11239988, -0.13101134,\n",
       "        0.03851715, -0.01313884, -0.05991154,  0.21078365,  0.42938306,\n",
       "       -0.10665671, -0.41104552, -0.02044424, -0.03827678, -0.06128765,\n",
       "       -0.06166908, -0.06603733, -0.02148198, -0.04000481, -0.04038828,\n",
       "       -0.01464797, -0.0043164 ,  0.0070882 ,  0.4348035 ,  0.        ,\n",
       "       -0.31792537,  0.7362217 ,  0.0321388 , -0.23766325, -0.13943471,\n",
       "       -0.01546291,  0.08000521,  0.00879487, -0.52395059, -0.18281953,\n",
       "        0.04831497,  0.14621056,  0.20448175,  0.18150174,  0.19959828])"
      ]
     },
     "execution_count": 1444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients of the model\n",
    "model_LR_Final.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain predictions\n",
    "y_prob = pipelineLog.predict_proba(dfPostImpute[testAll])\n",
    "y_hat = pipelineLog.predict(dfPostImpute[testAll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[236579,  66464],\n",
       "       [ 33070,  29592]], dtype=int64)"
      ]
     },
     "execution_count": 1446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(y_true=testLabels, y_pred=y_hat, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision(+)</th>\n",
       "      <th>Recall(+)</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Error Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Result</th>\n",
       "      <td>0.72783</td>\n",
       "      <td>0.30807</td>\n",
       "      <td>0.472248</td>\n",
       "      <td>0.696921</td>\n",
       "      <td>231814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision(+)  Recall(+)       AUC  Error Cost\n",
       "Result   0.72783       0.30807   0.472248  0.696921      231814"
      ]
     },
     "execution_count": 1447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scores for the model\n",
    "getScores(testLabels, y_hat, y_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the predictions into a dictionary object.\n",
    "resultsLR = {'id': dfFinal[testAll].id.values, 'true_class': testLabels, 'predict_class_LR': y_hat, \n",
    "                 'prob_LR':y_prob[:,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from dictionary results object.\n",
    "df_Results_LR = pd.DataFrame(resultsLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_class</th>\n",
       "      <th>predict_class_LR</th>\n",
       "      <th>prob_LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73713571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75051478</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.450985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75092979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75123593</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.223180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75358919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  true_class  predict_class_LR   prob_LR\n",
       "0  73713571           0                 0  0.377908\n",
       "1  75051478           0                 0  0.450985\n",
       "2  75092979           0                 0  0.360520\n",
       "3  75123593           0                 0  0.223180\n",
       "5  75358919           0                 0  0.393617"
      ]
     },
     "execution_count": 1452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview of dataframe with the results.\n",
    "df_Results_LR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results to a file.\n",
    "df_Results_LR.to_csv('results_LR3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Grid Search #1: Undersampling, and max_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest learner to be used in first gridsearch.\n",
    "rfLearn1 = RandomForestClassifier(n_estimators=50, max_features='auto', n_jobs=-1, random_state=rs, \n",
    "                             verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "steps = [('under', RandomUnderSampler()), ('model', rfLearn1)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to in gridsearch.\n",
    "param_grid = [{'under': [None,\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.2),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.6),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=1.0)],\n",
    "               'model__max_samples': [0.05, 0.1, 0.2, 0.4, 0.6, 0.8, None]},\n",
    "              \n",
    "              {'under': [None],\n",
    "               'model__class_weight': ['balanced', None], \n",
    "              'model__max_samples': [0.05, 0.1, 0.2, 0.4, 0.6, 0.8, None]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch object\n",
    "gridRF1 = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoringDict2, cv = kFold, verbose=3,\n",
    "                                        refit='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed: 66.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50building tree 2 of 50building tree 3 of 50building tree 4 of 50\n",
      "\n",
      "\n",
      "\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   12.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   27.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('under', RandomUnderSampler()),\n",
       "                                       ('model',\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               n_jobs=-1,\n",
       "                                                               random_state=0,\n",
       "                                                               verbose=3))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'model__max_samples': [0.05, 0.1, 0.2, 0.4, 0.6, 0.8,\n",
       "                                                 None],\n",
       "                          'under': [None,\n",
       "                                    RandomUnderSampler(random_state=0,\n",
       "                                                       sampling_s...\n",
       "                      'average_precision': make_scorer(average_precision_score, needs_proba=True),\n",
       "                      'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
       "                      'f1': make_scorer(f1_score),\n",
       "                      'loan_loss': make_scorer(calcCostRev, greater_is_better=False, needs_proba=True, cost=(1, 3.7), returnThreshold=False),\n",
       "                      'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True),\n",
       "                      'roc_auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 1224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using the gridsearch object.\n",
    "gridRF1.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the results of the gridsearch\n",
    "results_Grid1_RF = gridRF1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__max_samples': 0.6, 'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)}\n",
      "ROC-AUC: 0.6797368138908159\n",
      "F1: 0.31358887884145753\n",
      "Accuracy: 0.767412995867983\n",
      "Balanced Accuracy: 0.5951750965779591\n",
      "Loan Loss: -117247.46666666667\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on roc_auc:\n",
    "ind = np.argmin(results_Grid1_RF['rank_test_roc_auc'])\n",
    "print('Parameters:', results_Grid1_RF['params'][ind])\n",
    "print('ROC-AUC:', results_Grid1_RF['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid1_RF['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid1_RF['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid1_RF['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid1_RF['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__max_samples': 0.6, 'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)}\n",
      "ROC-AUC: 0.6797368138908159\n",
      "F1: 0.31358887884145753\n",
      "Accuracy: 0.767412995867983\n",
      "Balanced Accuracy: 0.5951750965779591\n",
      "Loan Loss: -117247.46666666667\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan loss:\n",
    "ind = np.argmin(results_Grid1_RF['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid1_RF['params'][ind])\n",
    "print('ROC-AUC:', results_Grid1_RF['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid1_RF['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid1_RF['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid1_RF['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid1_RF['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 21.6316282 ,  25.25661159,  12.75637404,   9.12195404,\n",
       "         36.48321478,  38.64590001,  18.67038576,  14.04448549,\n",
       "         80.54567607,  71.25994007,  30.87309361,  20.69643219,\n",
       "        132.5734818 , 119.71157749,  48.87266763,  31.08212241,\n",
       "        180.00739487, 163.59950662,  64.48137744,  42.7154301 ,\n",
       "        223.34902891, 212.04337263,  82.12765145,  58.19690569,\n",
       "        293.15650773, 266.17719618,  97.89947319,  60.58461833,\n",
       "         26.03538116,  37.87727896,  72.4190553 , 131.34054263,\n",
       "        187.92892154, 236.84856621, 267.27989697,  30.51621064,\n",
       "         43.5537742 ,  77.73167928, 138.27381333, 187.58780217,\n",
       "        240.37074272, 231.64949584]),\n",
       " 'std_fit_time': array([ 0.40164966,  2.46533821,  2.10967126,  0.52945885,  1.32065555,\n",
       "         3.3172217 ,  1.78665683,  0.84004002,  0.92958355, 10.04814206,\n",
       "         2.28060911,  1.08449482,  0.82412844,  7.27585925,  3.42088033,\n",
       "         0.76241183,  3.04395774,  7.02347749,  4.19947322,  1.17879327,\n",
       "         1.22038233,  1.90773929,  2.01133521,  2.25113877,  2.18307432,\n",
       "         9.13975749,  4.65325632,  6.22533375,  2.26398046,  0.15712359,\n",
       "         1.73099971,  1.13432887,  2.22949846,  1.04774974,  3.87896245,\n",
       "         2.09992408,  0.71286176,  1.68288347,  1.23572754,  2.34486432,\n",
       "         2.68446983, 34.21802104]),\n",
       " 'mean_score_time': array([14.14622315, 11.48160251, 11.59806101, 10.77753711, 14.91664569,\n",
       "        12.61771925, 15.35281897, 14.11800528, 18.7176377 , 17.24134819,\n",
       "        17.12891022, 17.38036331, 20.30357059, 20.05534228, 17.72607088,\n",
       "        18.32590795, 23.41950949, 21.53434086, 20.61668571, 19.18913841,\n",
       "        24.8925337 , 24.65498098, 24.75123795, 22.78839604, 29.35040092,\n",
       "        24.3475217 , 24.33173116, 19.23362629, 15.38905636, 16.87276038,\n",
       "        21.66694649, 25.14397264, 28.43642934, 30.37083626, 27.14693475,\n",
       "        19.22942909, 19.15578278, 22.97922087, 26.72259887, 28.45087179,\n",
       "        29.44867333, 14.39144746]),\n",
       " 'std_score_time': array([0.68208937, 0.96043714, 1.60694336, 0.74447383, 1.75429509,\n",
       "        2.08565979, 0.2636553 , 0.73730371, 0.47907586, 2.53899037,\n",
       "        1.98665805, 2.55490409, 2.79062569, 0.72877247, 3.45695325,\n",
       "        2.22952545, 0.45959742, 0.98628598, 1.94834906, 1.34988654,\n",
       "        0.93377246, 0.78193452, 1.80739215, 1.76457235, 0.50482922,\n",
       "        3.53238576, 3.99309913, 1.14547832, 1.28828979, 0.76512913,\n",
       "        1.12748574, 0.68995898, 0.18312582, 0.49749139, 2.27468396,\n",
       "        0.62084083, 0.29681705, 1.50150139, 0.83514667, 1.27902414,\n",
       "        1.28591812, 6.04056966]),\n",
       " 'param_model__max_samples': masked_array(data=[0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6, 0.8,\n",
       "                    0.8, 0.8, 0.8, None, None, None, None, 0.05, 0.1, 0.2,\n",
       "                    0.4, 0.6, 0.8, None, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8,\n",
       "                    None],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_under': masked_array(data=[None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None,\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=0.6),\n",
       "                    RandomUnderSampler(random_state=0, sampling_strategy=1.0),\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__class_weight': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', None, None, None,\n",
       "                    None, None, None, None],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__max_samples': 0.05, 'under': None},\n",
       "  {'model__max_samples': 0.05,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': 0.05,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': 0.05,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__max_samples': 0.1, 'under': None},\n",
       "  {'model__max_samples': 0.1,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': 0.1,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': 0.1,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__max_samples': 0.2, 'under': None},\n",
       "  {'model__max_samples': 0.2,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': 0.2,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': 0.2,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__max_samples': 0.4, 'under': None},\n",
       "  {'model__max_samples': 0.4,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': 0.4,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': 0.4,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__max_samples': 0.6, 'under': None},\n",
       "  {'model__max_samples': 0.6,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': 0.6,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': 0.6,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__max_samples': 0.8, 'under': None},\n",
       "  {'model__max_samples': 0.8,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': 0.8,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': 0.8,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__max_samples': None, 'under': None},\n",
       "  {'model__max_samples': None,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.2)},\n",
       "  {'model__max_samples': None,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=0.6)},\n",
       "  {'model__max_samples': None,\n",
       "   'under': RandomUnderSampler(random_state=0, sampling_strategy=1.0)},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': 0.05,\n",
       "   'under': None},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': 0.1,\n",
       "   'under': None},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': 0.2,\n",
       "   'under': None},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': 0.4,\n",
       "   'under': None},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': 0.6,\n",
       "   'under': None},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': 0.8,\n",
       "   'under': None},\n",
       "  {'model__class_weight': 'balanced',\n",
       "   'model__max_samples': None,\n",
       "   'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': 0.05, 'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': 0.1, 'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': 0.2, 'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': 0.4, 'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': 0.6, 'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': 0.8, 'under': None},\n",
       "  {'model__class_weight': None, 'model__max_samples': None, 'under': None}],\n",
       " 'split0_test_accuracy': array([0.84673652, 0.84657   , 0.76888359, 0.62582089, 0.84672798,\n",
       "        0.84629672, 0.77066841, 0.62512062, 0.84659562, 0.84632661,\n",
       "        0.77016029, 0.62675172, 0.8465273 , 0.84624975, 0.76943868,\n",
       "        0.626329  , 0.84655719, 0.84610885, 0.76860605, 0.62639305,\n",
       "        0.84667674, 0.84630953, 0.76845234, 0.6261881 , 0.84654438,\n",
       "        0.84613874, 0.76830716, 0.62638878, 0.8466127 , 0.84663831,\n",
       "        0.84646325, 0.84662124, 0.84660416, 0.84669809, 0.84649741,\n",
       "        0.84673652, 0.84672798, 0.84659562, 0.8465273 , 0.84655719,\n",
       "        0.84667674, 0.84654438]),\n",
       " 'split1_test_accuracy': array([0.84670236, 0.84632234, 0.7670817 , 0.62453992, 0.84640774,\n",
       "        0.8460448 , 0.76755993, 0.62690544, 0.84651876, 0.84600637,\n",
       "        0.76688102, 0.62791313, 0.84638639, 0.84608323, 0.76666752,\n",
       "        0.62787897, 0.84648033, 0.84606188, 0.76721407, 0.62515051,\n",
       "        0.84640347, 0.84584838, 0.76665044, 0.62660228, 0.84642482,\n",
       "        0.84590816, 0.76646684, 0.62814371, 0.84658708, 0.84658708,\n",
       "        0.84659562, 0.8465273 , 0.84657427, 0.84649314, 0.84641628,\n",
       "        0.84670236, 0.84640774, 0.84651876, 0.84638639, 0.84648033,\n",
       "        0.84640347, 0.84642482]),\n",
       " 'split2_test_accuracy': array([0.84653945, 0.84627045, 0.76968108, 0.62467068, 0.84648394,\n",
       "        0.84646686, 0.76852393, 0.62691666, 0.84649248, 0.8462064 ,\n",
       "        0.76741803, 0.62832573, 0.84640281, 0.84608684, 0.7665427 ,\n",
       "        0.62787739, 0.84655226, 0.84627045, 0.76641887, 0.62758703,\n",
       "        0.84631742, 0.84645832, 0.76700385, 0.62898329, 0.84657361,\n",
       "        0.84621494, 0.76568872, 0.62870148, 0.84657361, 0.84663766,\n",
       "        0.84655653, 0.84663766, 0.84661204, 0.84647967, 0.8464754 ,\n",
       "        0.84653945, 0.84648394, 0.84649248, 0.84640281, 0.84655226,\n",
       "        0.84631742, 0.84657361]),\n",
       " 'mean_test_accuracy': array([0.84665945, 0.8463876 , 0.76854879, 0.6250105 , 0.84653989,\n",
       "        0.84626946, 0.76891742, 0.62631424, 0.84653562, 0.84617979,\n",
       "        0.76815311, 0.62766353, 0.84643883, 0.84613994, 0.76754963,\n",
       "        0.62736179, 0.84652993, 0.84614706, 0.767413  , 0.62637687,\n",
       "        0.84646588, 0.84620541, 0.76736887, 0.62725789, 0.84651427,\n",
       "        0.84608728, 0.7668209 , 0.62774466, 0.84659113, 0.84662102,\n",
       "        0.84653847, 0.8465954 , 0.84659682, 0.84655697, 0.84646303,\n",
       "        0.84665945, 0.84653989, 0.84653562, 0.84643883, 0.84652993,\n",
       "        0.84646588, 0.84651427]),\n",
       " 'std_test_accuracy': array([8.59868007e-05, 1.30705325e-04, 1.08727922e-03, 5.75513729e-04,\n",
       "        1.36592195e-04, 1.73381686e-04, 1.29917627e-03, 8.44025863e-04,\n",
       "        4.37593284e-05, 1.32084755e-04, 1.43612309e-03, 6.66382587e-04,\n",
       "        6.29111331e-05, 7.76634806e-05, 1.33673000e-03, 7.30288894e-04,\n",
       "        3.51279632e-05, 8.93315766e-05, 9.03924980e-04, 9.94770057e-04,\n",
       "        1.53188120e-04, 2.59662549e-04, 7.79589339e-04, 1.23170571e-03,\n",
       "        6.43663867e-05, 1.30419113e-04, 1.09790251e-03, 9.85417674e-04,\n",
       "        1.62111798e-05, 2.40012942e-05, 5.55278811e-05, 4.86185726e-05,\n",
       "        1.62700054e-05, 9.99416816e-05, 3.42563602e-05, 8.59868007e-05,\n",
       "        1.36592195e-04, 4.37593284e-05, 6.29111331e-05, 3.51279632e-05,\n",
       "        1.53188120e-04, 6.43663867e-05]),\n",
       " 'rank_test_accuracy': array([ 1, 22, 30, 42,  8, 23, 29, 41, 11, 25, 31, 37, 20, 27, 32, 38, 13,\n",
       "        26, 33, 40, 17, 24, 34, 39, 15, 28, 35, 36,  6,  3, 10,  5,  4,  7,\n",
       "        19,  1,  8, 11, 20, 13, 17, 15]),\n",
       " 'split0_test_balanced_accuracy': array([0.50250202, 0.50392221, 0.58922717, 0.62719728, 0.50251981,\n",
       "        0.50401204, 0.59274714, 0.62850784, 0.50280701, 0.50446354,\n",
       "        0.59536997, 0.62888862, 0.50318912, 0.50528588, 0.59267185,\n",
       "        0.63004336, 0.50328669, 0.5052141 , 0.59415544, 0.62936189,\n",
       "        0.50371122, 0.50590346, 0.5932883 , 0.62885268, 0.50361023,\n",
       "        0.50607663, 0.59442424, 0.62808062, 0.50126434, 0.50147356,\n",
       "        0.50132453, 0.50198868, 0.50221835, 0.502525  , 0.50208682,\n",
       "        0.50250202, 0.50251981, 0.50280701, 0.50318912, 0.50328669,\n",
       "        0.50371122, 0.50361023]),\n",
       " 'split1_test_balanced_accuracy': array([0.50229917, 0.50408425, 0.5937806 , 0.6297634 , 0.50217089,\n",
       "        0.50344085, 0.59288698, 0.62999552, 0.50248762, 0.5041032 ,\n",
       "        0.59567156, 0.62845546, 0.50288899, 0.50469661, 0.59384432,\n",
       "        0.62990813, 0.50301296, 0.50494661, 0.59629065, 0.62803454,\n",
       "        0.50317309, 0.50499181, 0.59642598, 0.6292114 , 0.50334554,\n",
       "        0.50502711, 0.59628332, 0.63028136, 0.50103228, 0.50121496,\n",
       "        0.50142552, 0.50173912, 0.50191527, 0.50195871, 0.50227869,\n",
       "        0.50229917, 0.50217089, 0.50248762, 0.50288899, 0.50301296,\n",
       "        0.50317309, 0.50334554]),\n",
       " 'split2_test_balanced_accuracy': array([0.50210061, 0.50384848, 0.59350021, 0.62840202, 0.5022391 ,\n",
       "        0.50410146, 0.59345636, 0.63201159, 0.50280359, 0.50464413,\n",
       "        0.59565772, 0.63041167, 0.50286482, 0.50471054, 0.59419326,\n",
       "        0.63085483, 0.50391212, 0.505641  , 0.5950792 , 0.62821725,\n",
       "        0.50327109, 0.50604878, 0.5949679 , 0.6304574 , 0.50382197,\n",
       "        0.50577948, 0.59629219, 0.63005124, 0.50108181, 0.50181608,\n",
       "        0.50164259, 0.5022271 , 0.50215489, 0.502385  , 0.50218839,\n",
       "        0.50210061, 0.5022391 , 0.50280359, 0.50286482, 0.50391212,\n",
       "        0.50327109, 0.50382197]),\n",
       " 'mean_test_balanced_accuracy': array([0.5023006 , 0.50395164, 0.59216933, 0.62845423, 0.50230993,\n",
       "        0.50385145, 0.59303016, 0.63017165, 0.50269941, 0.50440362,\n",
       "        0.59556642, 0.62925191, 0.50298098, 0.50489768, 0.59356981,\n",
       "        0.63026877, 0.50340392, 0.50526724, 0.5951751 , 0.62853789,\n",
       "        0.50338513, 0.50564802, 0.59489406, 0.62950716, 0.50359258,\n",
       "        0.50562774, 0.59566658, 0.62947108, 0.50112614, 0.50150153,\n",
       "        0.50146421, 0.50198496, 0.50209617, 0.50228957, 0.50218463,\n",
       "        0.5023006 , 0.50230993, 0.50269941, 0.50298098, 0.50340392,\n",
       "        0.50338513, 0.50359258]),\n",
       " 'std_test_balanced_accuracy': array([1.63877129e-04, 9.84786772e-05, 2.08356508e-03, 1.04826342e-03,\n",
       "        1.50997584e-04, 2.92622795e-04, 3.06728393e-04, 1.43581143e-03,\n",
       "        1.49764250e-04, 2.24860051e-04, 1.39025568e-04, 8.38918742e-04,\n",
       "        1.47508670e-04, 2.74559455e-04, 6.50736321e-04, 4.18063980e-04,\n",
       "        3.76322039e-04, 2.85963599e-04, 8.74330685e-04, 5.87406883e-04,\n",
       "        2.34020386e-04, 4.67784840e-04, 1.28201561e-03, 6.87692937e-04,\n",
       "        1.94898913e-04, 4.41698434e-04, 8.78476767e-04, 9.87677468e-04,\n",
       "        9.97894566e-05, 2.46201138e-04, 1.32698991e-04, 1.99237245e-04,\n",
       "        1.30512707e-04, 2.40833680e-04, 7.83728715e-05, 1.63877129e-04,\n",
       "        1.50997584e-04, 1.49764250e-04, 1.47508670e-04, 3.76322039e-04,\n",
       "        2.34020386e-04, 1.94898913e-04]),\n",
       " 'rank_test_balanced_accuracy': array([34, 20, 14,  7, 32, 21, 13,  2, 30, 19,  9,  5, 28, 18, 12,  1, 24,\n",
       "        17, 10,  6, 26, 15, 11,  3, 22, 16,  8,  4, 42, 40, 41, 39, 38, 36,\n",
       "        37, 34, 32, 30, 28, 24, 26, 22]),\n",
       " 'split0_test_f1': array([0.01232733, 0.01948318, 0.30445007, 0.33998132, 0.01243535,\n",
       "        0.02062304, 0.30994565, 0.34105886, 0.0141591 , 0.02264827,\n",
       "        0.31397838, 0.34153397, 0.01614978, 0.02665297, 0.30981019,\n",
       "        0.34249694, 0.01652983, 0.02668179, 0.31207474, 0.34191248,\n",
       "        0.01820966, 0.02934038, 0.31074279, 0.34145241, 0.01808694,\n",
       "        0.03056228, 0.31247783, 0.34079934, 0.00674648, 0.00768062,\n",
       "        0.0074528 , 0.01019537, 0.01133831, 0.0125416 , 0.01100413,\n",
       "        0.01232733, 0.01243535, 0.0141591 , 0.01614978, 0.01652983,\n",
       "        0.01820966, 0.01808694]),\n",
       " 'split1_test_f1': array([0.01145438, 0.02089284, 0.31146734, 0.34209227, 0.01164995,\n",
       "        0.01861731, 0.3101119 , 0.34250843, 0.01285255, 0.02180694,\n",
       "        0.31432734, 0.3412606 , 0.0151117 , 0.02435922, 0.31155513,\n",
       "        0.34252218, 0.01544444, 0.02556895, 0.31527254, 0.34065117,\n",
       "        0.01640599, 0.02632289, 0.31545457, 0.34180083, 0.01716081,\n",
       "        0.02633283, 0.31523331, 0.34287094, 0.00570084, 0.00658058,\n",
       "        0.00756885, 0.00926156, 0.0099741 , 0.01040491, 0.01213919,\n",
       "        0.01145438, 0.01164995, 0.01285255, 0.0151117 , 0.01544444,\n",
       "        0.01640599, 0.01716081]),\n",
       " 'split2_test_f1': array([0.01095272, 0.0199265 , 0.31109351, 0.34092869, 0.01176438,\n",
       "        0.02059216, 0.31100266, 0.34425307, 0.01442005, 0.02379662,\n",
       "        0.31432528, 0.34300206, 0.01495153, 0.02441269, 0.31208244,\n",
       "        0.34334454, 0.01948105, 0.02823288, 0.31341935, 0.34102483,\n",
       "        0.01709542, 0.02963003, 0.31326848, 0.34310338, 0.01900186,\n",
       "        0.02900895, 0.31521807, 0.34272368, 0.00597543, 0.00932285,\n",
       "        0.00871676, 0.01128637, 0.01101231, 0.01246979, 0.01154639,\n",
       "        0.01095272, 0.01176438, 0.01442005, 0.01495153, 0.01948105,\n",
       "        0.01709542, 0.01900186]),\n",
       " 'mean_test_f1': array([0.01157814, 0.02010084, 0.30900364, 0.34100076, 0.01194989,\n",
       "        0.01994417, 0.3103534 , 0.34260678, 0.01381057, 0.02275061,\n",
       "        0.31421033, 0.34193221, 0.01540434, 0.02514163, 0.31114925,\n",
       "        0.34278788, 0.01715177, 0.02682787, 0.31358888, 0.34119616,\n",
       "        0.01723702, 0.0284311 , 0.31315528, 0.34211887, 0.0180832 ,\n",
       "        0.02863469, 0.31430974, 0.34213132, 0.00614092, 0.00786135,\n",
       "        0.0079128 , 0.01024776, 0.0107749 , 0.01180543, 0.01156324,\n",
       "        0.01157814, 0.01194989, 0.01381057, 0.01540434, 0.01715177,\n",
       "        0.01723702, 0.0180832 ]),\n",
       " 'std_test_f1': array([0.00056797, 0.00058855, 0.00322348, 0.0008633 , 0.00034643,\n",
       "        0.00093832, 0.00046408, 0.00130588, 0.00068574, 0.0008155 ,\n",
       "        0.00016401, 0.00076469, 0.00053115, 0.00106891, 0.00097102,\n",
       "        0.00039375, 0.00170561, 0.00109244, 0.00131099, 0.00052899,\n",
       "        0.00074312, 0.00149541, 0.00192524, 0.00071054, 0.00075161,\n",
       "        0.00174683, 0.00129537, 0.00094377, 0.00044263, 0.0011268 ,\n",
       "        0.00057045, 0.00082746, 0.00058168, 0.00099075, 0.00046354,\n",
       "        0.00056797, 0.00034643, 0.00068574, 0.00053115, 0.00170561,\n",
       "        0.00074312, 0.00075161]),\n",
       " 'rank_test_f1': array([35, 20, 14,  7, 32, 21, 13,  2, 30, 19,  9,  5, 28, 18, 12,  1, 26,\n",
       "        17, 10,  6, 24, 16, 11,  4, 22, 15,  8,  3, 42, 41, 40, 39, 38, 34,\n",
       "        37, 35, 32, 30, 28, 26, 24, 22]),\n",
       " 'split0_test_average_precision': array([0.25261831, 0.25388509, 0.25594588, 0.25412292, 0.25284004,\n",
       "        0.2534716 , 0.2585914 , 0.25657753, 0.25649243, 0.25628797,\n",
       "        0.26017211, 0.25744952, 0.2551566 , 0.25704264, 0.25888451,\n",
       "        0.25805026, 0.25572822, 0.25682734, 0.2612837 , 0.257954  ,\n",
       "        0.25429907, 0.25774717, 0.2592404 , 0.257172  , 0.25509782,\n",
       "        0.25692553, 0.26035024, 0.25726543, 0.24864909, 0.25155255,\n",
       "        0.25101085, 0.25444982, 0.25331208, 0.25345081, 0.25167586,\n",
       "        0.25261831, 0.25284004, 0.25649243, 0.2551566 , 0.25572822,\n",
       "        0.25429907, 0.25509782]),\n",
       " 'split1_test_average_precision': array([0.25033817, 0.25422095, 0.25791838, 0.25398514, 0.25116945,\n",
       "        0.25220749, 0.25826127, 0.25633024, 0.25266677, 0.25453647,\n",
       "        0.25953729, 0.25786148, 0.25381567, 0.25617542, 0.26002552,\n",
       "        0.25770742, 0.25398055, 0.25580383, 0.25968989, 0.25674635,\n",
       "        0.25217182, 0.25456117, 0.26016898, 0.2577059 , 0.25463933,\n",
       "        0.25495283, 0.26053258, 0.25749625, 0.24930865, 0.24890609,\n",
       "        0.25012312, 0.25047289, 0.2523215 , 0.25194344, 0.25195124,\n",
       "        0.25033817, 0.25116945, 0.25266677, 0.25381567, 0.25398055,\n",
       "        0.25217182, 0.25463933]),\n",
       " 'split2_test_average_precision': array([0.25135519, 0.25320801, 0.25798606, 0.25418692, 0.25322358,\n",
       "        0.25301291, 0.25836356, 0.25742631, 0.25500561, 0.255494  ,\n",
       "        0.26003687, 0.25695173, 0.25561053, 0.25673753, 0.26003038,\n",
       "        0.2575434 , 0.2558691 , 0.25724792, 0.25926026, 0.2570307 ,\n",
       "        0.25421308, 0.25590724, 0.25889065, 0.25866639, 0.2543863 ,\n",
       "        0.2577469 , 0.26031728, 0.25778774, 0.24855174, 0.25079351,\n",
       "        0.25251358, 0.25260501, 0.25310088, 0.25254836, 0.25248408,\n",
       "        0.25135519, 0.25322358, 0.25500561, 0.25561053, 0.2558691 ,\n",
       "        0.25421308, 0.2543863 ]),\n",
       " 'mean_test_average_precision': array([0.25143722, 0.25377135, 0.25728344, 0.25409833, 0.25241102,\n",
       "        0.25289733, 0.25840541, 0.25677803, 0.2547216 , 0.25543948,\n",
       "        0.25991542, 0.25742091, 0.25486094, 0.25665186, 0.2596468 ,\n",
       "        0.25776702, 0.25519262, 0.25662637, 0.26007795, 0.25724368,\n",
       "        0.25356132, 0.25607186, 0.25943334, 0.2578481 , 0.25470782,\n",
       "        0.25654175, 0.26040003, 0.25751647, 0.24883649, 0.25041738,\n",
       "        0.25121585, 0.25250924, 0.25291149, 0.25264754, 0.25203706,\n",
       "        0.25143722, 0.25241102, 0.2547216 , 0.25486094, 0.25519262,\n",
       "        0.25356132, 0.25470782]),\n",
       " 'std_test_average_precision': array([9.32668232e-04, 4.21281040e-04, 9.46197709e-04, 8.41916212e-05,\n",
       "        8.91781419e-04, 5.22500348e-04, 1.37982902e-04, 4.69390922e-04,\n",
       "        1.57467880e-03, 7.16086371e-04, 2.73022170e-04, 3.71952916e-04,\n",
       "        7.61989791e-04, 3.59186328e-04, 5.39026914e-04, 2.11172066e-04,\n",
       "        8.58994864e-04, 6.06433431e-04, 8.70449725e-04, 5.15508210e-04,\n",
       "        9.83155545e-04, 1.30587805e-03, 5.39415986e-04, 6.18311983e-04,\n",
       "        2.94484131e-04, 1.17250941e-03, 9.46875193e-05, 2.13711817e-04,\n",
       "        3.36223156e-04, 1.11266373e-03, 9.86608699e-04, 1.62498336e-03,\n",
       "        4.26002978e-04, 6.19363132e-04, 3.35487670e-04, 9.32668232e-04,\n",
       "        8.91781419e-04, 1.57467880e-03, 7.61989791e-04, 8.58994864e-04,\n",
       "        9.83155545e-04, 2.94484131e-04]),\n",
       " 'rank_test_average_precision': array([38, 28, 11, 27, 35, 32,  6, 13, 23, 18,  3, 10, 21, 14,  4,  8, 19,\n",
       "        15,  2, 12, 29, 17,  5,  7, 25, 16,  1,  9, 42, 41, 40, 34, 31, 33,\n",
       "        37, 38, 35, 23, 21, 19, 29, 25]),\n",
       " 'split0_test_roc_auc': array([0.66840335, 0.66945631, 0.67683517, 0.67641451, 0.66837819,\n",
       "        0.66928717, 0.67813989, 0.67778838, 0.6702129 , 0.67197121,\n",
       "        0.67985454, 0.67905938, 0.67020618, 0.67183827, 0.67915945,\n",
       "        0.67966985, 0.67011185, 0.67281207, 0.67932992, 0.67869601,\n",
       "        0.66983091, 0.67108239, 0.678637  , 0.67872784, 0.67037015,\n",
       "        0.67179808, 0.67876172, 0.67857124, 0.66651618, 0.66804575,\n",
       "        0.66905284, 0.66935518, 0.66798759, 0.66857415, 0.6672345 ,\n",
       "        0.66840335, 0.66837819, 0.6702129 , 0.67020618, 0.67011185,\n",
       "        0.66983091, 0.67037015]),\n",
       " 'split1_test_roc_auc': array([0.66590822, 0.67003027, 0.67700395, 0.67610263, 0.66730248,\n",
       "        0.66813061, 0.67735924, 0.6780803 , 0.66821729, 0.66964854,\n",
       "        0.67909782, 0.67829348, 0.66973   , 0.67135628, 0.67819937,\n",
       "        0.67825223, 0.668943  , 0.67048616, 0.67956223, 0.67680838,\n",
       "        0.66736415, 0.66970171, 0.67904675, 0.67820236, 0.66933799,\n",
       "        0.67030392, 0.67886771, 0.67928058, 0.66616592, 0.66618108,\n",
       "        0.66716767, 0.66736792, 0.66756221, 0.66808281, 0.66824113,\n",
       "        0.66590822, 0.66730248, 0.66821729, 0.66973   , 0.668943  ,\n",
       "        0.66736415, 0.66933799]),\n",
       " 'split2_test_roc_auc': array([0.66704318, 0.66932785, 0.67810537, 0.67778658, 0.66960065,\n",
       "        0.66902146, 0.67961151, 0.68013833, 0.66973322, 0.67083185,\n",
       "        0.68021005, 0.67958262, 0.67141994, 0.6729407 , 0.68052606,\n",
       "        0.68017508, 0.67027583, 0.67238749, 0.68031829, 0.67845946,\n",
       "        0.67011238, 0.67065225, 0.67941745, 0.68016793, 0.66877012,\n",
       "        0.67193095, 0.68051939, 0.67900114, 0.66702621, 0.6678843 ,\n",
       "        0.66921014, 0.66981926, 0.66970975, 0.66959509, 0.66903423,\n",
       "        0.66704318, 0.66960065, 0.66973322, 0.67141994, 0.67027583,\n",
       "        0.67011238, 0.66877012]),\n",
       " 'mean_test_roc_auc': array([0.66711825, 0.66960481, 0.67731483, 0.6767679 , 0.66842711,\n",
       "        0.66881308, 0.67837021, 0.67866901, 0.6693878 , 0.6708172 ,\n",
       "        0.6797208 , 0.67897849, 0.67045204, 0.67204509, 0.67929496,\n",
       "        0.67936572, 0.66977689, 0.67189524, 0.67973681, 0.67798795,\n",
       "        0.66910248, 0.67047878, 0.67903374, 0.67903271, 0.66949275,\n",
       "        0.67134432, 0.67938294, 0.67895099, 0.66656944, 0.66737038,\n",
       "        0.66847688, 0.66884745, 0.66841985, 0.66875068, 0.66816995,\n",
       "        0.66711825, 0.66842711, 0.6693878 , 0.67045204, 0.66977689,\n",
       "        0.66910248, 0.66949275]),\n",
       " 'std_test_roc_auc': array([0.00102002, 0.00030538, 0.00056323, 0.00073148, 0.00093886,\n",
       "        0.00049462, 0.0009338 , 0.00104579, 0.00085053, 0.00094828,\n",
       "        0.00046381, 0.00052939, 0.00071148, 0.00066316, 0.00095468,\n",
       "        0.00081393, 0.00059344, 0.00101133, 0.00042196, 0.00083965,\n",
       "        0.00123455, 0.00057685, 0.00031875, 0.00083089, 0.00066231,\n",
       "        0.00073767, 0.00080475, 0.00029175, 0.00035323, 0.00084354,\n",
       "        0.00092798, 0.0010632 , 0.00092848, 0.00062988, 0.00073646,\n",
       "        0.00102002, 0.00093886, 0.00085053, 0.00071148, 0.00059344,\n",
       "        0.00123455, 0.00066231]),\n",
       " 'rank_test_roc_auc': array([40, 24, 13, 14, 35, 32, 11, 10, 27, 18,  2,  8, 20, 15,  5,  4, 22,\n",
       "        16,  1, 12, 29, 19,  6,  7, 25, 17,  3,  9, 42, 39, 34, 31, 37, 33,\n",
       "        38, 40, 35, 27, 20, 22, 29, 25]),\n",
       " 'split0_test_loan_loss': array([-131886.1, -131529. , -118236.5, -126217.2, -131851.8, -131419.6,\n",
       "        -117669.2, -125915.1, -131684.5, -131221.1, -117217.5, -125435.1,\n",
       "        -131612.5, -130976.7, -117683.8, -125341.8, -131548.2, -131081.5,\n",
       "        -117391.9, -125507.4, -131541. , -130926.6, -117688.3, -125641. ,\n",
       "        -131599. , -130795.2, -117372.2, -125644.3, -132303. , -132249.7,\n",
       "        -132321.5, -131995.9, -132031.2, -131852.3, -131943.5, -131886.1,\n",
       "        -131851.8, -131684.5, -131612.5, -131548.2, -131541. , -131599. ]),\n",
       " 'split1_test_loan_loss': array([-131936.6, -131333.2, -117425.1, -125884.7, -131890.2, -131549.8,\n",
       "        -117849.1, -125304.3, -131818.5, -131254.3, -117499. , -125491.2,\n",
       "        -131760.3, -131071.7, -117768.4, -125348.2, -131736. , -131071.3,\n",
       "        -117090.4, -125830.3, -131629.7, -131139.5, -117316.4, -125370.7,\n",
       "        -131548.9, -131077.4, -117354.5, -125038.2, -132359.7, -132284.2,\n",
       "        -132237.1, -132183.9, -132082.4, -132056.8, -132030.1, -131936.6,\n",
       "        -131890.2, -131818.5, -131760.3, -131736. , -131629.7, -131548.9]),\n",
       " 'split2_test_loan_loss': array([-131938.3, -131435. , -117328.8, -126234.8, -132016. , -131374.3,\n",
       "        -118094. , -124912.4, -131770.4, -131191.3, -117169.9, -125122.4,\n",
       "        -131745.7, -131252.2, -117564. , -125095.7, -131462.3, -130944.1,\n",
       "        -117260.1, -125431.2, -131614.6, -130855.1, -117229.8, -124952.8,\n",
       "        -131482. , -130877.9, -117057.5, -125041. , -132338.4, -132165.1,\n",
       "        -132117.3, -131935.4, -132019.7, -131918.5, -131994.1, -131938.3,\n",
       "        -132016. , -131770.4, -131745.7, -131462.3, -131614.6, -131482. ]),\n",
       " 'mean_test_loan_loss': array([-131920.33333333, -131432.4       , -117663.46666667,\n",
       "        -126112.23333333, -131919.33333333, -131447.9       ,\n",
       "        -117870.76666667, -125377.26666667, -131757.8       ,\n",
       "        -131222.23333333, -117295.46666667, -125349.56666667,\n",
       "        -131706.16666667, -131100.2       , -117672.06666667,\n",
       "        -125261.9       , -131582.16666667, -131032.3       ,\n",
       "        -117247.46666667, -125589.63333333, -131595.1       ,\n",
       "        -130973.73333333, -117411.5       , -125321.5       ,\n",
       "        -131543.3       , -130916.83333333, -117261.4       ,\n",
       "        -125241.16666667, -132333.7       , -132233.        ,\n",
       "        -132225.3       , -132038.4       , -132044.43333333,\n",
       "        -131942.53333333, -131989.23333333, -131920.33333333,\n",
       "        -131919.33333333, -131757.8       , -131706.16666667,\n",
       "        -131582.16666667, -131595.1       , -131543.3       ]),\n",
       " 'std_test_loan_loss': array([ 24.21656917,  79.95615465, 407.09853298, 161.05072355,\n",
       "         70.12832682,  74.38964982, 174.0992884 , 412.58932232,\n",
       "         55.42604683,  25.73212432, 145.22580885, 162.25560768,\n",
       "         66.49999165, 114.26358417,  83.85739217, 117.55018786,\n",
       "        114.28975846,  62.50567974, 123.41059742, 172.99700062,\n",
       "         38.7479892 , 120.79460069, 198.89456168, 283.10220769,\n",
       "         47.92890568, 118.45129332, 144.36003602, 285.06060564,\n",
       "         23.38503795,  50.03578719,  83.78082517, 105.80721462,\n",
       "         27.253909  ,  85.19883932,  35.52138636,  24.21656917,\n",
       "         70.12832682,  55.42604683,  66.49999165, 114.28975846,\n",
       "         38.7479892 ,  47.92890568]),\n",
       " 'rank_test_loan_loss': array([34, 20,  5, 14, 32, 21,  7, 12, 30, 19,  3, 11, 28, 18,  6,  9, 24,\n",
       "        17,  1, 13, 26, 16,  4, 10, 22, 15,  2,  8, 42, 41, 40, 38, 39, 36,\n",
       "        37, 34, 32, 30, 28, 24, 26, 22]),\n",
       " 'split0_test_neg_brier_score': array([0.12435002, 0.12487291, 0.17093035, 0.22674327, 0.12433827,\n",
       "        0.12490138, 0.17058732, 0.22674426, 0.1240761 , 0.12470199,\n",
       "        0.17076007, 0.22662717, 0.12416249, 0.12475028, 0.17069829,\n",
       "        0.22672122, 0.12420034, 0.1247191 , 0.17072772, 0.2267499 ,\n",
       "        0.12430983, 0.12479116, 0.17084714, 0.22692417, 0.12423133,\n",
       "        0.12481017, 0.17096219, 0.22682569, 0.12413509, 0.12391858,\n",
       "        0.12392586, 0.12377791, 0.12391496, 0.12390547, 0.12406639,\n",
       "        0.12435002, 0.12433827, 0.1240761 , 0.12416249, 0.12420034,\n",
       "        0.12430983, 0.12423133]),\n",
       " 'split1_test_neg_brier_score': array([0.12458805, 0.12489595, 0.17190956, 0.22744549, 0.12452575,\n",
       "        0.12512012, 0.17134413, 0.22733007, 0.12439302, 0.12502014,\n",
       "        0.17139234, 0.22654963, 0.1243234 , 0.12488179, 0.17143314,\n",
       "        0.2268701 , 0.12436126, 0.12496533, 0.17115367, 0.22744548,\n",
       "        0.12457863, 0.12511292, 0.17156789, 0.22683985, 0.12435215,\n",
       "        0.12503946, 0.17146836, 0.2267935 , 0.12411475, 0.12415608,\n",
       "        0.12407186, 0.12406104, 0.12398879, 0.12401248, 0.12399334,\n",
       "        0.12458805, 0.12452575, 0.12439302, 0.1243234 , 0.12436126,\n",
       "        0.12457863, 0.12435215]),\n",
       " 'split2_test_neg_brier_score': array([0.12442451, 0.12497342, 0.17115776, 0.22708244, 0.12422943,\n",
       "        0.12498388, 0.17078039, 0.22667895, 0.12420789, 0.12484385,\n",
       "        0.17122692, 0.22657684, 0.1241016 , 0.12470281, 0.17115602,\n",
       "        0.22690269, 0.12419001, 0.12474685, 0.17127803, 0.22672373,\n",
       "        0.12428765, 0.12493215, 0.17111337, 0.22653188, 0.12438264,\n",
       "        0.12476837, 0.17127949, 0.22690752, 0.12409003, 0.12398941,\n",
       "        0.12384188, 0.12385556, 0.12384297, 0.12389799, 0.12391983,\n",
       "        0.12442451, 0.12422943, 0.12420789, 0.1241016 , 0.12419001,\n",
       "        0.12428765, 0.12438264]),\n",
       " 'mean_test_neg_brier_score': array([0.12445419, 0.12491409, 0.17133256, 0.2270904 , 0.12436448,\n",
       "        0.12500179, 0.17090395, 0.22691776, 0.12422567, 0.12485533,\n",
       "        0.17112644, 0.22658454, 0.12419583, 0.12477829, 0.17109582,\n",
       "        0.22683134, 0.12425054, 0.12481043, 0.17105314, 0.22697303,\n",
       "        0.12439204, 0.12494541, 0.17117613, 0.2267653 , 0.12432204,\n",
       "        0.12487266, 0.17123668, 0.22684224, 0.12411329, 0.12402136,\n",
       "        0.12394653, 0.12389817, 0.12391557, 0.12393865, 0.12399318,\n",
       "        0.12445419, 0.12436448, 0.12422567, 0.12419583, 0.12425054,\n",
       "        0.12439204, 0.12432204]),\n",
       " 'std_test_neg_brier_score': array([9.94165822e-05, 4.29921412e-05, 4.18430002e-04, 2.86737952e-04,\n",
       "        1.22381932e-04, 9.01921625e-05, 3.21082541e-04, 2.92767015e-04,\n",
       "        1.29991234e-04, 1.30135918e-04, 2.67724688e-04, 3.21217274e-05,\n",
       "        9.35686214e-05, 7.57064764e-05, 3.03007742e-04, 7.89918522e-05,\n",
       "        7.84085240e-05, 1.10119621e-04, 2.35642859e-04, 3.34237719e-04,\n",
       "        1.32252969e-04, 1.31692307e-04, 2.97576459e-04, 1.68605274e-04,\n",
       "        6.53364252e-05, 1.19167829e-04, 2.08849827e-04, 4.79965453e-05,\n",
       "        1.84212850e-05, 9.95577093e-05, 9.50200099e-05, 1.19449182e-04,\n",
       "        5.95306055e-05, 5.22983999e-05, 5.98316080e-05, 9.94165822e-05,\n",
       "        1.22381932e-04, 1.29991234e-04, 9.35686214e-05, 7.84085240e-05,\n",
       "        1.32252969e-04, 6.53364252e-05]),\n",
       " 'rank_test_neg_brier_score': array([22, 17,  8,  1, 26, 15, 14,  3, 32, 19, 11,  7, 34, 21, 12,  5, 30,\n",
       "        20, 13,  2, 24, 16, 10,  6, 28, 18,  9,  4, 36, 37, 39, 42, 41, 40,\n",
       "        38, 22, 26, 32, 34, 30, 24, 28])}"
      ]
     },
     "execution_count": 1230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All results\n",
    "results_Grid1_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Grid Search #2: max_features and max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest object for gridsearch.\n",
    "rfLearn1 = RandomForestClassifier(n_estimators=50, max_features='auto', n_jobs=-1, random_state=rs, \n",
    "                             verbose=3, max_samples=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "steps4 = [('under', RandomUnderSampler(sampling_strategy=0.6, random_state=rs)), ('model', rfLearn1)]\n",
    "pipeline4 = Pipeline(steps=steps4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to test in gridsearch\n",
    "param_grid4 = [{'model__max_features': [0.2, 0.3, 0.4, 0.5, 'auto'],\n",
    "              'model__max_depth': [None, 1, 2, 5, 10, 15, 20],\n",
    "               }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch object\n",
    "gridRF4 = GridSearchCV(pipeline4, param_grid=param_grid4, scoring=scoringDict2, cv = kFold2, verbose=1,\n",
    "                                        refit='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed: 36.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50building tree 2 of 50\n",
      "\n",
      "building tree 3 of 50building tree 4 of 50\n",
      "\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   27.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50building tree 32 of 50\n",
      "\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   53.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('under',\n",
       "                                        RandomUnderSampler(random_state=0,\n",
       "                                                           sampling_strategy=0.6)),\n",
       "                                       ('model',\n",
       "                                        RandomForestClassifier(max_samples=0.6,\n",
       "                                                               n_estimators=50,\n",
       "                                                               n_jobs=-1,\n",
       "                                                               random_state=0,\n",
       "                                                               verbose=3))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'model__max_depth': [None, 1, 2, 5, 10, 15, 20],\n",
       "                          'model__max_featur...\n",
       "                      'average_precision': make_scorer(average_precision_score, needs_proba=True),\n",
       "                      'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
       "                      'f1': make_scorer(f1_score),\n",
       "                      'loan_loss': make_scorer(calcCostRev, greater_is_better=False, needs_proba=True, cost=(1, 3.7), returnThreshold=False),\n",
       "                      'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True),\n",
       "                      'roc_auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using the gridsearch object.\n",
    "gridRF4.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the results of the gridsearch\n",
    "results_Grid2_RF = gridRF4.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__max_depth': 10, 'model__max_features': 0.5}\n",
      "ROC-AUC: 0.6913995756690011\n",
      "F1: 0.32349636371560814\n",
      "Accuracy: 0.7748653895945722\n",
      "Balanced Accuracy: 0.6014515156657071\n",
      "Loan Loss: -115540.59999999999\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on roc_auc:\n",
    "ind = np.argmin(results_Grid2_RF['rank_test_roc_auc'])\n",
    "print('Parameters:', results_Grid2_RF['params'][ind])\n",
    "print('ROC-AUC:', results_Grid2_RF['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid2_RF['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid2_RF['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid2_RF['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid2_RF['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__max_depth': 15, 'model__max_features': 0.5}\n",
      "ROC-AUC: 0.6906392541130514\n",
      "F1: 0.32798643327190424\n",
      "Accuracy: 0.7687067750615687\n",
      "Balanced Accuracy: 0.6047988491128883\n",
      "Loan Loss: -115331.43333333333\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan_loss:\n",
    "ind = np.argmin(results_Grid2_RF['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid2_RF['params'][ind])\n",
    "print('ROC-AUC:', results_Grid2_RF['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid2_RF['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid2_RF['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid2_RF['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid2_RF['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Grid Search #3: max_depth and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest object for gridsearch\n",
    "rfLearn1 = RandomForestClassifier(n_estimators=50, max_features=0.5, n_jobs=-1, random_state=rs, \n",
    "                             verbose=3, max_samples=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "steps5 = [('under', RandomUnderSampler(sampling_strategy=0.6, random_state=rs)), ('model', rfLearn1)]\n",
    "pipeline5 = Pipeline(steps=steps5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for gridsearch.\n",
    "param_grid5 = [{'model__max_depth': [10, 15],\n",
    "                'under': [RandomUnderSampler(random_state=rs, sampling_strategy=0.3),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.4),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.5),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.6),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.7),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.8),\n",
    "                        RandomUnderSampler(random_state=rs, sampling_strategy=0.9)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create gridsearch object.\n",
    "gridRF5 = GridSearchCV(pipeline5, param_grid=param_grid5, scoring=scoringDict2, cv = kFold2, verbose=1,\n",
    "                                        refit='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed: 25.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50building tree 2 of 50building tree 3 of 50\n",
      "\n",
      "building tree 4 of 50\n",
      "\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   38.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('under',\n",
       "                                        RandomUnderSampler(random_state=0,\n",
       "                                                           sampling_strategy=0.6)),\n",
       "                                       ('model',\n",
       "                                        RandomForestClassifier(max_features=0.5,\n",
       "                                                               max_samples=0.6,\n",
       "                                                               n_estimators=50,\n",
       "                                                               n_jobs=-1,\n",
       "                                                               random_state=0,\n",
       "                                                               verbose=3))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'model__max_depth': [10, 15],\n",
       "                          'under': [Random...\n",
       "                      'average_precision': make_scorer(average_precision_score, needs_proba=True),\n",
       "                      'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
       "                      'f1': make_scorer(f1_score),\n",
       "                      'loan_loss': make_scorer(calcCostRev, greater_is_better=False, needs_proba=True, cost=(1, 3.7), returnThreshold=False),\n",
       "                      'neg_brier_score': make_scorer(brier_score_loss, needs_proba=True),\n",
       "                      'roc_auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 1306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using the gridsearch.\n",
    "gridRF5.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the results of the gridsearch.\n",
    "results_Grid3_RF = gridRF5.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__max_depth': 10, 'under': RandomUnderSampler(random_state=0, sampling_strategy=0.3)}\n",
      "ROC-AUC: 0.6916859372585149\n",
      "F1: 0.06657678173288102\n",
      "Accuracy: 0.8449358306779792\n",
      "Balanced Accuracy: 0.5136707334185636\n",
      "Loan Loss: -129672.66666666667\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on roc_auc:\n",
    "ind = np.argmin(results_Grid3_RF['rank_test_roc_auc'])\n",
    "print('Parameters:', results_Grid3_RF['params'][ind])\n",
    "print('ROC-AUC:', results_Grid3_RF['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid3_RF['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid3_RF['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid3_RF['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid3_RF['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'model__max_depth': 10, 'under': RandomUnderSampler(random_state=0, sampling_strategy=0.7)}\n",
      "ROC-AUC: 0.6912330886391942\n",
      "F1: 0.34485558008344447\n",
      "Accuracy: 0.7316938811657594\n",
      "Balanced Accuracy: 0.6208392206826635\n",
      "Loan Loss: -115038.46666666667\n"
     ]
    }
   ],
   "source": [
    "#Best parameters based on loan_loss:\n",
    "ind = np.argmin(results_Grid3_RF['rank_test_loan_loss'])\n",
    "print('Parameters:', results_Grid3_RF['params'][ind])\n",
    "print('ROC-AUC:', results_Grid3_RF['mean_test_roc_auc'][ind])\n",
    "print('F1:', results_Grid3_RF['mean_test_f1'][ind])\n",
    "print('Accuracy:', results_Grid3_RF['mean_test_accuracy'][ind])\n",
    "print('Balanced Accuracy:', results_Grid3_RF['mean_test_balanced_accuracy'][ind])\n",
    "print('Loan Loss:', results_Grid3_RF['mean_test_loan_loss'][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF: Determine optimal forest size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   6 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   6 out of   6 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  12 out of  12 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  12 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  15 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  15 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  21 out of  21 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  21 out of  21 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  24 out of  24 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  24 out of  24 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  27 out of  27 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  27 out of  27 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  30 out of  30 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  30 out of  30 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done  33 out of  33 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  33 out of  33 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done  36 out of  36 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  36 out of  36 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  39 out of  39 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  39 out of  39 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  42 out of  42 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  42 out of  42 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done  45 out of  45 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  45 out of  45 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  48 out of  48 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  48 out of  48 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done  51 out of  51 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done  51 out of  51 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  54 out of  54 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done  54 out of  54 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done  57 out of  57 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done  57 out of  57 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  60 out of  60 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  60 out of  60 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  63 out of  63 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  63 out of  63 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  66 out of  66 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  66 out of  66 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  69 out of  69 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  69 out of  69 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  75 out of  75 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  75 out of  75 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  78 out of  78 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  78 out of  78 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  81 out of  81 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  81 out of  81 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  84 out of  84 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  84 out of  84 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  87 out of  87 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  87 out of  87 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  90 out of  90 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  90 out of  90 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  93 out of  93 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done  93 out of  93 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done  99 out of  99 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  99 out of  99 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 102 out of 102 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 102 out of 102 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 105 out of 105 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 105 out of 105 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 111 out of 111 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 111 out of 111 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 114 out of 114 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 114 out of 114 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 117 out of 117 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 117 out of 117 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done 123 out of 123 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 123 out of 123 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 126 out of 126 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 126 out of 126 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 129 out of 129 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 129 out of 129 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 132 out of 132 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 132 out of 132 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 135 out of 135 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done 135 out of 135 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=3)]: Done 138 out of 138 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=3)]: Done 138 out of 138 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 141 out of 141 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 141 out of 141 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=3)]: Done 144 out of 144 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done 144 out of 144 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 147 out of 147 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 147 out of 147 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 150 out of 150 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 150 out of 150 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=3)]: Done 153 out of 153 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=3)]: Done 153 out of 153 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 156 out of 156 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 156 out of 156 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 159 out of 159 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 159 out of 159 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=3)]: Done 162 out of 162 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done 162 out of 162 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=3)]: Done 165 out of 165 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 165 out of 165 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 168 out of 168 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done 168 out of 168 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 171 out of 171 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 171 out of 171 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=3)]: Done 174 out of 174 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=3)]: Done 174 out of 174 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3building tree 3 of 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=3)]: Done 177 out of 177 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=3)]: Done 177 out of 177 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=3)]: Done 180 out of 180 | elapsed:    8.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done 180 out of 180 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "\n",
      "building tree 3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=3)]: Done 183 out of 183 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 183 out of 183 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=3)]: Done 186 out of 186 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 186 out of 186 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 189 out of 189 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done 189 out of 189 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 192 out of 192 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 192 out of 192 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3\n",
      "building tree 2 of 3building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 195 out of 195 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 195 out of 195 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done 198 out of 198 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 198 out of 198 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3building tree 2 of 3\n",
      "building tree 3 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=3)]: Done 201 out of 201 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=3)]: Done 201 out of 201 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "train_results_AUC = []\n",
    "test_results_AUC = []\n",
    "\n",
    "train_results_BalAcc = []\n",
    "test_results_BalAcc = []\n",
    "\n",
    "train_results_LoanLoss = []\n",
    "test_results_LoanLoss = []\n",
    "\n",
    "train_results_OvrAcc = []\n",
    "test_results_OvrAcc = []\n",
    "\n",
    "#balanced_accuracy_score\n",
    "\n",
    "list_nb_trees = [3]*67\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=0, max_features=0.5, n_jobs=3, random_state=rs, \n",
    "                             verbose=3, max_samples=0.6, max_depth = 10, warm_start=True)\n",
    "\n",
    "\n",
    "for nb_trees in list_nb_trees:\n",
    "\n",
    "    rf.n_estimators += nb_trees\n",
    "    \n",
    "    steps5 = [('under', RandomUnderSampler(sampling_strategy=0.7, random_state=rs)), ('model', rf)]\n",
    "    pipeline5 = Pipeline(steps=steps5)\n",
    "    pipeline5.fit(dfTrain, yTrain)\n",
    "    \n",
    "    trainProb = pipeline5.predict_proba(dfTrain)\n",
    "    testProb = pipeline5.predict_proba(dfTest)\n",
    "    \n",
    "    #AUC metrics\n",
    "    trainAUC = roc_auc_score(y_true=yTrain, y_score=trainProb[:,1])\n",
    "    testAUC = roc_auc_score(y_true=yTest, y_score=testProb[:,1])\n",
    "    train_results_AUC.append(trainAUC)\n",
    "    test_results_AUC.append(testAUC)\n",
    "    \n",
    "    #Balanced accuracy metrics\n",
    "    trainAcc = balanced_accuracy_score(y_true=yTrain, y_pred=np.argmax(trainProb, axis=1))\n",
    "    testAcc = balanced_accuracy_score(y_true=yTest, y_pred=np.argmax(testProb, axis=1))\n",
    "    train_results_BalAcc.append(trainAcc)\n",
    "    test_results_BalAcc.append(testAcc)\n",
    "    \n",
    "    #Overall accuracy.\n",
    "    trainAcc_Ovr = accuracy_score(y_true=yTrain, y_pred=np.argmax(trainProb, axis=1))\n",
    "    testAcc_Ovr = accuracy_score(y_true=yTest, y_pred=np.argmax(testProb, axis=1))\n",
    "    train_results_OvrAcc.append(trainAcc_Ovr)\n",
    "    test_results_OvrAcc.append(testAcc_Ovr)\n",
    "    \n",
    "    #Loan loss metric\n",
    "    loan_loss_train = calcCostRev(yTrain, trainProb[:,1], cost = (1,3.7), returnThreshold = False)\n",
    "    loan_loss_test = calcCostRev(yTest, testProb[:,1], cost = (1,3.7), returnThreshold = False)\n",
    "    train_results_LoanLoss.append(loan_loss_train)\n",
    "    test_results_LoanLoss.append(loan_loss_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [3]\n",
    "num = len(list_nb_trees)\n",
    "for i in range(1,num):\n",
    "    actual.append(3 + actual[i-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize AUC-ROC based on forest size.\n",
    "line1, = plt.plot(actual, train_results_AUC, color=\"r\", label=\"Training Set\")\n",
    "line2, = plt.plot(actual, test_results_AUC, color=\"g\", label=\"Testing Set\")\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('ROC_AUC')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize balanced accuracy based on forest size.\n",
    "line1, = plt.plot(actual, train_results_BalAcc, color=\"r\", label=\"Training Set\")\n",
    "line2, = plt.plot(actual, test_results_BalAcc, color=\"g\", label=\"Testing Set\")\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Balanced Accuracy')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualized overall accuracy based on forest size.\n",
    "line1, = plt.plot(actual, train_results_OvrAcc, color=\"r\", label=\"Training Set\")\n",
    "line2, = plt.plot(actual, test_results_OvrAcc, color=\"g\", label=\"Testing Set\")\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Overall Accuracy')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualized error cost based on forest size.\n",
    "line1, = plt.plot(actual, np.array(train_results_LoanLoss)/dfTrain.shape[0], color=\"r\", label=\"Training Set\")\n",
    "line2, = plt.plot(actual, np.array(test_results_LoanLoss)/dfTest.shape[0], color=\"g\", label=\"Testing Set\")\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Avg. Cost of Error')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL FINAL RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest object used as the final model for predictions.\n",
    "rfLearnFinal2 = RandomForestClassifier(n_estimators=75, max_features=0.5, n_jobs=-1, random_state=rs, \n",
    "                             verbose=3, max_samples=0.6, max_depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest learner to be used as final model\n",
    "rfLearnFinal = RandomForestClassifier(n_estimators=75, n_jobs=-1, random_state=rs,\\\n",
    "                                 verbose=2, max_depth=10, max_features=0.4,\\\n",
    "                                 max_samples = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline.\n",
    "stepsRF = [('under', RandomUnderSampler(sampling_strategy=0.7, random_state=rs)), ('model', rfLearnFinal2)]\n",
    "pipelineRF = Pipeline(steps=stepsRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 75building tree 2 of 75building tree 3 of 75\n",
      "\n",
      "building tree 4 of 75\n",
      "\n",
      "building tree 5 of 75\n",
      "building tree 6 of 75\n",
      "building tree 7 of 75\n",
      "building tree 8 of 75\n",
      "building tree 9 of 75\n",
      "building tree 10 of 75\n",
      "building tree 11 of 75\n",
      "building tree 12 of 75\n",
      "building tree 13 of 75\n",
      "building tree 14 of 75\n",
      "building tree 15 of 75\n",
      "building tree 16 of 75\n",
      "building tree 17 of 75\n",
      "building tree 18 of 75\n",
      "building tree 19 of 75\n",
      "building tree 20 of 75\n",
      "building tree 21 of 75\n",
      "building tree 22 of 75\n",
      "building tree 23 of 75\n",
      "building tree 24 of 75\n",
      "building tree 25 of 75\n",
      "building tree 26 of 75\n",
      "building tree 27 of 75\n",
      "building tree 28 of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   20.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 29 of 75\n",
      "building tree 30 of 75\n",
      "building tree 31 of 75\n",
      "building tree 32 of 75\n",
      "building tree 33 of 75\n",
      "building tree 34 of 75\n",
      "building tree 35 of 75\n",
      "building tree 36 of 75\n",
      "building tree 37 of 75\n",
      "building tree 38 of 75\n",
      "building tree 39 of 75\n",
      "building tree 40 of 75\n",
      "building tree 41 of 75\n",
      "building tree 42 of 75\n",
      "building tree 43 of 75\n",
      "building tree 44 of 75\n",
      "building tree 45 of 75\n",
      "building tree 46 of 75\n",
      "building tree 47 of 75\n",
      "building tree 48 of 75\n",
      "building tree 49 of 75\n",
      "building tree 50 of 75\n",
      "building tree 51 of 75\n",
      "building tree 52 of 75\n",
      "building tree 53 of 75\n",
      "building tree 54 of 75\n",
      "building tree 55 of 75\n",
      "building tree 56 of 75\n",
      "building tree 57 of 75\n",
      "building tree 58 of 75\n",
      "building tree 59 of 75\n",
      "building tree 60 of 75\n",
      "building tree 61 of 75\n",
      "building tree 62 of 75\n",
      "building tree 63 of 75\n",
      "building tree 64 of 75\n",
      "building tree 65 of 75\n",
      "building tree 66 of 75\n",
      "building tree 67 of 75\n",
      "building tree 68 of 75\n",
      "building tree 69 of 75\n",
      "building tree 70 of 75\n",
      "building tree 71 of 75\n",
      "building tree 72 of 75\n",
      "building tree 73 of 75\n",
      "building tree 74 of 75\n",
      "building tree 75 of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('under',\n",
       "                 RandomUnderSampler(random_state=0, sampling_strategy=0.7)),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(max_depth=10, max_features=0.5,\n",
       "                                        max_samples=0.6, n_estimators=75,\n",
       "                                        n_jobs=-1, random_state=0,\n",
       "                                        verbose=3))])"
      ]
     },
     "execution_count": 1377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model.\n",
    "pipelineRF.fit(dfPostImpute[trainAll], trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1378,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:    2.0s finished\n"
     ]
    }
   ],
   "source": [
    "#Get probability predictions\n",
    "y_prob_RF = pipelineRF.predict_proba(dfPostImpute[testAll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "#Get label predictions\n",
    "y_hat_RF = pipelineRF.predict(dfPostImpute[testAll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision(+)</th>\n",
       "      <th>Recall(+)</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Error Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Result</th>\n",
       "      <td>0.734964</td>\n",
       "      <td>0.311934</td>\n",
       "      <td>0.453465</td>\n",
       "      <td>0.695715</td>\n",
       "      <td>233913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision(+)  Recall(+)       AUC  Error Cost\n",
       "Result  0.734964      0.311934   0.453465  0.695715      233913"
      ]
     },
     "execution_count": 1380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scores based on default prob threshold.\n",
    "getScores(testLabels, y_hat_RF, y_prob_RF[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[240365,  62678],\n",
       "       [ 34247,  28415]], dtype=int64)"
      ]
     },
     "execution_count": 1381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(testLabels, y_hat_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Series object with the results\n",
    "resultsRF = {'id': dfFinal[testAll].id.values, 'true_class': testLabels, 'predict_class_RF': y_hat_RF, \n",
    "                 'prob_RF':y_prob_RF[:,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the series to a dataframe\n",
    "df_Results_RF = pd.DataFrame(resultsRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_class</th>\n",
       "      <th>predict_class_RF</th>\n",
       "      <th>prob_RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73713571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75051478</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.483233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75092979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75123593</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75358919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  true_class  predict_class_RF   prob_RF\n",
       "0  73713571           0                 0  0.376075\n",
       "1  75051478           0                 0  0.483233\n",
       "2  75092979           0                 0  0.410791\n",
       "3  75123593           0                 0  0.155637\n",
       "5  75358919           0                 0  0.409393"
      ]
     },
     "execution_count": 1393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview the results dataframe.\n",
    "df_Results_RF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results\n",
    "df_Results_RF.to_csv('results_RF2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the random forest object from the pipeline object.\n",
    "modelRF = pipelineRF[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with features and their importance as determined by the Random Forest model.\n",
    "df_RF_Features = pd.DataFrame([dfPostImpute.columns,modelRF.feature_importances_]).T\n",
    "df_RF_Features.columns = ['Feature', 'Importance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.297629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>grade_A</td>\n",
       "      <td>0.131468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>grade_B</td>\n",
       "      <td>0.0673115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.0425632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dti</td>\n",
       "      <td>0.0423373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg_cur_bal</td>\n",
       "      <td>0.0400542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc_open_past_24mths</td>\n",
       "      <td>0.0377215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fico</td>\n",
       "      <td>0.0336661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>installment</td>\n",
       "      <td>0.0305057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>grade_C</td>\n",
       "      <td>0.0214035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature Importance\n",
       "20              int_rate   0.297629\n",
       "58               grade_A   0.131468\n",
       "59               grade_B  0.0673115\n",
       "1             annual_inc  0.0425632\n",
       "5                    dti  0.0423373\n",
       "2            avg_cur_bal  0.0400542\n",
       "0   acc_open_past_24mths  0.0377215\n",
       "6                   fico  0.0336661\n",
       "8            installment  0.0305057\n",
       "60               grade_C  0.0214035"
      ]
     },
     "execution_count": 1387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 most important features in the random forest.\n",
    "df_RF_Features.sort_values(by='Importance', ascending=False)[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
